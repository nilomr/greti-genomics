
# time: 2024-08-27 15:03:13 UTC
# mode: r
+# Import configuration file
+config <- config::get()

# time: 2024-08-27 15:03:45 UTC
# mode: r
+.vsc.attach()

# time: 2024-08-27 15:04:20 UTC
# mode: r
+# Import configuration file
+config <- config::get()

# time: 2024-08-27 15:09:14 UTC
# mode: r
+.vsc.attach()

# time: 2024-08-27 15:10:27 UTC
# mode: r
+# Import configuration file
+config <- config::get()
+
+# parameters
+percent.train = 0.7
+num.threads = 44
+num.eigenvectors = 5
+set.seed(555)
+
+# Input files
+immigrant.vcf <- file.path(config$path$data, "600K_immigrants.vcf")
+resident.vcf <- file.path(config$path$data, "600K_residents.vcf")
+
+immigrant.gds = file.path(config$path$data, "immigrant_recode.gds")
+if (!file.exists(immigrant.gds)) {
+    SNPRelate::snpgdsVCF2GDS(immigrant.vcf, immigrant.gds, method="biallelic.only")
+
+}
+
+genofile.maf = SNPRelate::snpgdsOpen(immigrant.gds)

# time: 2024-08-27 15:10:30 UTC
# mode: r
+genofile.maf

# time: 2024-08-27 15:14:00 UTC
# mode: r
+# Input files
+immigrant.info.file <- file.path(config$path$data, "600K_immigrants.fam")

# time: 2024-08-27 15:14:21 UTC
# mode: r
+immigrant.labels = read.table(immigrant.info.file, header=TRUE, sep="\t")

# time: 2024-08-27 15:14:23 UTC
# mode: r
+immigrant.labels

# time: 2024-08-27 15:15:10 UTC
# mode: r
+immigrant.labels = read.table(immigrant.info.file, header=FALSE, sep="\t")

# time: 2024-08-27 15:15:12 UTC
# mode: r
+immigrant.labels

# time: 2024-08-27 15:16:24 UTC
# mode: r
+genofile.maf

# time: 2024-08-27 15:17:04 UTC
# mode: r
+sample.id = read.gdsn(index.gdsn(genofile.maf, "sample.id"))

# time: 2024-08-27 15:17:12 UTC
# mode: r
+sample.id = gdsfmt::read.gdsn(index.gdsn(genofile.maf, "sample.id"))

# time: 2024-08-27 15:17:19 UTC
# mode: r
+sample.id = gdsfmt::read.gdsn(gdsfmt::index.gdsn(genofile.maf, "sample.id"))

# time: 2024-08-27 15:17:21 UTC
# mode: r
+sample.id

# time: 2024-08-27 15:22:58 UTC
# mode: r
+# Import configuration file
+config <- config::get()
+
+# parameters
+percent.train = 0.7
+num.threads = 44
+num.eigenvectors = 5
+set.seed(555)
+
+# Input files
+immigrant.info <- file.path(config$path$data, "600K_immigrants.fam")
+immigrant.vcf <- file.path(config$path$data, "600K_immigrants.vcf")
+resident.vcf <- file.path(config$path$data, "600K_residents.vcf")
+
+immigrant.gds = file.path(config$path$data, "immigrant_recode.gds")
+resident.gds = file.path(config$path$data, "resident_recode.gds")
+
+SNPRelate::snpgdsVCF2GDS(immigrant.vcf, immigrant.gds, method="biallelic.only")
+SNPRelate::snpgdsVCF2GDS(resident.vcf, resident.gds, method="biallelic.only")

# time: 2024-08-27 15:25:13 UTC
# mode: r
+# remove any existing gds files before creating new ones
+file.remove(list.files(pattern = "\\.gds$"))

# time: 2024-08-27 15:26:51 UTC
# mode: r
+# Import configuration file
+config <- config::get()
+
+# parameters
+percent.train = 0.7
+num.threads = 44
+num.eigenvectors = 5
+set.seed(555)
+
+# Input files
+immigrant.info <- file.path(config$path$data, "600K_immigrants.fam")
+immigrant.vcf <- file.path(config$path$data, "600K_immigrants.vcf")
+resident.vcf <- file.path(config$path$data, "600K_residents.vcf")
+
+immigrant.gds = file.path(config$path$data, "immigrant_recode.gds")
+resident.gds = file.path(config$path$data, "resident_recode.gds")

# time: 2024-08-27 15:27:31 UTC
# mode: r
+# Import configuration file
+config <- config::get()
+
+# parameters
+percent.train = 0.7
+num.threads = 44
+num.eigenvectors = 5
+set.seed(555)
+
+# Input files
+immigrant.info <- file.path(config$path$data, "600K_immigrants.fam")
+immigrant.vcf <- file.path(config$path$data, "600K_immigrants.vcf")
+resident.vcf <- file.path(config$path$data, "600K_residents.vcf")
+
+immigrant.gds = file.path(config$path$data, "immigrant_recode.gds")
+resident.gds = file.path(config$path$data, "resident_recode.gds")
+
+# remove any existing gds files before creating new ones
+file.remove(list.files(config$path$data, pattern = "\\.gds$"))
+
+SNPRelate::snpgdsVCF2GDS(immigrant.vcf, immigrant.gds, method="biallelic.only")
+SNPRelate::snpgdsVCF2GDS(resident.vcf, resident.gds, method="biallelic.only")

# time: 2024-08-27 15:29:27 UTC
# mode: r
+# Append the immigrant and resident data
+SNPRelate::snpgdsCombineGeno(c(immigrant.gds, resident.gds), "allsamples.gds")

# time: 2024-08-27 15:29:58 UTC
# mode: r
+# Append the immigrant and resident data
+SNPRelate::snpgdsCombineGeno(c(immigrant.gds, resident.gds), file.path(config$path$data,"allsamples.gds"))

# time: 2024-08-27 15:30:38 UTC
# mode: r
+all.gds = file.path(config$path$data, "all_recode.gds")

# time: 2024-08-27 15:30:40 UTC
# mode: r
+# Append the immigrant and resident data
+SNPRelate::snpgdsCombineGeno(c(immigrant.gds, resident.gds), all.gds)

# time: 2024-08-27 15:31:14 UTC
# mode: r
+all.maf = SNPRelate::snpgdsOpen(all.gds)

# time: 2024-08-27 15:31:15 UTC
# mode: r
+resident.maf = SNPRelate::snpgdsOpen(resident.gds)

# time: 2024-08-27 15:31:18 UTC
# mode: r
+immigrant.maf = SNPRelate::snpgdsOpen(immigrant.gds)

# time: 2024-08-27 15:31:18 UTC
# mode: r
+resident.maf = SNPRelate::snpgdsOpen(resident.gds)

# time: 2024-08-27 15:31:20 UTC
# mode: r
+all.maf = SNPRelate::snpgdsOpen(all.gds)

# time: 2024-08-27 15:31:21 UTC
# mode: r
+res.sample.id = gdsfmt::read.gdsn(gdsfmt::index.gdsn(genofile.maf, "sample.id"))

# time: 2024-08-27 15:31:23 UTC
# mode: r
+all.maf

# time: 2024-08-27 15:32:00 UTC
# mode: r
+immigrant.id = gdsfmt::read.gdsn(gdsfmt::index.gdsn(immigrant.maf, "sample.id"))

# time: 2024-08-27 15:32:07 UTC
# mode: r
+resident.id = gdsfmt::read.gdsn(gdsfmt::index.gdsn(resident.maf, "sample.id"))

# time: 2024-08-27 15:32:08 UTC
# mode: r
+resident.id

# time: 2024-08-27 15:33:06 UTC
# mode: r
+all.id = gdsfmt::read.gdsn(gdsfmt::index.gdsn(all.maf, "sample.id"))

# time: 2024-08-27 15:33:32 UTC
# mode: r
+all.id$super_pop

# time: 2024-08-27 15:35:06 UTC
# mode: r
+num.samples <- min(length(immigrant.id), length(resident.id))
+immigrant.id <- sample(immigrant.id, num.samples)
+resident.id <- sample(resident.id, num.samples)
+all.id <- c(immigrant.id, resident.id)

# time: 2024-08-27 15:35:12 UTC
# mode: r
+immigrant.id

# time: 2024-08-27 15:35:15 UTC
# mode: r
+resident.id

# time: 2024-08-27 15:35:24 UTC
# mode: r
+num.samples

# time: 2024-08-27 15:35:38 UTC
# mode: r
+immigrant.id

# time: 2024-08-27 15:35:41 UTC
# mode: r
+immigrant.id = gdsfmt::read.gdsn(gdsfmt::index.gdsn(immigrant.maf, "sample.id"))

# time: 2024-08-27 15:35:43 UTC
# mode: r
+immigrant.id

# time: 2024-08-27 15:41:30 UTC
# mode: r
+# Load all the data
+all.maf = SNPRelate::snpgdsOpen(all.gds)
+
+# Load labels
+immigrant.id = gdsfmt::read.gdsn(gdsfmt::index.gdsn(SNPRelate::snpgdsOpen(immigrant.gds), "sample.id"))
+resident.id = gdsfmt::read.gdsn(gdsfmt::index.gdsn(SNPRelate::snpgdsOpen(resident.gds), "sample.id"))
+all.id = gdsfmt::read.gdsn(gdsfmt::index.gdsn(all.maf, "sample.id"))
+
+# randomly subsample the data so that the number of samples in all from immigrant and resident are the same
+num.samples <- min(length(immigrant.id), length(resident.id))
+immigrant.id <- sample(immigrant.id, num.samples)
+resident.id <- sample(resident.id, num.samples)
+all.id <- c(immigrant.id, resident.id)
+
+# Split the data into training and testing sets
+num.train <- round(num.samples * percent.train)
+num.test <- num.samples - num.train
+train.id <- sample(all.id, num.train)
+test.id <- setdiff(all.id, train.id)
+
+# Reduce dimensionality via pca
+num.threads = 3
+pca = snpgdsPCA(all.maf, sample.id=train.id, num.thread=num.threads)

# time: 2024-08-27 15:41:44 UTC
# mode: r
+# Load labels
+immigrant.id = gdsfmt::read.gdsn(gdsfmt::index.gdsn(SNPRelate::snpgdsOpen(immigrant.gds), "sample.id"))

# time: 2024-08-27 15:41:45 UTC
# mode: r
+resident.id = gdsfmt::read.gdsn(gdsfmt::index.gdsn(SNPRelate::snpgdsOpen(resident.gds), "sample.id"))

# time: 2024-08-27 15:41:45 UTC
# mode: r
+all.id = gdsfmt::read.gdsn(gdsfmt::index.gdsn(all.maf, "sample.id"))

# time: 2024-08-27 15:41:50 UTC
# mode: r
+# randomly subsample the data so that the number of samples in all from immigrant and resident are the same
+num.samples <- min(length(immigrant.id), length(resident.id))

# time: 2024-08-27 15:41:51 UTC
# mode: r
+immigrant.id <- sample(immigrant.id, num.samples)

# time: 2024-08-27 15:41:51 UTC
# mode: r
+resident.id <- sample(resident.id, num.samples)

# time: 2024-08-27 15:41:52 UTC
# mode: r
+all.id <- c(immigrant.id, resident.id)

# time: 2024-08-27 15:41:53 UTC
# mode: r
+# Split the data into training and testing sets
+num.train <- round(num.samples * percent.train)

# time: 2024-08-27 15:41:54 UTC
# mode: r
+num.test <- num.samples - num.train

# time: 2024-08-27 15:41:54 UTC
# mode: r
+train.id <- sample(all.id, num.train)

# time: 2024-08-27 15:41:55 UTC
# mode: r
+test.id <- setdiff(all.id, train.id)

# time: 2024-08-27 15:41:56 UTC
# mode: r
+# Reduce dimensionality via pca
+num.threads = 3

# time: 2024-08-27 15:41:56 UTC
# mode: r
+pca = snpgdsPCA(all.maf, sample.id=train.id, num.thread=num.threads)

# time: 2024-08-27 15:42:06 UTC
# mode: r
+pca = SNPRelate::snpgdsPCA(all.maf, sample.id=train.id, num.thread=num.threads)

# time: 2024-08-27 15:42:42 UTC
# mode: r
+pc.percent = pca$varprop*100 
+head(round(pc.percent, num.eigenvectors))

# time: 2024-08-27 15:43:03 UTC
# mode: r
+lbls <- paste("PC", 1:num.eigenvectors, "\n", format(pc.percent[1:num.eigenvectors], digits=2), "%", sep="")
+pairs(pca$eigenvect[,1:num.eigenvectors], col=tab$pop, labels=lbls)

# time: 2024-08-27 15:43:21 UTC
# mode: r
+tab <- data.frame(sample.id=pca$sample.id, pop=factor(pop_code)[match(pca$sample.id, pca$sample.id)], EV1=pca$eigenvect[,1], EV2=pca$eigenvect[,2], stringsAsFactors=FALSE)
+
+lbls <- paste("PC", 1:num.eigenvectors, "\n", format(pc.percent[1:num.eigenvectors], digits=2), "%", sep="")
+pairs(pca$eigenvect[,1:num.eigenvectors], col=tab$pop, labels=lbls)

# time: 2024-08-27 15:43:42 UTC
# mode: r
+test.id

# time: 2024-08-27 15:46:34 UTC
# mode: r
+train.id

# time: 2024-08-27 15:48:55 UTC
# mode: r
+renv::install('rlint')

# time: 2024-08-27 15:53:55 UTC
# mode: r
+.vsc.attach()

# time: 2024-08-27 15:55:58 UTC
# mode: r
+pca$sample.id

# time: 2024-08-27 15:56:03 UTC
# mode: r
+# Import configuration file
+config <- config::get()
+
+# parameters
+percent.train <- 0.7
+num.threads <- 44
+num.eigenvectors <- 5
+set.seed(555)
+
+# Input files
+immigrant.info <- file.path(config$path$data, "600K_immigrants.fam")
+immigrant.vcf <- file.path(config$path$data, "600K_immigrants.vcf")
+resident.vcf <- file.path(config$path$data, "600K_residents.vcf")
+
+immigrant.gds <- file.path(config$path$data, "immigrant_recode.gds")
+resident.gds <- file.path(config$path$data, "resident_recode.gds")
+all.gds <- file.path(config$path$data, "all_recode.gds")
+
+# remove any existing gds files before creating new ones
+file.remove(list.files(config$path$data, pattern = "\\.gds$"))
+
+SNPRelate::snpgdsVCF2GDS(immigrant.vcf, immigrant.gds, method = "biallelic.only")
+SNPRelate::snpgdsVCF2GDS(resident.vcf, resident.gds, method = "biallelic.only")
+
+# Append the immigrant and resident data
+SNPRelate::snpgdsCombineGeno(c(immigrant.gds, resident.gds), all.gds)
+
+# Load all the data
+all.maf <- SNPRelate::snpgdsOpen(all.gds)
+
+# Load labels
+immigrant.id <- gdsfmt::read.gdsn(gdsfmt::index.gdsn(
+    SNPRelate::snpgdsOpen(immigrant.gds),
+    "sample.id"
+))
+resident.id <- gdsfmt::read.gdsn(gdsfmt::index.gdsn(
+    SNPRelate::snpgdsOpen(resident.gds),
+    "sample.id"
+))
+all.id <- gdsfmt::read.gdsn(gdsfmt::index.gdsn(all.maf, "sample.id"))
+
+# randomly subsample the data so that the number of samples in all from immigrant and resident are the same
+num.samples <- min(length(immigrant.id), length(resident.id))
+immigrant.id <- sample(immigrant.id, num.samples)
+resident.id <- sample(resident.id, num.samples)
+all.id <- c(immigrant.id, resident.id)
+
+# Split the data into training and testing sets
+num.train <- round(num.samples * percent.train)
+num.test <- num.samples - num.train
+train.id <- sample(all.id, num.train)
+test.id <- setdiff(all.id, train.id)
+pop_code <- c(rep("immigrant", num.train), rep("resident", num.test))
+
+# Reduce dimensionality via pca
+num.threads <- 3
+pca <- SNPRelate::snpgdsPCA(all.maf, sample.id = train.id, num.thread = num.threads)

# time: 2024-08-27 15:57:06 UTC
# mode: r
+list.files(path = config$path$data, pattern = "*.gds$")

# time: 2024-08-27 15:57:44 UTC
# mode: r
+pca$sample.id

# time: 2024-08-27 15:58:06 UTC
# mode: r
+tab <- data.frame(sample.id = pca$sample.id, pop = factor(pop_code)[match(pca$sample.id, pca$sample.id)], EV1 = pca$eigenvect[, 1], EV2 = pca$eigenvect[, 2], stringsAsFactors = FALSE)

# time: 2024-08-27 15:58:08 UTC
# mode: r
+tab

# time: 2024-08-27 15:58:43 UTC
# mode: r
+pop_code

# time: 2024-08-27 15:59:10 UTC
# mode: r
+train.id

# time: 2024-08-27 15:59:27 UTC
# mode: r
+length(test.id)

# time: 2024-08-27 15:59:29 UTC
# mode: r
+# check the length of the training and testing sets
+length(train.id)

# time: 2024-08-27 15:59:41 UTC
# mode: r
+resident.id

# time: 2024-08-27 16:00:41 UTC
# mode: r
+all.id <- gdsfmt::read.gdsn(gdsfmt::index.gdsn(all.maf, "sample.id"))
+
+# randomly subsample the data so that the number of samples in all from immigrant and resident are the same
+min.samples <- min(length(immigrant.id), length(resident.id))
+immigrant.id <- sample(immigrant.id, min.samples)
+resident.id <- sample(resident.id, min.samples)
+all.id <- c(immigrant.id, resident.id)
+
+# Split the data into training and testing sets
+num.samples <- length(all.id)
+num.train <- round(num.samples * percent.train)
+num.test <- num.samples - num.train
+train.id <- sample(all.id, num.train)
+test.id <- setdiff(all.id, train.id)

# time: 2024-08-27 16:00:44 UTC
# mode: r
+# check the length of the training and testing sets
+length(train.id)

# time: 2024-08-27 16:00:46 UTC
# mode: r
+length(test.id)

# time: 2024-08-27 16:02:58 UTC
# mode: r
+# create a 'train.label' list with wether each sample in train.id is from "immigrant" or "resident"
+
+train.label <- ifelse(train.id %in% immigrant.id, "immigrant", "resident")

# time: 2024-08-27 16:02:58 UTC
# mode: r
+train.label <- ifelse(train.id %in% immigrant.id, "immigrant", "resident")

# time: 2024-08-27 16:03:00 UTC
# mode: r
+train.label

# time: 2024-08-27 16:03:41 UTC
# mode: r
+test.label <- ifelse(test.id %in% immigrant.id, "immigrant", "resident")

# time: 2024-08-27 16:04:20 UTC
# mode: r
+# Identify the population code for each sample
+train.label <- ifelse(train.id %in% immigrant.id, "immigrant", "resident")

# time: 2024-08-27 16:04:20 UTC
# mode: r
+test.label <- ifelse(test.id %in% immigrant.id, "immigrant", "resident")

# time: 2024-08-27 16:04:21 UTC
# mode: r
+# Reduce dimensionality via pca
+num.threads <- 3

# time: 2024-08-27 16:04:22 UTC
# mode: r
+pca <- SNPRelate::snpgdsPCA(all.maf, sample.id = train.id, num.thread = num.threads)

# time: 2024-08-27 16:04:30 UTC
# mode: r
+tab <- data.frame(sample.id = pca$sample.id, pop = factor(train.label), EV1 = pca$eigenvect[, 1], EV2 = pca$eigenvect[, 2], stringsAsFactors = FALSE)

# time: 2024-08-27 16:04:32 UTC
# mode: r
+lbls <- paste("PC", 1:num.eigenvectors, "\n", format(pc.percent[1:num.eigenvectors], digits = 2), "%", sep = "")

# time: 2024-08-27 16:04:47 UTC
# mode: r
+pc.percent <- pca$varprop * 100

# time: 2024-08-27 16:04:48 UTC
# mode: r
+lbls <- paste("PC", 1:num.eigenvectors, "\n", format(pc.percent[1:num.eigenvectors], digits = 2), "%", sep = "")

# time: 2024-08-27 16:04:51 UTC
# mode: r
+pairs(pca$eigenvect[, 1:num.eigenvectors], col = tab$pop, labels = lbls)

# time: 2024-08-27 16:05:37 UTC
# mode: r
+pairs(pca$eigenvect[, 1:num.eigenvectors], col = tab$pop, labels = lbls, aspect = 1)

# time: 2024-08-27 16:08:24 UTC
# mode: r
+# Calculate SNP loadings
+snp_loadings <- SNPRelate::snpgdsPCASNPLoading(pca, all.maf, num.thread = num.threads)

# time: 2024-08-27 16:08:46 UTC
# mode: r
+test.id

# time: 2024-08-27 16:08:52 UTC
# mode: r
+# Extract list of SNPs used in PCA
+snp_list <- SNPRelate::snpgdsSNPList(all.maf)

# time: 2024-08-27 16:08:54 UTC
# mode: r
+snp_rs_id <- snp_list$rs.id[snp_loadings$snp.id]

# time: 2024-08-27 16:08:55 UTC
# mode: r
+snp_chrom <- snp_list$chromosome[snp_loadings$snp.id]

# time: 2024-08-27 16:08:56 UTC
# mode: r
+snp_pos <- snp_list$position[snp_loadings$snp.id]

# time: 2024-08-27 16:08:57 UTC
# mode: r
+# Create dataframe with one SNP per row
+snp_df <- data.frame(snp_rs_id, snp_chrom, snp_pos)

# time: 2024-08-27 16:09:04 UTC
# mode: r
+snp_chrom

# time: 2024-08-27 16:09:09 UTC
# mode: r
+snp_pos

# time: 2024-08-27 16:09:11 UTC
# mode: r
+snp_rs_id

# time: 2024-08-27 16:09:26 UTC
# mode: r
+snp_loadings

# time: 2024-08-27 16:10:55 UTC
# mode: r
+# Extract list of SNPs used in PCA
+snpset.id <- unlist(snpset.maf)

# time: 2024-08-27 16:11:16 UTC
# mode: r
+# Extract list of SNPs used in PCA
+snpset.id <- unlist(all.maf)

# time: 2024-08-27 16:11:21 UTC
# mode: r
+snp_list <- SNPRelate::snpgdsSNPList(all.maf)
+snp_rs_id <- snp_list$rs.id[ssnpset.id]
+snp_chrom <- snp_list$chromosome[ssnpset.id]
+snp_pos <- snp_list$position[ssnpset.id]

# time: 2024-08-27 16:11:30 UTC
# mode: r
+snp_rs_id <- snp_list$rs.id[snpset.id]

# time: 2024-08-27 16:11:34 UTC
# mode: r
+snp_rs_id <- snp_list$rs.id[snpset.id]
+snp_chrom <- snp_list$chromosome[snpset.id]
+snp_pos <- snp_list$position[snpset.id]

# time: 2024-08-27 16:11:42 UTC
# mode: r
+snpset.id

# time: 2024-08-27 16:11:53 UTC
# mode: r
+# Extract list of SNPs used in PCA
+snpset.id <- unlist(all.maf)

# time: 2024-08-27 16:11:56 UTC
# mode: r
+snpset.id

# time: 2024-08-27 16:12:05 UTC
# mode: r
+snp_list

# time: 2024-08-27 16:12:13 UTC
# mode: r
+snp_rs_id <- snp_list$rs.id[snp_list]

# time: 2024-08-27 16:12:14 UTC
# mode: r
+snp_chrom <- snp_list$chromosome[snp_list]

# time: 2024-08-27 16:12:17 UTC
# mode: r
+snp_pos <- snp_list$position[snp_list]

# time: 2024-08-27 16:12:30 UTC
# mode: r
+snp_list <- SNPRelate::snpgdsSNPList(all.maf)

# time: 2024-08-27 16:12:32 UTC
# mode: r
+snp_list

# time: 2024-08-27 16:14:11 UTC
# mode: r
+snp_list$rs.id

# time: 2024-08-27 16:14:32 UTC
# mode: r
+snpset.id <- unlist(all.maf)
+snp_list <- SNPRelate::snpgdsSNPList(all.maf)
+snp_rs_id <- snp_list$snp.id
+snp_chrom <- snp_list$chromosome
+snp_pos <- snp_list$position

# time: 2024-08-27 16:14:36 UTC
# mode: r
+# Create dataframe with one SNP per row
+snp_df <- data.frame(snp_rs_id, snp_chrom, snp_pos)

# time: 2024-08-27 16:14:42 UTC
# mode: r
+# Transpose SNP loading table
+snp_loadings_transposed <- t(snp_loadings$snploading)

# time: 2024-08-27 16:14:46 UTC
# mode: r
+# Match SNPs with loading vectors
+loading_mat <- data.frame(snp_df, snp_loadings_transposed[, 1:num.eigenvectors])

# time: 2024-08-27 16:15:10 UTC
# mode: r
+# Match SNPs with loading vectors
+loading_mat <- data.frame(snp_df, snp_loadings_transposed)

# time: 2024-08-27 16:15:28 UTC
# mode: r
+# Match SNPs with loading vectors
+loading_mat <- data.frame(snp_df, snp_loadings_transposed[, 1:num.eigenvectors])

# time: 2024-08-27 16:15:44 UTC
# mode: r
+snp_df

# time: 2024-08-27 16:15:51 UTC
# mode: r
+head(snp_df)

# time: 2024-08-27 16:16:22 UTC
# mode: r
+# Match SNPs with loading vectors
+loading_mat <- data.frame(snp_df, snp_loadings_transposed[, 1:num.eigenvectors])

# time: 2024-08-27 16:16:29 UTC
# mode: r
+snp_loadings_transposed

# time: 2024-08-27 16:16:40 UTC
# mode: r
+length(snp_loadings_transposed)

# time: 2024-08-27 16:16:44 UTC
# mode: r
+# Match SNPs with loading vectors
+loading_mat <- data.frame(snp_df, snp_loadings_transposed[, 1:num.eigenvectors])

# time: 2024-08-27 16:17:43 UTC
# mode: r
+head(tab)

# time: 2024-08-27 16:18:09 UTC
# mode: r
+snp_list

# time: 2024-08-27 16:18:17 UTC
# mode: r
+head(tab)

# time: 2024-08-27 16:21:40 UTC
# mode: r
+snp_list$snp.id

# time: 2024-08-27 16:21:54 UTC
# mode: r
+tab

# time: 2024-08-27 16:22:32 UTC
# mode: r
+snp_rs_id <- snp_list$snp.id[pca$snp.id]

# time: 2024-08-27 16:22:39 UTC
# mode: r
+snp_pos <- snp_list$position[pca$snp.id]

# time: 2024-08-27 16:22:41 UTC
# mode: r
+snp_chrom <- snp_list$chromosome[pca$snp.id]

# time: 2024-08-27 16:22:44 UTC
# mode: r
+snp_list$chromosome[pca$snp.id]

# time: 2024-08-27 16:22:49 UTC
# mode: r
+# Create dataframe with one SNP per row
+snp_df <- data.frame(snp_rs_id, snp_chrom, snp_pos)

# time: 2024-08-27 16:22:51 UTC
# mode: r
+# Transpose SNP loading table
+snp_loadings_transposed <- t(snp_loadings$snploading)

# time: 2024-08-27 16:22:52 UTC
# mode: r
+# Match SNPs with loading vectors
+loading_mat <- data.frame(snp_df, snp_loadings_transposed[, 1:num.eigenvectors])

# time: 2024-08-27 16:23:04 UTC
# mode: r
+# Create data frames, where sample is paired with eigenvectors
+train_eigenvects <- data.frame(pca$sample.id, pca$eigenvect[, 1:num.eigenvectors])

# time: 2024-08-27 16:23:04 UTC
# mode: r
+test_eigenvects <- data.frame(sample_loadings$sample.id, sample_loadings$eigenvect[, 1:num.eigenvectors])

# time: 2024-08-27 16:23:10 UTC
# mode: r
+# Calculate sample loadings for testing set using SNP loadings
+sample_loadings <- SNPRelate::snpgdsPCASampLoading(snp_loadings, all.maf, sample.id = test.id, num.thread = num.threads)

# time: 2024-08-27 16:23:15 UTC
# mode: r
+# Create data frames, where sample is paired with eigenvectors
+train_eigenvects <- data.frame(pca$sample.id, pca$eigenvect[, 1:num.eigenvectors])

# time: 2024-08-27 16:23:15 UTC
# mode: r
+test_eigenvects <- data.frame(sample_loadings$sample.id, sample_loadings$eigenvect[, 1:num.eigenvectors])

# time: 2024-08-27 16:23:17 UTC
# mode: r
+# Ensure sample.id column is same for all data frames
+colnames(train_eigenvects)[1] <- "sample.id"

# time: 2024-08-27 16:23:17 UTC
# mode: r
+colnames(test_eigenvects)[1] <- "sample.id"

# time: 2024-08-27 16:23:18 UTC
# mode: r
+colnames(sample.info)[1] <- "sample.id"

# time: 2024-08-27 16:23:19 UTC
# mode: r
+# Merge data frames
+train_df <- merge(sample.info, train_eigenvects)

# time: 2024-08-27 16:23:21 UTC
# mode: r
+test_df <- merge(sample.info, test_eigenvects)

# time: 2024-08-27 16:24:46 UTC
# mode: r
+train_eigenvects

# time: 2024-08-27 16:26:00 UTC
# mode: r
+all.label <- ifelse(all.id %in% immigrant.id, "immigrant", "resident")

# time: 2024-08-27 16:26:12 UTC
# mode: r
+# Create a dataframe with 'sample.id' and 'pop' columns
+sample.info <- data.frame(sample.id = all.id, pop = all.label)

# time: 2024-08-27 16:26:17 UTC
# mode: r
+sample.info

# time: 2024-08-27 16:26:34 UTC
# mode: r
+# Merge data frames
+train_df <- merge(sample.info, train_eigenvects)

# time: 2024-08-27 16:26:35 UTC
# mode: r
+test_df <- merge(sample.info, test_eigenvects)

# time: 2024-08-27 16:26:36 UTC
# mode: r
+train_df

# time: 2024-08-27 16:26:50 UTC
# mode: r
+# Check that there is no overlap between training and testing sets
+if (length(intersect(train_df$sample.id, test_df$sample.id)) > 0) {
+    stop("Overlap between training and testing sets")
+}

# time: 2024-08-27 16:27:22 UTC
# mode: r
+train.df

# time: 2024-08-27 16:27:40 UTC
# mode: r
+output.forest <- randomForest(train_df[, 5:9],
+    y = train_df$pop,
+    data = train_df, xtest = test_df[, 5:9], ytest = test_df$pop,
+    ntree = 500, replace = FALSE
+)

# time: 2024-08-27 16:27:57 UTC
# mode: r
+renv::install('randomForest')

# time: 2024-08-27 16:28:02 UTC
# mode: r
+# train random forest using training eigenvectors
+output.forest <- randomForest::randomForest(train_df[, 5:9],
+    y = train_df$pop,
+    data = train_df, xtest = test_df[, 5:9], ytest = test_df$pop,
+    ntree = 500, replace = FALSE
+)

# time: 2024-08-27 16:28:07 UTC
# mode: r
+train_df

# time: 2024-08-27 16:28:40 UTC
# mode: r
+train_df[, 5:9]

# time: 2024-08-27 16:29:22 UTC
# mode: r
+output.forest <- randomForest::randomForest(train_df[, num.eigenvectors-2:2+num.eigenvectors],
+    y = train_df$pop,
+    data = train_df, xtest = test_df[, num.eigenvectors-2:2+num.eigenvectors], ytest = test_df$pop,
+    ntree = 500, replace = FALSE
+)

# time: 2024-08-27 16:29:28 UTC
# mode: r
+train_df[, num.eigenvectors-2:2+num.eigenvectors]

# time: 2024-08-27 16:29:32 UTC
# mode: r
+num.eigenvectors-2

# time: 2024-08-27 16:29:38 UTC
# mode: r
+2+num.eigenvectors

# time: 2024-08-27 16:29:50 UTC
# mode: r
+ncol(train_df)

# time: 2024-08-27 16:30:07 UTC
# mode: r
+train_df[, num.eigenvectors-2:2+num.eigenvectors]

# time: 2024-08-27 16:30:34 UTC
# mode: r
+# train random forest using training eigenvectors
+output.forest <- randomForest::randomForest(train_df[, (num.eigenvectors-2):(2+num.eigenvectors)],
+    y = train_df$pop,
+    data = train_df, xtest = test_df[, (num.eigenvectors-2):(2+num.eigenvectors)], ytest = test_df$pop,
+    ntree = 500, replace = FALSE
+)

# time: 2024-08-27 16:30:58 UTC
# mode: r
+train_df$pop

# time: 2024-08-27 16:31:04 UTC
# mode: r
+rain_df[, (num.eigenvectors - 2):(2 + num.eigenvectors)]

# time: 2024-08-27 16:31:09 UTC
# mode: r
+train_df[, (num.eigenvectors - 2):(2 + num.eigenvectors)]

# time: 2024-08-27 16:31:32 UTC
# mode: r
+test_df

# time: 2024-08-27 16:31:35 UTC
# mode: r
+test_df[, (num.eigenvectors - 2):(2 + num.eigenvectors)]

# time: 2024-08-27 16:31:39 UTC
# mode: r
+output.forest <- randomForest::randomForest(
+    train_df[, (num.eigenvectors - 2):(2 + num.eigenvectors)],
+    y = train_df$pop,
+    data = train_df, 
+    xtest = test_df[, (num.eigenvectors - 2):(2 + num.eigenvectors)], 
+    ytest = test_df$pop,
+    ntree = 500, 
+    replace = FALSE
+)

# time: 2024-08-27 16:31:52 UTC
# mode: r
+test_df$pop

# time: 2024-08-27 16:32:45 UTC
# mode: r
+ncol(train_df[, (num.eigenvectors - 2):(2 + num.eigenvectors)])

# time: 2024-08-27 16:33:09 UTC
# mode: r
+# train random forest using training eigenvectors
+output.forest <- randomForest::randomForest(
+    train_df[, (num.eigenvectors - 2):(2 + num.eigenvectors)],
+    y = as.factor(train_df$pop),
+    data = train_df, 
+    xtest = test_df[, (num.eigenvectors - 2):(2 + num.eigenvectors)], 
+    ytest = as.factor(test_df$pop),
+    ntree = 500, 
+    replace = FALSE
+)

# time: 2024-08-27 16:33:15 UTC
# mode: r
+# Print confusion matrix
+confusionMatrix(output.forest$test$predicted, test_df$pop)

# time: 2024-08-27 16:33:29 UTC
# mode: r
+print(output.forest)

# time: 2024-08-27 16:34:17 UTC
# mode: r
+# train random forest using training eigenvectors
+output.forest <- randomForest::randomForest(
+    train_df[, (num.eigenvectors - 2):(2 + num.eigenvectors)],
+    y = as.factor(train_df$pop),
+    data = train_df, 
+    xtest = test_df[, (num.eigenvectors - 2):(2 + num.eigenvectors)], 
+    ytest = as.factor(test_df$pop),
+    ntree = 1000, 
+    replace = FALSE
+)
+print(output.forest)
+
+# Print confusion matrix

# time: 2024-08-27 16:35:16 UTC
# mode: r
+output.forest <- randomForest::randomForest(
+    train_df[, (num.eigenvectors - 2):(2 + num.eigenvectors)],
+    y = as.factor(train_df$pop),
+    data = train_df, 
+    xtest = test_df[, (num.eigenvectors - 2):(2 + num.eigenvectors)], 
+    ytest = as.factor(test_df$pop),
+    ntree = 1000, 
+    replace = FALSE,
+    proximity = TRUE
+)
+print(output.forest)

# time: 2024-08-27 16:36:52 UTC
# mode: r
+renv::install('rfPermute')

# time: 2024-08-27 16:56:48 UTC
# mode: r
+# Print confusion matrix
+rfPermute::confusionMatrix(output.forest)

# time: 2024-08-27 16:58:17 UTC
# mode: r
+rfPermute::plotPredictedProbs(output.forest, bins = 30, plot = TRUE)

# time: 2024-08-27 16:58:45 UTC
# mode: r
+t

# time: 2024-08-27 16:58:48 UTC
# mode: r
+rfPermute::plotPredictedProbs(output.forest, bins = 30, plot = TRUE)

# time: 2024-08-27 16:59:54 UTC
# mode: r
+rfPermute::plotProximity(output.forest, plot = TRUE)

# time: 2024-08-27 17:00:38 UTC
# mode: r
+# Import configuration file
+config <- config::get()
+
+# parameters
+percent.train <- 0.7
+num.threads <- 44
+num.eigenvectors <- 20
+set.seed(555)
+
+# Input files
+immigrant.info <- file.path(config$path$data, "600K_immigrants.fam")
+immigrant.vcf <- file.path(config$path$data, "600K_immigrants.vcf")
+resident.vcf <- file.path(config$path$data, "600K_residents.vcf")
+
+immigrant.gds <- file.path(config$path$data, "immigrant_recode.gds")
+resident.gds <- file.path(config$path$data, "resident_recode.gds")
+all.gds <- file.path(config$path$data, "all_recode.gds")
+
+# remove any existing gds files before creating new ones
+file.remove(list.files(path = config$path$data, pattern = "*.gds$", full.names = TRUE))
+
+SNPRelate::snpgdsVCF2GDS(immigrant.vcf, immigrant.gds, method = "biallelic.only")
+SNPRelate::snpgdsVCF2GDS(resident.vcf, resident.gds, method = "biallelic.only")
+
+# Append the immigrant and resident data
+SNPRelate::snpgdsCombineGeno(c(immigrant.gds, resident.gds), all.gds)
+
+# Load all the data
+all.maf <- SNPRelate::snpgdsOpen(all.gds)
+
+# Load labels
+immigrant.id <- gdsfmt::read.gdsn(gdsfmt::index.gdsn(
+    SNPRelate::snpgdsOpen(immigrant.gds),
+    "sample.id"
+))
+resident.id <- gdsfmt::read.gdsn(gdsfmt::index.gdsn(
+    SNPRelate::snpgdsOpen(resident.gds),
+    "sample.id"
+))
+all.id <- gdsfmt::read.gdsn(gdsfmt::index.gdsn(all.maf, "sample.id"))
+
+# randomly subsample the data so that the number of samples in all from immigrant and resident are the same
+min.samples <- min(length(immigrant.id), length(resident.id))
+immigrant.id <- sample(immigrant.id, min.samples)
+resident.id <- sample(resident.id, min.samples)
+all.id <- c(immigrant.id, resident.id)
+
+# Split the data into training and testing sets
+num.samples <- length(all.id)
+num.train <- round(num.samples * percent.train)
+num.test <- num.samples - num.train
+train.id <- sample(all.id, num.train)
+test.id <- setdiff(all.id, train.id)
+
+# Identify the population code for each sample
+train.label <- ifelse(train.id %in% immigrant.id, "immigrant", "resident")
+test.label <- ifelse(test.id %in% immigrant.id, "immigrant", "resident")
+all.label <- ifelse(all.id %in% immigrant.id, "immigrant", "resident")
+
+# Reduce dimensionality via pca
+num.threads <- 3
+pca <- SNPRelate::snpgdsPCA(all.maf, sample.id = train.id, num.thread = num.threads)
+
+tab <- data.frame(sample.id = pca$sample.id, pop = factor(train.label), EV1 = pca$eigenvect[, 1], EV2 = pca$eigenvect[, 2], stringsAsFactors = FALSE)
+
+pc.percent <- pca$varprop * 100
+lbls <- paste("PC", 1:num.eigenvectors, "\n", format(pc.percent[1:num.eigenvectors], digits = 2), "%", sep = "")
+pairs(pca$eigenvect[, 1:num.eigenvectors], col = tab$pop, labels = lbls, aspect = 1)
+
+
+
+# Calculate SNP loadings
+snp_loadings <- SNPRelate::snpgdsPCASNPLoading(pca, all.maf, num.thread = num.threads)
+
+# Calculate sample loadings for testing set using SNP loadings
+sample_loadings <- SNPRelate::snpgdsPCASampLoading(snp_loadings, all.maf, sample.id = test.id, num.thread = num.threads)
+
+# Extract list of SNPs used in PCA
+snpset.id <- unlist(all.maf)
+snp_list <- SNPRelate::snpgdsSNPList(all.maf)
+snp_rs_id <- snp_list$snp.id[pca$snp.id]
+snp_chrom <- snp_list$chromosome[pca$snp.id]
+snp_pos <- snp_list$position[pca$snp.id]
+
+# Create dataframe with one SNP per row
+snp_df <- data.frame(snp_rs_id, snp_chrom, snp_pos)
+
+# Transpose SNP loading table
+snp_loadings_transposed <- t(snp_loadings$snploading)
+
+# Match SNPs with loading vectors
+loading_mat <- data.frame(snp_df, snp_loadings_transposed[, 1:num.eigenvectors])
+
+# Write loading data frame to file
+
+# Create data frames, where sample is paired with eigenvectors
+train_eigenvects <- data.frame(pca$sample.id, pca$eigenvect[, 1:num.eigenvectors])
+test_eigenvects <- data.frame(sample_loadings$sample.id, sample_loadings$eigenvect[, 1:num.eigenvectors])
+
+# Ensure sample.id column is same for all data frames
+colnames(train_eigenvects)[1] <- "sample.id"
+colnames(test_eigenvects)[1] <- "sample.id"
+
+# Create a dataframe with 'sample.id' and 'pop' columns
+sample.info <- data.frame(sample.id = all.id, pop = all.label)
+
+# Merge data frames
+train_df <- merge(sample.info, train_eigenvects)
+test_df <- merge(sample.info, test_eigenvects)
+
+# Check that there is no overlap between training and testing sets
+if (length(intersect(train_df$sample.id, test_df$sample.id)) > 0) {
+    stop("There is overlap between training and testing sets")
+}
+
+
+# train random forest using training eigenvectors
+output.forest <- randomForest::randomForest(
+    train_df[, (num.eigenvectors - 2):(2 + num.eigenvectors)],
+    y = as.factor(train_df$pop),
+    data = train_df,
+    xtest = test_df[, (num.eigenvectors - 2):(2 + num.eigenvectors)],
+    ytest = as.factor(test_df$pop),
+    ntree = 1000,
+    replace = FALSE,
+    proximity = TRUE
+)
+print(output.forest)
+
+# Print confusion matrix
+rfPermute::confusionMatrix(output.forest)
+
+rfPermute::plotPredictedProbs(output.forest, bins = 30, plot = TRUE)
+
+rfPermute::plotProximity(output.forest, plot = TRUE)

# time: 2024-08-27 17:02:34 UTC
# mode: r
+# Import configuration file
+config <- config::get()
+
+# parameters
+percent.train <- 0.7
+num.threads <- 44
+num.eigenvectors <- 20
+set.seed(555)
+
+# Input files
+immigrant.info <- file.path(config$path$data, "600K_immigrants.fam")
+immigrant.vcf <- file.path(config$path$data, "600K_immigrants.vcf")
+resident.vcf <- file.path(config$path$data, "600K_residents.vcf")
+
+immigrant.gds <- file.path(config$path$data, "immigrant_recode.gds")
+resident.gds <- file.path(config$path$data, "resident_recode.gds")
+all.gds <- file.path(config$path$data, "all_recode.gds")
+
+# remove any existing gds files before creating new ones
+file.remove(list.files(path = config$path$data, pattern = "*.gds$", full.names = TRUE))
+SNPRelate::snpgdsVCF2GDS(immigrant.vcf, immigrant.gds, method = "biallelic.only")
+SNPRelate::snpgdsVCF2GDS(resident.vcf, resident.gds, method = "biallelic.only")

# time: 2024-08-27 17:03:15 UTC
# mode: r
+# Append the immigrant and resident data
+SNPRelate::snpgdsCombineGeno(c(immigrant.gds, resident.gds), all.gds)

# time: 2024-08-27 17:03:26 UTC
# mode: r
+# Load all the data
+all.maf <- SNPRelate::snpgdsOpen(all.gds)

# time: 2024-08-27 17:03:43 UTC
# mode: r
+# Load labels
+immigrant.id <- gdsfmt::read.gdsn(gdsfmt::index.gdsn(
+    SNPRelate::snpgdsOpen(immigrant.gds),
+    "sample.id"
+))

# time: 2024-08-27 17:03:44 UTC
# mode: r
+resident.id <- gdsfmt::read.gdsn(gdsfmt::index.gdsn(
+    SNPRelate::snpgdsOpen(resident.gds),
+    "sample.id"
+))

# time: 2024-08-27 17:04:18 UTC
# mode: r
+# Append the immigrant and resident data
+SNPRelate::snpgdsCombineGeno(c(immigrant.gds, resident.gds), all.gds, allow.duplicate = TRUE)

# time: 2024-08-27 17:04:29 UTC
# mode: r
+# Append the immigrant and resident data
+SNPRelate::snpgdsCombineGeno(c(immigrant.gds, resident.gds), all.gds)

# time: 2024-08-27 17:06:53 UTC
# mode: r
+gdsfmt::closefn.gds(immigrant.gds)

# time: 2024-08-27 17:07:09 UTC
# mode: r
+gdsfmt::closefn.gds(all.maf)

# time: 2024-08-27 17:07:14 UTC
# mode: r
+# Append the immigrant and resident data
+SNPRelate::snpgdsCombineGeno(c(immigrant.gds, resident.gds), all.gds)

# time: 2024-08-27 17:07:49 UTC
# mode: r
+gdsfmt::showfile.gds(closeall = TRUE)

# time: 2024-08-27 17:07:54 UTC
# mode: r
+# Append the immigrant and resident data
+SNPRelate::snpgdsCombineGeno(c(immigrant.gds, resident.gds), all.gds)

# time: 2024-08-27 17:08:17 UTC
# mode: r
+# Load all the data
+all.maf <- SNPRelate::snpgdsOpen(all.gds)
+
+# Load labels
+immigrant.id <- gdsfmt::read.gdsn(gdsfmt::index.gdsn(
+    SNPRelate::snpgdsOpen(immigrant.gds),
+    "sample.id"
+))
+resident.id <- gdsfmt::read.gdsn(gdsfmt::index.gdsn(
+    SNPRelate::snpgdsOpen(resident.gds),
+    "sample.id"
+))
+
+all.id <- gdsfmt::read.gdsn(gdsfmt::index.gdsn(all.maf, "sample.id"))
+
+# randomly subsample the data so that the number of samples in all from immigrant and resident are the same
+min.samples <- min(length(immigrant.id), length(resident.id))
+immigrant.id <- sample(immigrant.id, min.samples)
+resident.id <- sample(resident.id, min.samples)
+all.id <- c(immigrant.id, resident.id)
+
+# Split the data into training and testing sets
+num.samples <- length(all.id)
+num.train <- round(num.samples * percent.train)
+num.test <- num.samples - num.train
+train.id <- sample(all.id, num.train)
+test.id <- setdiff(all.id, train.id)
+
+# Identify the population code for each sample
+train.label <- ifelse(train.id %in% immigrant.id, "immigrant", "resident")
+test.label <- ifelse(test.id %in% immigrant.id, "immigrant", "resident")
+all.label <- ifelse(all.id %in% immigrant.id, "immigrant", "resident")
+
+# Reduce dimensionality via pca
+num.threads <- 3
+pca <- SNPRelate::snpgdsPCA(all.maf, sample.id = train.id, num.thread = num.threads)
+
+tab <- data.frame(sample.id = pca$sample.id, pop = factor(train.label), EV1 = pca$eigenvect[, 1], EV2 = pca$eigenvect[, 2], stringsAsFactors = FALSE)
+
+pc.percent <- pca$varprop * 100
+lbls <- paste("PC", 1:num.eigenvectors, "\n", format(pc.percent[1:num.eigenvectors], digits = 2), "%", sep = "")
+pairs(pca$eigenvect[, 1:num.eigenvectors], col = tab$pop, labels = lbls, aspect = 1)
+
+
+
+# Calculate SNP loadings
+snp_loadings <- SNPRelate::snpgdsPCASNPLoading(pca, all.maf, num.thread = num.threads)
+
+# Calculate sample loadings for testing set using SNP loadings
+sample_loadings <- SNPRelate::snpgdsPCASampLoading(snp_loadings, all.maf, sample.id = test.id, num.thread = num.threads)
+
+# Extract list of SNPs used in PCA
+snpset.id <- unlist(all.maf)
+snp_list <- SNPRelate::snpgdsSNPList(all.maf)
+snp_rs_id <- snp_list$snp.id[pca$snp.id]
+snp_chrom <- snp_list$chromosome[pca$snp.id]
+snp_pos <- snp_list$position[pca$snp.id]
+
+# Create dataframe with one SNP per row
+snp_df <- data.frame(snp_rs_id, snp_chrom, snp_pos)
+
+# Transpose SNP loading table
+snp_loadings_transposed <- t(snp_loadings$snploading)
+
+# Match SNPs with loading vectors
+loading_mat <- data.frame(snp_df, snp_loadings_transposed[, 1:num.eigenvectors])
+
+# Write loading data frame to file
+
+# Create data frames, where sample is paired with eigenvectors
+train_eigenvects <- data.frame(pca$sample.id, pca$eigenvect[, 1:num.eigenvectors])
+test_eigenvects <- data.frame(sample_loadings$sample.id, sample_loadings$eigenvect[, 1:num.eigenvectors])
+
+# Ensure sample.id column is same for all data frames
+colnames(train_eigenvects)[1] <- "sample.id"
+colnames(test_eigenvects)[1] <- "sample.id"
+
+# Create a dataframe with 'sample.id' and 'pop' columns
+sample.info <- data.frame(sample.id = all.id, pop = all.label)
+
+# Merge data frames
+train_df <- merge(sample.info, train_eigenvects)
+test_df <- merge(sample.info, test_eigenvects)
+
+# Check that there is no overlap between training and testing sets
+if (length(intersect(train_df$sample.id, test_df$sample.id)) > 0) {
+    stop("There is overlap between training and testing sets")
+}
+
+
+# train random forest using training eigenvectors
+output.forest <- randomForest::randomForest(
+    train_df[, (num.eigenvectors - 2):(2 + num.eigenvectors)],
+    y = as.factor(train_df$pop),
+    data = train_df,
+    xtest = test_df[, (num.eigenvectors - 2):(2 + num.eigenvectors)],
+    ytest = as.factor(test_df$pop),
+    ntree = 1000,
+    replace = FALSE,
+    proximity = TRUE
+)
+print(output.forest)
+
+# Print confusion matrix
+rfPermute::confusionMatrix(output.forest)
+
+rfPermute::plotPredictedProbs(output.forest, bins = 30, plot = TRUE)
+
+rfPermute::plotProximity(output.forest, plot = TRUE)

# time: 2024-08-27 17:09:08 UTC
# mode: r
+# train random forest using training eigenvectors
+output.forest <- randomForest::randomForest(
+    train_df[, (num.eigenvectors - 2):(2 + num.eigenvectors)],
+    y = as.factor(train_df$pop),
+    data = train_df,
+    xtest = test_df[, (num.eigenvectors - 2):(2 + num.eigenvectors)],
+    ytest = as.factor(test_df$pop),
+    ntree = 1000,
+    replace = FALSE,
+    proximity = TRUE
+)
+print(output.forest)
+
+# Print confusion matrix
+rfPermute::confusionMatrix(output.forest)

# time: 2024-08-27 17:09:19 UTC
# mode: r
+# Print confusion matrix
+rfPermute::confusionMatrix(output.forest)

# time: 2024-08-27 17:10:05 UTC
# mode: r
+# train random forest using training eigenvectors
+output.forest <- randomForest::randomForest(
+    train_df[, (num.eigenvectors - 2):(2 + num.eigenvectors)],
+    y = as.factor(train_df$pop),
+    data = train_df,
+    xtest = test_df[, (num.eigenvectors - 2):(2 + num.eigenvectors)],
+    ytest = as.factor(test_df$pop),
+    ntree = 1000,
+    replace = FALSE,
+    proximity = TRUE
+)
+print(output.forest)
+
+# Print confusion matrix
+rfPermute::confusionMatrix(output.forest)

# time: 2024-08-27 17:10:28 UTC
# mode: r
+rfPermute::plotPredictedProbs(output.forest, bins = 30, plot = TRUE)

# time: 2024-08-27 17:10:41 UTC
# mode: r
+rfPermute::plotPredictedProbs(output.forest, bins = 20, plot = TRUE)

# time: 2024-08-27 17:11:01 UTC
# mode: r
+rfPermute::plotProximity(output.forest, plot = TRUE)

# time: 2024-08-27 17:11:19 UTC
# mode: r
+num.eigenvectors

# time: 2024-08-27 17:13:53 UTC
# mode: r
+rfPermute::summary(output.forest)

# time: 2024-08-27 17:13:58 UTC
# mode: r
+summary(output.forest)

# time: 2024-08-27 17:14:31 UTC
# mode: r
+# train random forest using training eigenvectors
+output.forest <- randomForest::randomForest(
+    train_df[, (num.eigenvectors - 2):(2 + num.eigenvectors)],
+    y = as.factor(train_df$pop),
+    data = train_df,
+    xtest = test_df[, (num.eigenvectors - 2):(2 + num.eigenvectors)],
+    ytest = as.factor(test_df$pop),
+    ntree = 270,
+    replace = FALSE,
+    proximity = TRUE
+)
+print(output.forest)
+
+# Print confusion matrix
+rfPermute::confusionMatrix(output.forest)

# time: 2024-08-27 17:15:29 UTC
# mode: r
+# train random forest using training eigenvectors
+output.forest <- randomForest::randomForest(
+    train_df[, (num.eigenvectors - 2):(2 + num.eigenvectors)],
+    y = as.factor(train_df$pop),
+    data = train_df,
+    xtest = test_df[, (num.eigenvectors - 2):(2 + num.eigenvectors)],
+    ytest = as.factor(test_df$pop),
+    ntree = 2000,
+    replace = FALSE,
+    proximity = TRUE
+)
+print(output.forest)

# time: 2024-08-27 17:15:35 UTC
# mode: r
+summary(output.forest)

# time: 2024-08-27 17:15:58 UTC
# mode: r
+# train random forest using training eigenvectors
+output.forest <- randomForest::randomForest(
+    train_df[, (num.eigenvectors - 2):(2 + num.eigenvectors)],
+    y = as.factor(train_df$pop),
+    data = train_df,
+    xtest = test_df[, (num.eigenvectors - 2):(2 + num.eigenvectors)],
+    ytest = as.factor(test_df$pop),
+    ntree = 2000,
+    replace = FALSE,
+    proximity = TRUE
+)

# time: 2024-08-27 17:16:02 UTC
# mode: r
+summary(output.forest)

# time: 2024-08-27 17:17:33 UTC
# mode: r
+# Print confusion matrix
+rfPermute::confusionMatrix(output.forest)

# time: 2024-08-27 17:19:10 UTC
# mode: r
+summary(output.forest)

# time: 2024-08-27 17:19:23 UTC
# mode: r
+rfPermute::plotProximity(output.forest)

# time: 2024-08-27 17:19:36 UTC
# mode: r
+xt = summary(output.forest)

# time: 2024-08-27 17:19:39 UTC
# mode: r
+xt

# time: 2024-08-27 17:19:44 UTC
# mode: r
+xt.data

# time: 2024-08-27 17:19:52 UTC
# mode: r
+xt$data

# time: 2024-08-27 17:20:05 UTC
# mode: r
+as.data.frame(xt$data)

# time: 2024-08-27 17:20:28 UTC
# mode: r
+dplyr::as_tibble(xt$data)

# time: 2024-08-27 17:21:03 UTC
# mode: r
+oob_df = dplyr::as_tibble(xt$data)

# time: 2024-08-27 17:21:03 UTC
# mode: r
+# plot using ggplot2
+ggplot2::ggplot(oob_df, ggplot2::aes(x = factor(truth), y = factor(predicted))) +
+    ggplot2::geom_tile(ggplot2::aes(fill = Freq), colour = "white") +
+    ggplot2::scale_fill_gradient(low = "white", high = "steelblue") +
+    ggplot2::theme_minimal() +
+    ggplot2::labs(x = "Truth", y = "Predicted", fill = "Frequency") +
+    ggplot2::theme(axis.text.x = ggplot2::element_text(angle = 45, hjust = 1))

# time: 2024-08-27 17:21:36 UTC
# mode: r
+# plot using ggplot2.
+# data looks like # A tibble: 6,000 × 3
+#    trees class error
+#    <int> <fct> <dbl>
+#  1     1 OOB    54.4
+#  2     2 OOB    55.9
+#  3     3 OOB    56.4
+#  4     4 OOB    58.7
+#  5     5 OOB    57.3
+#  6     6 OOB    59.0
+ggplot2::ggplot(oob_df, ggplot2::aes(x = trees, y = error, color = class)) +
+    ggplot2::geom_line() +
+    ggplot2::theme_minimal() +
+    ggplot2::labs(title = "OOB error rate", x = "Number of trees", y = "Error rate") +
+    ggplot2::scale_color_manual(values = c("OOB" = "red", "Test" = "blue"))

# time: 2024-08-27 17:21:54 UTC
# mode: r
+oob_df

# time: 2024-08-27 17:22:04 UTC
# mode: r
+unique(oob_df$class)

# time: 2024-08-27 17:22:18 UTC
# mode: r
+# plot using ggplot2.
+# data looks like # A tibble: 6,000 × 3
+#    trees class error
+#    <int> <fct> <dbl>
+#  1     1 OOB    54.4
+#  2     2 OOB    55.9
+#  3     3 OOB    56.4
+#  4     4 OOB    58.7
+#  5     5 OOB    57.3
+#  6     6 OOB    59.0
+ggplot2::ggplot(oob_df, ggplot2::aes(x = trees, y = error, color = class)) +
+    ggplot2::geom_line() +
+    ggplot2::theme_minimal() +
+    ggplot2::labs(title = "OOB error rate", x = "Number of trees", y = "Error rate") +
+    ggplot2::scale_color_manual(values = c(
+        "OOB" = "red",
+        "immigrant" = "blue",
+        "resident" = "green"
+    ))

# time: 2024-08-27 17:22:49 UTC
# mode: r
+# plot using ggplot2.
+# data looks like # A tibble: 6,000 × 3
+#    trees class error
+#    <int> <fct> <dbl>
+#  1     1 OOB    54.4
+#  2     2 OOB    55.9
+#  3     3 OOB    56.4
+#  4     4 OOB    58.7
+#  5     5 OOB    57.3
+#  6     6 OOB    59.0
+ggplot2::ggplot(oob_df, ggplot2::aes(x = trees, y = error, color = class)) +
+    ggplot2::geom_line() +
+    ggplot2::theme_minimal() +
+    ggplot2::labs(title = "OOB error rate", x = "Number of trees", y = "Error rate") +
+    ggplot2::scale_color_manual(values = c(
+        "OOB" = "#4e4e4e",
+        "immigrant" = "#5d8566",
+        "resident" = "#c29007"
+    ))

# time: 2024-08-27 17:23:15 UTC
# mode: r
+# plot using ggplot2.
+# data looks like # A tibble: 6,000 × 3
+#    trees class error
+#    <int> <fct> <dbl>
+#  1     1 OOB    54.4
+#  2     2 OOB    55.9
+#  3     3 OOB    56.4
+#  4     4 OOB    58.7
+#  5     5 OOB    57.3
+#  6     6 OOB    59.0
+ggplot2::ggplot(oob_df, ggplot2::aes(x = trees, y = error, color = class)) +
+    ggplot2::geom_line() +
+    ggplot2::theme_minimal() +
+    ggplot2::labs(title = "OOB error rate", x = "Number of trees", y = "Error rate") +
+    ggplot2::scale_color_manual(values = c(
+        "OOB" = "#4e4e4e",
+        "immigrant" = "#5d8566",
+        "resident" = "#c29007"
+    )) +
+    # set y axis limits between 0 and 100
+    ggplot2::coord_cartesian(ylim = c(0, 100))

# time: 2024-08-27 17:24:21 UTC
# mode: r
+ggplot2::ggplot(oob_df, ggplot2::aes(x = trees, y = error, color = class)) +
+    ggplot2::geom_line() +
+    ggplot2::theme_minimal() +
+    ggplot2::labs(title = "OOB error rate", x = "Number of trees", y = "Error rate") +
+    ggplot2::scale_color_manual(values = c(
+        "OOB" = "#4e4e4e",
+        "immigrant" = "#5d8566",
+        "resident" = "#c29007"
+    )) +
+    # set y axis limits between 0 and 100
+    ggplot2::coord_cartesian(ylim = c(0, 100)) +
+    # line at 50% error rate
+    ggplot2::geom_hline(yintercept = 50, linetype = "dashed", color = "red")

# time: 2024-08-27 17:24:40 UTC
# mode: r
+# aspect ratio of 1
+    ggplot2::coord_fixed(ratio = 1)

# time: 2024-08-27 17:24:52 UTC
# mode: r
+ggplot2::ggplot(oob_df, ggplot2::aes(x = trees, y = error, color = class)) +
+    ggplot2::geom_line() +
+    ggplot2::theme_minimal() +
+    ggplot2::labs(title = "OOB error rate", x = "Number of trees", y = "Error rate") +
+    ggplot2::scale_color_manual(values = c(
+        "OOB" = "#4e4e4e",
+        "immigrant" = "#5d8566",
+        "resident" = "#c29007"
+    )) +
+    # set y axis limits between 0 and 100
+    ggplot2::coord_cartesian(ylim = c(0, 100)) +
+    # line at 50% error rate
+    ggplot2::geom_hline(yintercept = 50, linetype = "dashed", color = "red") +
+    # aspect ratio of 1
+    ggplot2::coord_fixed(ratio = 1)

# time: 2024-08-27 17:25:13 UTC
# mode: r
+ggplot2::ggplot(oob_df, ggplot2::aes(x = trees, y = error, color = class)) +
+    ggplot2::geom_line() +
+    ggplot2::theme_minimal() +
+    ggplot2::labs(title = "OOB error rate", x = "Number of trees", y = "Error rate") +
+    ggplot2::scale_color_manual(values = c(
+        "OOB" = "#4e4e4e",
+        "immigrant" = "#5d8566",
+        "resident" = "#c29007"
+    )) +
+    # set y axis limits between 0 and 100
+    ggplot2::coord_cartesian(ylim = c(0, 100)) +
+    # line at 50% error rate
+    ggplot2::geom_hline(yintercept = 50, linetype = "dashed", color = "red") +
+    # aspect ratio of 1
+    ggplot::theme(aspect.ratio = 1)

# time: 2024-08-27 17:25:18 UTC
# mode: r
+ggplot2::ggplot(oob_df, ggplot2::aes(x = trees, y = error, color = class)) +
+    ggplot2::geom_line() +
+    ggplot2::theme_minimal() +
+    ggplot2::labs(title = "OOB error rate", x = "Number of trees", y = "Error rate") +
+    ggplot2::scale_color_manual(values = c(
+        "OOB" = "#4e4e4e",
+        "immigrant" = "#5d8566",
+        "resident" = "#c29007"
+    )) +
+    # set y axis limits between 0 and 100
+    ggplot2::coord_cartesian(ylim = c(0, 100)) +
+    # line at 50% error rate
+    ggplot2::geom_hline(yintercept = 50, linetype = "dashed", color = "red") +
+    # aspect ratio of 1
+    ggplot2::theme(aspect.ratio = 1)

# time: 2024-08-27 17:25:31 UTC
# mode: r
+ggplot2::ggplot(oob_df, ggplot2::aes(x = trees, y = error, color = class)) +
+    ggplot2::geom_line() +
+    ggplot2::theme_minimal() +
+    ggplot2::labs(title = "OOB error rate", x = "Number of trees", y = "Error rate") +
+    ggplot2::scale_color_manual(values = c(
+        "OOB" = "#4e4e4e",
+        "immigrant" = "#5d8566",
+        "resident" = "#c29007"
+    )) +
+    # set y axis limits between 0 and 100
+    ggplot2::coord_cartesian(ylim = c(0, 100)) +
+    # line at 50% error rate
+    ggplot2::geom_hline(yintercept = 50, linetype = "dashed", color = "red") +
+    # aspect ratio of 1
+    ggplot2::theme(aspect.ratio = 1, # text size
+                   text = ggplot2::element_text(size = 12))

# time: 2024-08-27 17:27:39 UTC
# mode: r
+# Train 100 random forests with randomized labels
+random_accuracy <- replicate(100, {
+    # Randomize labels
+    train_df$pop <- sample(train_df$pop)
+    
+    # Train random forest
+    random_forest <- randomForest::randomForest(
+        train_df[, (num.eigenvectors - 2):(2 + num.eigenvectors)],
+        y = as.factor(train_df$pop),
+        data = train_df,
+        ntree = 2000,
+        replace = FALSE,
+        proximity = TRUE
+    )
+    
+    # Return accuracy
+    rfPermute::confusionMatrix(random_forest)$overall["Accuracy"]
+})
+
+# Print average random accuracy
+mean_random_accuracy <- mean(random_accuracy)
+print(paste("Average random accuracy:", mean_random_accuracy))
+# Print confusion matrix

# time: 2024-08-27 17:27:49 UTC
# mode: r
+random_forest

# time: 2024-08-27 17:27:53 UTC
# mode: r
+# Train 100 random forests with randomized labels
+random_accuracy <- replicate(100, {
+    # Randomize labels
+    train_df$pop <- sample(train_df$pop)
+    
+    # Train random forest
+    random_forest <- randomForest::randomForest(
+        train_df[, (num.eigenvectors - 2):(2 + num.eigenvectors)],
+        y = as.factor(train_df$pop),
+        data = train_df,
+        ntree = 2000,
+        replace = FALSE,
+        proximity = TRUE
+    )
+    
+
+})

# time: 2024-08-27 17:29:12 UTC
# mode: r
+random_accuracy

# time: 2024-08-27 17:31:01 UTC
# mode: r
+# for each of the forests in random_accuracy, get the summary and add to a dataframe
+random_accuracy_df <- dplyr::bind_rows(
+    lapply(random_accuracy, function(forest) {
+        xt <- summary(forest)
+        dplyr::as_tibble(xt$data)
+    })
+)

# time: 2024-08-27 17:31:23 UTC
# mode: r
+random_accuracy[1]

# time: 2024-08-27 17:31:47 UTC
# mode: r
+xt

# time: 2024-08-27 17:31:58 UTC
# mode: r
+# for each of the forests in random_accuracy, get the summary and add to a dataframe
+random_accuracy_df <- dplyr::bind_rows(
+    lapply(random_accuracy, function(forest) {
+        d <- summary(forest)
+        dplyr::as_tibble(d$data)
+    })
+)

# time: 2024-08-27 17:32:11 UTC
# mode: r
+d

# time: 2024-08-27 17:32:13 UTC
# mode: r
+# for each of the forests in random_accuracy, get the summary and add to a dataframe
+random_accuracy_df <- dplyr::bind_rows(
+    lapply(random_accuracy, function(forest) {
+        d <- summary(forest)
+        dplyr::as_tibble(d$data)
+    })
+)

# time: 2024-08-27 17:32:17 UTC
# mode: r
+d

# time: 2024-08-27 17:32:25 UTC
# mode: r
+# for each of the forests in random_accuracy, get the summary and add to a dataframe
+random_accuracy_df <- dplyr::bind_rows(
+    lapply(random_accuracy, function(forest) {
+        print(d)
+    })
+)

# time: 2024-08-27 17:32:42 UTC
# mode: r
+# for each of the forests in random_accuracy, get the summary and add to a dataframe
+random_accuracy_df <- dplyr::bind_rows(
+    lapply(random_accuracy, function(forest) {
+        print(summary(d))
+    })
+)

# time: 2024-08-27 17:32:50 UTC
# mode: r
+# for each of the forests in random_accuracy, get the summary and add to a dataframe
+random_accuracy_df <- dplyr::bind_rows(
+    lapply(random_accuracy, function(forest) {
+        print(summary(forest))
+    })
+)

# time: 2024-08-27 17:33:17 UTC
# mode: r
+# for each of the forests in random_accuracy, get the summary and add to a dataframe
+random_accuracy_df <- dplyr::bind_rows(
+    lapply(random_accuracy, function(forest) {
+        d = summary(forest)
+        d$data
+    })
+)

# time: 2024-08-27 17:33:34 UTC
# mode: r
+# for each of the forests in random_accuracy, get the summary and add to a dataframe
+random_accuracy_df <- dplyr::bind_rows(
+    lapply(random_accuracy, function(forest) {
+        d = summary(forest)$data
+    })
+)

# time: 2024-08-27 17:33:51 UTC
# mode: r
+# for each of the forests in random_accuracy, get the summary and add to a dataframe
+random_accuracy_df <- dplyr::bind_rows(
+    lapply(random_accuracy, function(forest) {
+        d = summary(forest)
+        print(d)
+    })
+)

# time: 2024-08-27 17:34:33 UTC
# mode: r
+random_accuracy[1]

# time: 2024-08-27 17:34:48 UTC
# mode: r
+# Train 100 random forests with randomized labels
+random_accuracy <- replicate(100, {
+    # Randomize labels
+    train_df$pop <- sample(train_df$pop)
+    
+    # Train random forest
+    random_forest <- randomForest::randomForest(
+        train_df[, (num.eigenvectors - 2):(2 + num.eigenvectors)],
+        y = as.factor(train_df$pop),
+        data = train_df,
+        ntree = 2000,
+        replace = FALSE,
+        proximity = TRUE
+    )
+
+    random_forest
+    
+
+})

# time: 2024-08-27 17:35:07 UTC
# mode: r
+# Train 100 random forests with randomized labels
+random_accuracy <- replicate(50, {
+    # Randomize labels
+    train_df$pop <- sample(train_df$pop)
+    
+    # Train random forest
+    random_forest <- randomForest::randomForest(
+        train_df[, (num.eigenvectors - 2):(2 + num.eigenvectors)],
+        y = as.factor(train_df$pop),
+        data = train_df,
+        ntree = 1000,
+        replace = FALSE,
+        proximity = TRUE
+    )
+
+    random_forest
+    
+
+})

# time: 2024-08-27 17:35:31 UTC
# mode: r
+random_accuracy[1]

# time: 2024-08-27 17:35:36 UTC
# mode: r
+random_accuracy[[1]]

# time: 2024-08-27 17:35:57 UTC
# mode: r
+summary(random_accuracy[[1]])

# time: 2024-08-27 17:37:14 UTC
# mode: r
+a

# time: 2024-08-27 17:37:16 UTC
# mode: r
+# Train 100 random forests with randomized labels
+random_accuracy <- replicate(50, {
+    # Randomize labels
+    train_df$pop <- sample(train_df$pop)
+    
+    # Train random forest
+    random_forest <- randomForest::randomForest(
+        train_df[, (num.eigenvectors - 2):(2 + num.eigenvectors)],
+        y = as.factor(train_df$pop),
+        data = train_df,
+        ntree = 1000,
+        replace = FALSE,
+        proximity = TRUE
+    )
+
+    return(random_forest)
+    
+
+})

# time: 2024-08-27 17:37:35 UTC
# mode: r
+print(random_accuracy[1])

# time: 2024-08-27 17:37:43 UTC
# mode: r
+print(random_accuracy[[1]])

# time: 2024-08-27 17:37:50 UTC
# mode: r
+print(random_accuracy[[1]]$confusion)

# time: 2024-08-27 17:40:41 UTC
# mode: r
+# Train 100 random forests with randomized labels
+random_accuracy <- replicate(50, {
+    # Randomize labels
+    train_df$pop <- sample(train_df$pop)
+
+    # Train random forest
+    random_forest <- randomForest::randomForest(
+        train_df[, (num.eigenvectors - 2):(2 + num.eigenvectors)],
+        y = as.factor(train_df$pop),
+        data = train_df,
+        xtest = test_df[, (num.eigenvectors - 2):(2 + num.eigenvectors)],
+        ytest = as.factor(test_df$pop),
+        ntree = 1000,
+        replace = FALSE,
+        proximity = TRUE
+    )
+
+    return(random_forest)
+})

# time: 2024-08-27 17:42:00 UTC
# mode: r
+renv::install('progressr')

# time: 2024-08-27 17:42:13 UTC
# mode: r
+handlers(handler_txtprogressbar(char = cli::col_red(cli::symbol$heart)))

# time: 2024-08-27 17:42:28 UTC
# mode: r
+progressr::handlers(progressr::handler_txtprogressbar(char = cli::col_red(cli::symbol$heart)))

# time: 2024-08-27 17:42:45 UTC
# mode: r
+# Train 100 random forests with randomized labels
+random_accuracy <- replicate(50, {
+    # Randomize labels
+    train_df$pop <- sample(train_df$pop)
+
+    # Train random forest
+    random_forest <- randomForest::randomForest(
+        train_df[, (num.eigenvectors - 2):(2 + num.eigenvectors)],
+        y = as.factor(train_df$pop),
+        data = train_df,
+        xtest = test_df[, (num.eigenvectors - 2):(2 + num.eigenvectors)],
+        ytest = as.factor(test_df$pop),
+        ntree = 1000,
+        replace = FALSE,
+        proximity = TRUE
+    )
+
+    progressr::set_progress(progressr::get_progress() + 1)
+    return(random_forest)
+}, .progress = "text")

# time: 2024-08-27 17:42:55 UTC
# mode: r
+# Train 100 random forests with randomized labels
+random_accuracy <- replicate(50, {
+    # Randomize labels
+    train_df$pop <- sample(train_df$pop)
+
+    # Train random forest
+    random_forest <- randomForest::randomForest(
+        train_df[, (num.eigenvectors - 2):(2 + num.eigenvectors)],
+        y = as.factor(train_df$pop),
+        data = train_df,
+        xtest = test_df[, (num.eigenvectors - 2):(2 + num.eigenvectors)],
+        ytest = as.factor(test_df$pop),
+        ntree = 1000,
+        replace = FALSE,
+        proximity = TRUE
+    )
+
+    progressr::set_progress(progressr::get_progress() + 1)
+    return(random_forest)
+})

# time: 2024-08-27 17:44:24 UTC
# mode: r
+# Train 100 random forests with randomized labels
+random_accuracy <- replicate(50, {
+
+    p <- progressr::progressor(along = x)
+
+    # Randomize labels
+    train_df$pop <- sample(train_df$pop)
+
+    # Train random forest
+    random_forest <- randomForest::randomForest(
+        train_df[, (num.eigenvectors - 2):(2 + num.eigenvectors)],
+        y = as.factor(train_df$pop),
+        data = train_df,
+        xtest = test_df[, (num.eigenvectors - 2):(2 + num.eigenvectors)],
+        ytest = as.factor(test_df$pop),
+        ntree = 1000,
+        replace = FALSE,
+        proximity = TRUE
+    )
+    p()
+    return(random_forest)
+})

# time: 2024-08-27 17:44:48 UTC
# mode: r
+# Train 100 random forests with randomized labels
+random_accuracy <- replicate(50, {
+
+    p <- progressr::progressor(steps = length(50))
+
+    # Randomize labels
+    train_df$pop <- sample(train_df$pop)
+
+    # Train random forest
+    random_forest <- randomForest::randomForest(
+        train_df[, (num.eigenvectors - 2):(2 + num.eigenvectors)],
+        y = as.factor(train_df$pop),
+        data = train_df,
+        xtest = test_df[, (num.eigenvectors - 2):(2 + num.eigenvectors)],
+        ytest = as.factor(test_df$pop),
+        ntree = 1000,
+        replace = FALSE,
+        proximity = TRUE
+    )
+    p()
+    return(random_forest)
+})

# time: 2024-08-27 17:45:10 UTC
# mode: r
+# Train 100 random forests with randomized labels
+random_accuracy <- replicate(50, progressr::with_progress({
+    p <- progressr::progressor(steps = length(50))
+
+    # Randomize labels
+    train_df$pop <- sample(train_df$pop)
+
+    # Train random forest
+    random_forest <- randomForest::randomForest(
+        train_df[, (num.eigenvectors - 2):(2 + num.eigenvectors)],
+        y = as.factor(train_df$pop),
+        data = train_df,
+        xtest = test_df[, (num.eigenvectors - 2):(2 + num.eigenvectors)],
+        ytest = as.factor(test_df$pop),
+        ntree = 1000,
+        replace = FALSE,
+        proximity = TRUE
+    )
+    p()
+    return(random_forest)
+}))

# time: 2024-08-27 17:45:24 UTC
# mode: r
+# Train 100 random forests with randomized labels
+random_accuracy <- replicate(50, progressr::with_progress({
+    p <- progressr::progressor(steps = 50)
+
+    # Randomize labels
+    train_df$pop <- sample(train_df$pop)
+
+    # Train random forest
+    random_forest <- randomForest::randomForest(
+        train_df[, (num.eigenvectors - 2):(2 + num.eigenvectors)],
+        y = as.factor(train_df$pop),
+        data = train_df,
+        xtest = test_df[, (num.eigenvectors - 2):(2 + num.eigenvectors)],
+        ytest = as.factor(test_df$pop),
+        ntree = 1000,
+        replace = FALSE,
+        proximity = TRUE
+    )
+    p()
+    return(random_forest)
+}))

# time: 2024-08-27 17:46:32 UTC
# mode: r
+progressr::handlers("cli")

# time: 2024-08-27 17:46:34 UTC
# mode: r
+progressr::handlers(progressr::handler_txtprogressbar(char = cli::col_red(cli::symbol$heart)))

# time: 2024-08-27 17:46:41 UTC
# mode: r
+progressr::handlers("cli")

# time: 2024-08-27 17:46:45 UTC
# mode: r
+# Train 100 random forests with randomized labels
+random_accuracy <- replicate(50, progressr::with_progress({
+    p <- progressr::progressor(steps = length(50))
+
+    # Randomize labels
+    train_df$pop <- sample(train_df$pop)
+
+    # Train random forest
+    random_forest <- randomForest::randomForest(
+        train_df[, (num.eigenvectors - 2):(2 + num.eigenvectors)],
+        y = as.factor(train_df$pop),
+        data = train_df,
+        xtest = test_df[, (num.eigenvectors - 2):(2 + num.eigenvectors)],
+        ytest = as.factor(test_df$pop),
+        ntree = 1000,
+        replace = FALSE,
+        proximity = TRUE
+    )
+    p()
+    return(random_forest)
+}))

# time: 2024-08-27 17:47:34 UTC
# mode: r
+# Train 100 random forests with randomized labels
+random_accuracy <- replicate(50, {
+    # Randomize labels
+    train_df$pop <- sample(train_df$pop)
+
+    # Train random forest
+    random_forest <- randomForest::randomForest(
+        train_df[, (num.eigenvectors - 2):(2 + num.eigenvectors)],
+        y = as.factor(train_df$pop),
+        data = train_df,
+        xtest = test_df[, (num.eigenvectors - 2):(2 + num.eigenvectors)],
+        ytest = as.factor(test_df$pop),
+        ntree = 1000,
+        replace = FALSE,
+        proximity = TRUE
+    )
+
+    return(random_forest)
+})

# time: 2024-08-27 17:48:02 UTC
# mode: r
+# for each of the forests in random_accuracy, get the summary and add to a dataframe
+random_accuracy_df <- dplyr::bind_rows(
+    lapply(random_accuracy, function(forest) {
+        d <- summary(forest)
+        d$data
+    })
+)

# time: 2024-08-27 17:48:11 UTC
# mode: r
+random_accuracy[[1]]

# time: 2024-08-27 17:49:51 UTC
# mode: r
+ggplot2::ggplot(oob_df, ggplot2::aes(x = trees, y = error, color = class)) +
+    ggplot2::geom_line() +
+    ggplot2::theme_minimal() +
+    ggplot2::labs(title = "OOB error rate", x = "Number of trees", y = "Error rate") +
+    ggplot2::scale_color_manual(values = c(
+        "OOB" = "#4e4e4e",
+        "immigrant" = "#5d8566",
+        "resident" = "#c29007"
+    )) +
+    ggplot2::coord_cartesian(ylim = c(0, 100)) +
+    ggplot2::geom_hline(yintercept = 50, linetype = "dashed", color = "red") +
+    ggplot2::theme(
+        aspect.ratio = 1,
+        text = ggplot2::element_text(size = 12)
+    )

# time: 2024-08-27 17:50:31 UTC
# mode: r
+# Print confusion matrix
+rfPermute::confusionMatrix(output.forest)

# time: 2024-08-29 10:34:14 UTC
# mode: r
+# Import configuration file
+config <- config::get()
+
+# parameters
+percent.train <- 0.7
+num.threads <- 44
+num.eigenvectors <- 20
+set.seed(555)
+
+# Input files
+immigrant.info <- file.path(config$path$data, "600K_immigrants.fam")
+immigrant.vcf <- file.path(config$path$data, "600K_immigrants.vcf")
+resident.vcf <- file.path(config$path$data, "600K_residents.vcf")
+
+immigrant.gds <- file.path(config$path$data, "immigrant_recode.gds")
+resident.gds <- file.path(config$path$data, "resident_recode.gds")
+all.gds <- file.path(config$path$data, "all_recode.gds")
+
+# remove any existing gds files before creating new ones
+file.remove(list.files(path = config$path$data, pattern = "*.gds$", full.names = TRUE))
+SNPRelate::snpgdsVCF2GDS(immigrant.vcf, immigrant.gds, method = "biallelic.only")
+SNPRelate::snpgdsVCF2GDS(resident.vcf, resident.gds, method = "biallelic.only")
+gdsfmt::showfile.gds(closeall = TRUE)
+# Append the immigrant and resident data
+SNPRelate::snpgdsCombineGeno(c(immigrant.gds, resident.gds), all.gds)
+
+# Load all the data
+all.maf <- SNPRelate::snpgdsOpen(all.gds)
+
+# Load labels
+immigrant.id <- gdsfmt::read.gdsn(gdsfmt::index.gdsn(
+    SNPRelate::snpgdsOpen(immigrant.gds),
+    "sample.id"
+))
+resident.id <- gdsfmt::read.gdsn(gdsfmt::index.gdsn(
+    SNPRelate::snpgdsOpen(resident.gds),
+    "sample.id"
+))
+
+all.id <- gdsfmt::read.gdsn(gdsfmt::index.gdsn(all.maf, "sample.id"))
+
+# randomly subsample the data so that the number of samples in all from immigrant and resident are the same
+min.samples <- min(length(immigrant.id), length(resident.id))
+immigrant.id <- sample(immigrant.id, min.samples)
+resident.id <- sample(resident.id, min.samples)
+all.id <- c(immigrant.id, resident.id)
+
+# Split the data into training and testing sets
+num.samples <- length(all.id)
+num.train <- round(num.samples * percent.train)
+num.test <- num.samples - num.train
+train.id <- sample(all.id, num.train)
+test.id <- setdiff(all.id, train.id)
+
+# Identify the population code for each sample
+train.label <- ifelse(train.id %in% immigrant.id, "immigrant", "resident")
+test.label <- ifelse(test.id %in% immigrant.id, "immigrant", "resident")
+all.label <- ifelse(all.id %in% immigrant.id, "immigrant", "resident")
+
+# Reduce dimensionality via pca
+num.threads <- 3
+pca <- SNPRelate::snpgdsPCA(all.maf, sample.id = train.id, num.thread = num.threads)
+
+tab <- data.frame(sample.id = pca$sample.id, pop = factor(train.label), EV1 = pca$eigenvect[, 1], EV2 = pca$eigenvect[, 2], stringsAsFactors = FALSE)
+
+pc.percent <- pca$varprop * 100
+lbls <- paste("PC", 1:num.eigenvectors, "\n", format(pc.percent[1:num.eigenvectors], digits = 2), "%", sep = "")
+pairs(pca$eigenvect[, 1:num.eigenvectors], col = tab$pop, labels = lbls, aspect = 1)
+
+
+
+# Calculate SNP loadings
+snp_loadings <- SNPRelate::snpgdsPCASNPLoading(pca, all.maf, num.thread = num.threads)
+
+# Calculate sample loadings for testing set using SNP loadings
+sample_loadings <- SNPRelate::snpgdsPCASampLoading(snp_loadings, all.maf, sample.id = test.id, num.thread = num.threads)
+
+# Extract list of SNPs used in PCA
+snpset.id <- unlist(all.maf)
+snp_list <- SNPRelate::snpgdsSNPList(all.maf)
+snp_rs_id <- snp_list$snp.id[pca$snp.id]
+snp_chrom <- snp_list$chromosome[pca$snp.id]
+snp_pos <- snp_list$position[pca$snp.id]
+
+# Create dataframe with one SNP per row
+snp_df <- data.frame(snp_rs_id, snp_chrom, snp_pos)
+
+# Transpose SNP loading table
+snp_loadings_transposed <- t(snp_loadings$snploading)
+
+# Match SNPs with loading vectors
+loading_mat <- data.frame(snp_df, snp_loadings_transposed[, 1:num.eigenvectors])
+
+# Write loading data frame to file
+
+# Create data frames, where sample is paired with eigenvectors
+train_eigenvects <- data.frame(pca$sample.id, pca$eigenvect[, 1:num.eigenvectors])
+test_eigenvects <- data.frame(sample_loadings$sample.id, sample_loadings$eigenvect[, 1:num.eigenvectors])
+
+# Ensure sample.id column is same for all data frames
+colnames(train_eigenvects)[1] <- "sample.id"
+colnames(test_eigenvects)[1] <- "sample.id"
+
+# Create a dataframe with 'sample.id' and 'pop' columns
+sample.info <- data.frame(sample.id = all.id, pop = all.label)
+
+# Merge data frames
+train_df <- merge(sample.info, train_eigenvects)
+test_df <- merge(sample.info, test_eigenvects)
+
+# Check that there is no overlap between training and testing sets
+if (length(intersect(train_df$sample.id, test_df$sample.id)) > 0) {
+    stop("There is overlap between training and testing sets")
+}
+
+
+# train random forest using training eigenvectors
+output.forest <- randomForest::randomForest(
+    train_df[, (num.eigenvectors - 2):(2 + num.eigenvectors)],
+    y = as.factor(train_df$pop),
+    data = train_df,
+    xtest = test_df[, (num.eigenvectors - 2):(2 + num.eigenvectors)],
+    ytest = as.factor(test_df$pop),
+    ntree = 2000,
+    replace = FALSE,
+    proximity = TRUE
+)
+print(output.forest)

# time: 2024-08-29 12:46:26 UTC
# mode: r
+loading_mat

# time: 2024-08-29 12:46:33 UTC
# mode: r
+snp_loadings

# time: 2024-08-29 12:52:03 UTC
# mode: r
+# Calculate the number of samples from each group
+num_immigrant <- sum(all.label == "immigrant")
+num_resident <- sum(all.label == "resident")
+
+# Calculate the number of samples from each group in the train set
+num_train_immigrant <- round(num_immigrant * percent.train)
+num_train_resident <- round(num_resident * percent.train)
+
+# Calculate the number of samples from each group in the test set
+num_test_immigrant <- num_immigrant - num_train_immigrant
+num_test_resident <- num_resident - num_train_resident
+
+# Create train and test sets with balanced samples from each group
+train_immigrant <- sample(immigrant.id, num_train_immigrant)
+train_resident <- sample(resident.id, num_train_resident)
+test_immigrant <- sample(setdiff(immigrant.id, train_immigrant), num_test_immigrant)
+test_resident <- sample(setdiff(resident.id, train_resident), num_test_resident)
+
+# Combine the train and test sets
+train.id <- c(train_immigrant, train_resident)
+test.id <- c(test_immigrant, test_resident)

# time: 2024-08-29 12:52:05 UTC
# mode: r
+test.id

# time: 2024-08-29 12:52:10 UTC
# mode: r
+# Identify the population code for each sample
+train.label <- ifelse(train.id %in% immigrant.id, "immigrant", "resident")

# time: 2024-08-29 12:52:11 UTC
# mode: r
+test.label <- ifelse(test.id %in% immigrant.id, "immigrant", "resident")

# time: 2024-08-29 12:52:11 UTC
# mode: r
+all.label <- ifelse(all.id %in% immigrant.id, "immigrant", "resident")

# time: 2024-08-29 12:53:17 UTC
# mode: r
+# check the proportion of immigrant and residents in each label variable
+prop_immigrant <- sum(train.label == "immigrant") / length(train.label)
+prop_resident <- sum(train.label == "resident") / length(train.label)
+
+prop_immigrant
+prop_resident

# time: 2024-08-29 14:40:22 UTC
# mode: r
+# Import configuration file
+config <- config::get()
+
+# parameters
+percent.train <- 0.7
+num.threads <- 44
+num.eigenvectors <- 20
+set.seed(555)
+
+# Input files
+immigrant.info <- file.path(config$path$data, "600K_immigrants.fam")
+immigrant.vcf <- file.path(config$path$data, "600K_immigrants.vcf")
+resident.vcf <- file.path(config$path$data, "600K_residents.vcf")
+
+immigrant.gds <- file.path(config$path$data, "immigrant_recode.gds")
+resident.gds <- file.path(config$path$data, "resident_recode.gds")
+all.gds <- file.path(config$path$data, "all_recode.gds")
+
+# remove any existing gds files before creating new ones
+file.remove(list.files(path = config$path$data, pattern = "*.gds$", full.names = TRUE))
+SNPRelate::snpgdsVCF2GDS(immigrant.vcf, immigrant.gds, method = "biallelic.only")
+SNPRelate::snpgdsVCF2GDS(resident.vcf, resident.gds, method = "biallelic.only")
+gdsfmt::showfile.gds(closeall = TRUE)
+# Append the immigrant and resident data
+SNPRelate::snpgdsCombineGeno(c(immigrant.gds, resident.gds), all.gds)
+
+# Load all the data
+all.maf <- SNPRelate::snpgdsOpen(all.gds)
+
+# Load labels
+immigrant.id <- gdsfmt::read.gdsn(gdsfmt::index.gdsn(
+    SNPRelate::snpgdsOpen(immigrant.gds),
+    "sample.id"
+))
+resident.id <- gdsfmt::read.gdsn(gdsfmt::index.gdsn(
+    SNPRelate::snpgdsOpen(resident.gds),
+    "sample.id"
+))
+
+all.id <- gdsfmt::read.gdsn(gdsfmt::index.gdsn(all.maf, "sample.id"))
+
+# randomly subsample the data so that the number of samples in all from immigrant and resident are the same
+min.samples <- min(length(immigrant.id), length(resident.id))
+immigrant.id <- sample(immigrant.id, min.samples)
+resident.id <- sample(resident.id, min.samples)
+all.id <- c(immigrant.id, resident.id)
+
+# Split the data into training and testing sets
+# Calculate the number of samples from each group
+num_immigrant <- sum(all.label == "immigrant")
+num_resident <- sum(all.label == "resident")
+
+# Calculate the number of samples from each group in the train set
+num_train_immigrant <- round(num_immigrant * percent.train)
+num_train_resident <- round(num_resident * percent.train)
+
+# Calculate the number of samples from each group in the test set
+num_test_immigrant <- num_immigrant - num_train_immigrant
+num_test_resident <- num_resident - num_train_resident
+
+# Create train and test sets with balanced samples from each group
+train_immigrant <- sample(immigrant.id, num_train_immigrant)
+train_resident <- sample(resident.id, num_train_resident)
+test_immigrant <- sample(setdiff(immigrant.id, train_immigrant), num_test_immigrant)
+test_resident <- sample(setdiff(resident.id, train_resident), num_test_resident)
+
+# Combine the train and test sets
+train.id <- c(train_immigrant, train_resident)
+test.id <- c(test_immigrant, test_resident)
+
+# Identify the population code for each sample
+train.label <- ifelse(train.id %in% immigrant.id, "immigrant", "resident")
+test.label <- ifelse(test.id %in% immigrant.id, "immigrant", "resident")
+all.label <- ifelse(all.id %in% immigrant.id, "immigrant", "resident")
+
+# check the proportion of immigrant and residents in each label variable
+prop_immigrant <- sum(train.label == "immigrant") / length(train.label)
+prop_resident <- sum(train.label == "resident") / length(train.label)

# time: 2024-08-29 14:42:13 UTC
# mode: r
+all.id <- gdsfmt::read.gdsn(gdsfmt::index.gdsn(all.maf, "sample.id"))
+
+# randomly subsample the data so that the number of samples in all from immigrant and resident are the same
+min.samples <- min(length(immigrant.id), length(resident.id))
+immigrant.id <- sample(immigrant.id, min.samples)
+resident.id <- sample(resident.id, min.samples)
+all.id <- c(immigrant.id, resident.id)
+
+# Split the data into training and testing sets
+# Calculate the number of samples from each group
+num_immigrant <- sum(all.label == "immigrant")
+num_resident <- sum(all.label == "resident")
+all.label <- ifelse(all.id %in% immigrant.id, "immigrant", "resident")
+
+# Calculate the number of samples from each group in the train set
+num_train_immigrant <- round(num_immigrant * percent.train)
+num_train_resident <- round(num_resident * percent.train)
+
+# Calculate the number of samples from each group in the test set
+num_test_immigrant <- num_immigrant - num_train_immigrant
+num_test_resident <- num_resident - num_train_resident
+
+# Create train and test sets with balanced samples from each group
+train_immigrant <- sample(immigrant.id, num_train_immigrant)
+train_resident <- sample(resident.id, num_train_resident)
+test_immigrant <- sample(setdiff(immigrant.id, train_immigrant), num_test_immigrant)
+test_resident <- sample(setdiff(resident.id, train_resident), num_test_resident)
+
+# Combine the train and test sets
+train.id <- c(train_immigrant, train_resident)
+test.id <- c(test_immigrant, test_resident)
+
+# Identify the population code for each sample
+train.label <- ifelse(train.id %in% immigrant.id, "immigrant", "resident")
+test.label <- ifelse(test.id %in% immigrant.id, "immigrant", "resident")
+
+# check the proportion of immigrant and residents in each label variable
+prop_immigrant <- sum(train.label == "immigrant") / length(train.label)
+prop_resident <- sum(train.label == "resident") / length(train.label)

# time: 2024-08-29 14:42:24 UTC
# mode: r
+all.id <- gdsfmt::read.gdsn(gdsfmt::index.gdsn(all.maf, "sample.id"))
+
+# randomly subsample the data so that the number of samples in all from immigrant and resident are the same
+min.samples <- min(length(immigrant.id), length(resident.id))
+immigrant.id <- sample(immigrant.id, min.samples)
+resident.id <- sample(resident.id, min.samples)
+all.id <- c(immigrant.id, resident.id)
+
+# Split the data into training and testing sets
+# Calculate the number of samples from each group
+all.label <- ifelse(all.id %in% immigrant.id, "immigrant", "resident")
+num_immigrant <- sum(all.label == "immigrant")
+num_resident <- sum(all.label == "resident")
+
+# Calculate the number of samples from each group in the train set
+num_train_immigrant <- round(num_immigrant * percent.train)
+num_train_resident <- round(num_resident * percent.train)
+
+# Calculate the number of samples from each group in the test set
+num_test_immigrant <- num_immigrant - num_train_immigrant
+num_test_resident <- num_resident - num_train_resident
+
+# Create train and test sets with balanced samples from each group
+train_immigrant <- sample(immigrant.id, num_train_immigrant)
+train_resident <- sample(resident.id, num_train_resident)
+test_immigrant <- sample(setdiff(immigrant.id, train_immigrant), num_test_immigrant)
+test_resident <- sample(setdiff(resident.id, train_resident), num_test_resident)
+
+# Combine the train and test sets
+train.id <- c(train_immigrant, train_resident)
+test.id <- c(test_immigrant, test_resident)
+
+# Identify the population code for each sample
+train.label <- ifelse(train.id %in% immigrant.id, "immigrant", "resident")
+test.label <- ifelse(test.id %in% immigrant.id, "immigrant", "resident")
+
+# check the proportion of immigrant and residents in each label variable
+prop_immigrant <- sum(train.label == "immigrant") / length(train.label)
+prop_resident <- sum(train.label == "resident") / length(train.label)

# time: 2024-08-29 14:42:25 UTC
# mode: r
+prop_resident

# time: 2024-08-29 14:43:03 UTC
# mode: r
+# now for the test set
+prop_immigrant_test <- sum(test.label == "immigrant") / length(test.label)

# time: 2024-08-29 14:43:04 UTC
# mode: r
+prop_resident_test <- sum(test.label == "resident") / length(test.label)

# time: 2024-08-29 14:43:05 UTC
# mode: r
+prop_immigrant_test

# time: 2024-08-29 14:43:07 UTC
# mode: r
+prop_resident_test

# time: 2024-08-29 14:43:33 UTC
# mode: r
+if (length(intersect(train.id, test.id)) > 0) {
+    stop("There is overlap between training and testing sets")
+}

# time: 2024-08-29 14:43:50 UTC
# mode: r
+test.label

# time: 2024-08-29 14:44:05 UTC
# mode: r
+immigrant.id

# time: 2024-08-29 14:44:09 UTC
# mode: r
+train.id

# time: 2024-08-29 14:46:05 UTC
# mode: r
+# Subsample data to balance immigrant and resident samples
+min_samples <- min(length(immigrant.id), length(resident.id))
+all.id <- c(sample(immigrant.id, min_samples), sample(resident.id, min_samples))
+all.label <- ifelse(all.id %in% immigrant.id, "immigrant", "resident")
+
+# Split data into training and testing sets
+num_train <- round(length(all.id) * percent.train)
+train.id <- sample(all.id, num_train)
+test.id <- setdiff(all.id, train.id)
+
+train.label <- all.label[all.id %in% train.id]
+test.label <- all.label[all.id %in% test.id]
+
+# Check proportions
+prop_train <- table(train.label) / length(train.label)
+prop_test <- table(test.label) / length(test.label)
+
+# Verify no overlap between train and test sets
+if (length(intersect(train.id, test.id)) > 0) stop("Overlap between train and test sets")

# time: 2024-08-29 14:46:07 UTC
# mode: r
+prop_train

# time: 2024-08-29 14:46:11 UTC
# mode: r
+prop_test

# time: 2024-08-29 14:47:37 UTC
# mode: r
+# Verify no overlap between train and test sets
+if (length(intersect(train.id, test.id)) > 0) stop("Overlap between train and test sets")

# time: 2024-08-29 14:47:51 UTC
# mode: r
+num.threads <- 4
+pca <- SNPRelate::snpgdsPCA(all.maf, sample.id = train.id, num.thread = num.threads)
+
+tab <- data.frame(sample.id = pca$sample.id, pop = factor(train.label), EV1 = pca$eigenvect[, 1], EV2 = pca$eigenvect[, 2], stringsAsFactors = FALSE)
+
+pc.percent <- pca$varprop * 100
+lbls <- paste("PC", 1:num.eigenvectors, "\n", format(pc.percent[1:num.eigenvectors], digits = 2), "%", sep = "")
+pairs(pca$eigenvect[, 1:num.eigenvectors], col = tab$pop, labels = lbls, aspect = 1)

# time: 2024-08-29 14:48:45 UTC
# mode: r
+# Calculate SNP loadings
+snp_loadings <- SNPRelate::snpgdsPCASNPLoading(pca, all.maf, num.thread = num.threads)
+
+# Calculate sample loadings for testing set using SNP loadings
+sample_loadings <- SNPRelate::snpgdsPCASampLoading(snp_loadings, all.maf, sample.id = test.id, num.thread = num.threads)
+
+# Extract list of SNPs used in PCA
+snpset.id <- unlist(all.maf)
+snp_list <- SNPRelate::snpgdsSNPList(all.maf)
+snp_rs_id <- snp_list$snp.id[pca$snp.id]
+snp_chrom <- snp_list$chromosome[pca$snp.id]
+snp_pos <- snp_list$position[pca$snp.id]
+
+# Create dataframe with one SNP per row
+snp_df <- data.frame(snp_rs_id, snp_chrom, snp_pos)
+
+# Transpose SNP loading table
+snp_loadings_transposed <- t(snp_loadings$snploading)
+
+# Match SNPs with loading vectors
+loading_mat <- data.frame(snp_df, snp_loadings_transposed[, 1:num.eigenvectors])
+
+# Write loading data frame to file
+
+# Create data frames, where sample is paired with eigenvectors
+train_eigenvects <- data.frame(pca$sample.id, pca$eigenvect[, 1:num.eigenvectors])
+test_eigenvects <- data.frame(sample_loadings$sample.id, sample_loadings$eigenvect[, 1:num.eigenvectors])
+
+# Ensure sample.id column is same for all data frames
+colnames(train_eigenvects)[1] <- "sample.id"
+colnames(test_eigenvects)[1] <- "sample.id"
+
+# Create a dataframe with 'sample.id' and 'pop' columns
+sample.info <- data.frame(sample.id = all.id, pop = all.label)
+
+# Merge data frames
+train_df <- merge(sample.info, train_eigenvects)
+test_df <- merge(sample.info, test_eigenvects)

# time: 2024-08-29 14:48:58 UTC
# mode: r
+output.forest <- randomForest::randomForest(
+    train_df[, (num.eigenvectors - 2):(2 + num.eigenvectors)],
+    y = as.factor(train_df$pop),
+    data = train_df,
+    xtest = test_df[, (num.eigenvectors - 2):(2 + num.eigenvectors)],
+    ytest = as.factor(test_df$pop),
+    ntree = 2000,
+    replace = FALSE,
+    proximity = TRUE
+)
+print(output.forest)

# time: 2024-08-29 14:49:40 UTC
# mode: r
+# Print confusion matrix
+rfPermute::confusionMatrix(output.forest)

# time: 2024-08-29 14:50:28 UTC
# mode: r
+rfPermute::plotPredictedProbs(output.forest, bins = 20, plot = TRUE)

# time: 2024-08-29 14:50:36 UTC
# mode: r
+rfPermute::plotProximity(output.forest)

# time: 2024-08-29 14:51:03 UTC
# mode: r
+xt <- summary(output.forest)

# time: 2024-08-29 14:54:11 UTC
# mode: r
+xt <- function(...) {
+    png(tempfile(fileext = ".png"))
+    xt = summary(output.forest)
+    dev.off()
+    return(xt)
+}

# time: 2024-08-29 14:54:13 UTC
# mode: r
+oob_df <- dplyr::as_tibble(xt$data)

# time: 2024-08-29 14:55:20 UTC
# mode: r
+oob_sum <- function(...) {
+    png(tempfile(fileext = ".png"))
+    xt = summary(...)
+    dev.off()
+    return(xt$data)
+}

# time: 2024-08-29 14:55:22 UTC
# mode: r
+oob_df <- dplyr::as_tibble(oob_sum(output.forest))

# time: 2024-08-29 14:55:28 UTC
# mode: r
+oob_df

# time: 2024-08-29 14:56:39 UTC
# mode: r
+invisible(capture.output(xt = summary(output.forest)))

# time: 2024-08-29 14:56:57 UTC
# mode: r
+oob_sum <- function(...) {
+    png(tempfile(fileext = ".png"))
+    invisible(capture.output(xt = summary(...)))
+    dev.off()
+    return(xt$data)
+}

# time: 2024-08-29 14:56:58 UTC
# mode: r
+oob_df <- dplyr::as_tibble(oob_sum(output.forest))

# time: 2024-08-29 14:57:28 UTC
# mode: r
+oob_sum <- function(...) {
+    png(tempfile(fileext = ".png"))
+    sink("file")
+    xt = summary(...)
+    dev.off()
+    sink()
+    return(xt$data)
+}

# time: 2024-08-29 14:57:30 UTC
# mode: r
+xt = summary(output.forest)

# time: 2024-08-29 14:57:34 UTC
# mode: r
+oob_sum <- function(...) {
+    png(tempfile(fileext = ".png"))
+    sink("file")
+    xt = summary(...)
+    dev.off()
+    sink()
+    return(xt$data)
+}

# time: 2024-08-29 14:57:36 UTC
# mode: r
+oob_df <- dplyr::as_tibble(oob_sum(output.forest))

# time: 2024-08-29 14:57:38 UTC
# mode: r
+oob_df

# time: 2024-08-29 14:58:00 UTC
# mode: r
+oob_sum <- function(...) { # This is so stupid omg
+    png(tempfile(fileext = ".png"))
+    sink("file")
+    xt = summary(...)
+    dev.off()
+    sink()
+    return(xt$data)
+}

# time: 2024-08-29 14:58:00 UTC
# mode: r
+oob_df <- dplyr::as_tibble(oob_sum(output.forest))

# time: 2024-08-29 14:58:04 UTC
# mode: r
+# for each of the forests in random_accuracy, get the summary and add to a dataframe
+random_accuracy_df <- dplyr::bind_rows(
+    lapply(random_accuracy, function(forest) {
+        d <- summary(forest)
+        d$data
+    })
+)

# time: 2024-08-29 14:58:29 UTC
# mode: r
+# train random forest using training eigenvectors
+output.forest <- randomForest::randomForest(
+    train_df[, (num.eigenvectors - 2):(2 + num.eigenvectors)],
+    y = as.factor(train_df$pop),
+    data = train_df,
+    xtest = test_df[, (num.eigenvectors - 2):(2 + num.eigenvectors)],
+    ytest = as.factor(test_df$pop),
+    ntree = 2000,
+    replace = FALSE,
+    proximity = TRUE
+)

# time: 2024-08-29 14:58:31 UTC
# mode: r
+print(output.forest)

# time: 2024-08-29 14:58:36 UTC
# mode: r
+# train random forest using training eigenvectors
+output.forest <- randomForest::randomForest(
+    train_df[, (num.eigenvectors - 2):(2 + num.eigenvectors)],
+    y = as.factor(train_df$pop),
+    data = train_df,
+    xtest = test_df[, (num.eigenvectors - 2):(2 + num.eigenvectors)],
+    ytest = as.factor(test_df$pop),
+    ntree = 2000,
+    replace = FALSE,
+    proximity = TRUE
+)

# time: 2024-08-29 14:58:40 UTC
# mode: r
+output.forest

# time: 2024-08-29 14:58:43 UTC
# mode: r
+# Print confusion matrix
+rfPermute::confusionMatrix(output.forest)

# time: 2024-08-29 14:58:50 UTC
# mode: r
+oob_df <- dplyr::as_tibble(oob_sum(output.forest))

# time: 2024-08-29 14:58:51 UTC
# mode: r
+ggplot2::ggplot(oob_df, ggplot2::aes(x = trees, y = error, color = class)) +
+    ggplot2::geom_line() +
+    ggplot2::theme_minimal() +
+    ggplot2::labs(title = "OOB error rate", x = "Number of trees", y = "Error rate") +
+    ggplot2::scale_color_manual(values = c(
+        "OOB" = "#4e4e4e",
+        "immigrant" = "#5d8566",
+        "resident" = "#c29007"
+    )) +
+    ggplot2::coord_cartesian(ylim = c(0, 100)) +
+    ggplot2::geom_hline(yintercept = 50, linetype = "dashed", color = "red") +
+    ggplot2::theme(
+        aspect.ratio = 1,
+        text = ggplot2::element_text(size = 12)
+    )

# time: 2024-08-29 14:59:12 UTC
# mode: r
+# train random forest using training eigenvectors
+output.forest <- randomForest::randomForest(
+    train_df[, (num.eigenvectors - 2):(2 + num.eigenvectors)],
+    y = as.factor(train_df$pop),
+    data = train_df,
+    xtest = test_df[, (num.eigenvectors - 2):(2 + num.eigenvectors)],
+    ytest = as.factor(test_df$pop),
+    ntree = 2000,
+    replace = FALSE,
+    proximity = TRUE
+)
+
+
+# Print confusion matrix
+# rfPermute::confusionMatrix(output.forest)
+# rfPermute::plotPredictedProbs(output.forest, bins = 20, plot = TRUE)
+# rfPermute::plotProximity(output.forest)
+
+
+
+oob_sum <- function(...) { # This is so stupid omg
+    png(tempfile(fileext = ".png"))
+    sink("file")
+    xt = summary(...)
+    dev.off()
+    sink()
+    return(xt$data)
+}
+oob_df <- dplyr::as_tibble(oob_sum(output.forest))
+
+
+
+ggplot2::ggplot(oob_df, ggplot2::aes(x = trees, y = error, color = class)) +
+    ggplot2::geom_line() +
+    ggplot2::theme_minimal() +
+    ggplot2::labs(title = "OOB error rate", x = "Number of trees", y = "Error rate") +
+    ggplot2::scale_color_manual(values = c(
+        "OOB" = "#4e4e4e",
+        "immigrant" = "#5d8566",
+        "resident" = "#c29007"
+    )) +
+    ggplot2::coord_cartesian(ylim = c(0, 100)) +
+    ggplot2::geom_hline(yintercept = 50, linetype = "dashed", color = "red") +
+    ggplot2::theme(
+        aspect.ratio = 1,
+        text = ggplot2::element_text(size = 12)
+    )
+
+#

# time: 2024-08-29 14:59:33 UTC
# mode: r
+# Subsample data to balance immigrant and resident samples
+min_samples <- min(length(immigrant.id), length(resident.id))
+all.id <- c(sample(immigrant.id, min_samples), sample(resident.id, min_samples))
+all.label <- ifelse(all.id %in% immigrant.id, "immigrant", "resident")
+
+# Split data into training and testing sets
+num_train <- round(length(all.id) * percent.train)
+train.id <- sample(all.id, num_train)
+test.id <- setdiff(all.id, train.id)
+
+train.label <- all.label[all.id %in% train.id]
+test.label <- all.label[all.id %in% test.id]
+
+# Check proportions
+prop_train <- table(train.label) / length(train.label)
+prop_test <- table(test.label) / length(test.label)
+
+# Verify no overlap between train and test sets
+if (length(intersect(train.id, test.id)) > 0) stop("Overlap between train and test sets")
+
+# Reduce dimensionality via pca
+num.threads <- 4
+pca <- SNPRelate::snpgdsPCA(all.maf, sample.id = train.id, num.thread = num.threads)
+
+tab <- data.frame(sample.id = pca$sample.id, pop = factor(train.label), EV1 = pca$eigenvect[, 1], EV2 = pca$eigenvect[, 2], stringsAsFactors = FALSE)
+
+pc.percent <- pca$varprop * 100
+lbls <- paste("PC", 1:num.eigenvectors, "\n", format(pc.percent[1:num.eigenvectors], digits = 2), "%", sep = "")
+pairs(pca$eigenvect[, 1:num.eigenvectors], col = tab$pop, labels = lbls, aspect = 1)
+
+
+# Calculate SNP loadings
+snp_loadings <- SNPRelate::snpgdsPCASNPLoading(pca, all.maf, num.thread = num.threads)
+
+# Calculate sample loadings for testing set using SNP loadings
+sample_loadings <- SNPRelate::snpgdsPCASampLoading(snp_loadings, all.maf, sample.id = test.id, num.thread = num.threads)
+
+# Extract list of SNPs used in PCA
+snpset.id <- unlist(all.maf)
+snp_list <- SNPRelate::snpgdsSNPList(all.maf)
+snp_rs_id <- snp_list$snp.id[pca$snp.id]
+snp_chrom <- snp_list$chromosome[pca$snp.id]
+snp_pos <- snp_list$position[pca$snp.id]
+
+# Create dataframe with one SNP per row
+snp_df <- data.frame(snp_rs_id, snp_chrom, snp_pos)
+
+# Transpose SNP loading table
+snp_loadings_transposed <- t(snp_loadings$snploading)
+
+# Match SNPs with loading vectors
+loading_mat <- data.frame(snp_df, snp_loadings_transposed[, 1:num.eigenvectors])
+
+# Write loading data frame to file
+
+# Create data frames, where sample is paired with eigenvectors
+train_eigenvects <- data.frame(pca$sample.id, pca$eigenvect[, 1:num.eigenvectors])
+test_eigenvects <- data.frame(sample_loadings$sample.id, sample_loadings$eigenvect[, 1:num.eigenvectors])
+
+# Ensure sample.id column is same for all data frames
+colnames(train_eigenvects)[1] <- "sample.id"
+colnames(test_eigenvects)[1] <- "sample.id"
+
+# Create a dataframe with 'sample.id' and 'pop' columns
+sample.info <- data.frame(sample.id = all.id, pop = all.label)
+
+# Merge data frames
+train_df <- merge(sample.info, train_eigenvects)
+test_df <- merge(sample.info, test_eigenvects)
+
+
+# train random forest using training eigenvectors
+output.forest <- randomForest::randomForest(
+    train_df[, (num.eigenvectors - 2):(2 + num.eigenvectors)],
+    y = as.factor(train_df$pop),
+    data = train_df,
+    xtest = test_df[, (num.eigenvectors - 2):(2 + num.eigenvectors)],
+    ytest = as.factor(test_df$pop),
+    ntree = 2000,
+    replace = FALSE,
+    proximity = TRUE
+)
+
+
+# Print confusion matrix
+# rfPermute::confusionMatrix(output.forest)
+# rfPermute::plotPredictedProbs(output.forest, bins = 20, plot = TRUE)
+# rfPermute::plotProximity(output.forest)
+
+
+
+oob_sum <- function(...) { # This is so stupid omg
+    png(tempfile(fileext = ".png"))
+    sink("file")
+    xt = summary(...)
+    dev.off()
+    sink()
+    return(xt$data)
+}
+oob_df <- dplyr::as_tibble(oob_sum(output.forest))
+
+
+
+ggplot2::ggplot(oob_df, ggplot2::aes(x = trees, y = error, color = class)) +
+    ggplot2::geom_line() +
+    ggplot2::theme_minimal() +
+    ggplot2::labs(title = "OOB error rate", x = "Number of trees", y = "Error rate") +
+    ggplot2::scale_color_manual(values = c(
+        "OOB" = "#4e4e4e",
+        "immigrant" = "#5d8566",
+        "resident" = "#c29007"
+    )) +
+    ggplot2::coord_cartesian(ylim = c(0, 100)) +
+    ggplot2::geom_hline(yintercept = 50, linetype = "dashed", color = "red") +
+    ggplot2::theme(
+        aspect.ratio = 1,
+        text = ggplot2::element_text(size = 12)
+    )
+
+#

# time: 2024-08-29 15:00:24 UTC
# mode: r
+# Print confusion matrix
+rfPermute::confusionMatrix(output.forest)

# time: 2024-08-29 15:03:34 UTC
# mode: r
+# Function to train a random forest with randomized labels
+train_random_forest <- function(train_df, test_df, num.eigenvectors) {
+    # Randomize training labels
+    train_df$random_pop <- sample(train_df$pop)
+
+    # Train random forest
+    rf <- randomForest::randomForest(
+        train_df[, (num.eigenvectors - 2):(2 + num.eigenvectors)],
+        y = as.factor(train_df$random_pop),
+        data = train_df,
+        xtest = test_df[, (num.eigenvectors - 2):(2 + num.eigenvectors)],
+        ytest = as.factor(test_df$pop),
+        ntree = 1000,
+        replace = FALSE,
+        proximity = TRUE
+    )
+
+    return(rf)
+}
+
+# Train 100 random forests with randomized labels
+set.seed(123) # For reproducibility
+random_forests <- replicate(5, train_random_forest(train_df, test_df, num.eigenvectors), simplify = FALSE)
+
+# Extract OOB error rates
+random_oob_data <- lapply(random_forests, function(rf) {
+    oob_sum(rf)$data
+})
+
+# Combine OOB error rates
+random_oob_df <- do.call(rbind, random_oob_data)
+random_oob_df$iteration <- rep(1:100, each = nrow(random_oob_data[[1]]))
+
+# Calculate mean OOB error rates across iterations
+mean_random_oob <- random_oob_df %>%
+    group_by(trees, class) %>%
+    summarise(mean_error = mean(error), .groups = "drop")
+
+# Add random forest results to the plot
+ggplot2::ggplot() +
+    # Original data
+    ggplot2::geom_line(data = oob_df, ggplot2::aes(x = trees, y = error, color = class)) +
+    # Random forest results
+    ggplot2::geom_line(data = mean_random_oob, ggplot2::aes(x = trees, y = mean_error, color = class), linetype = "dashed") +
+    ggplot2::theme_minimal() +
+    ggplot2::labs(
+        title = "OOB error rate (Solid: Original, Dashed: Randomized)",
+        x = "Number of trees", y = "Error rate"
+    ) +
+    ggplot2::scale_color_manual(values = c(
+        "OOB" = "#4e4e4e",
+        "immigrant" = "#5d8566",
+        "resident" = "#c29007"
+    )) +
+    ggplot2::coord_cartesian(ylim = c(0, 100)) +
+    ggplot2::geom_hline(yintercept = 50, linetype = "dotted", color = "red") +
+    ggplot2::theme(
+        aspect.ratio = 1,
+        text = ggplot2::element_text(size = 12)
+    )

# time: 2024-08-29 15:07:06 UTC
# mode: r
+# Function to train a random forest with randomized labels
+train_random_forest <- function(train_df, test_df, num.eigenvectors) {
+    # Randomize training labels
+    train_df$random_pop <- sample(train_df$pop)
+
+    # Train random forest
+    rf <- randomForest::randomForest(
+        train_df[, (num.eigenvectors - 2):(2 + num.eigenvectors)],
+        y = as.factor(train_df$random_pop),
+        data = train_df,
+        xtest = test_df[, (num.eigenvectors - 2):(2 + num.eigenvectors)],
+        ytest = as.factor(test_df$pop),
+        ntree = 1000,
+        replace = FALSE,
+        proximity = TRUE
+    )
+
+    return(rf)
+}
+
+# Train 100 random forests with randomized labels
+set.seed(123) # For reproducibility
+random_forests <- replicate(5, train_random_forest(train_df, test_df, num.eigenvectors), simplify = FALSE)
+
+# Extract OOB error rates
+random_oob_data <- lapply(random_forests, function(rf) {
+    oob_sum(rf)$data
+})
+
+# Combine OOB error rates
+random_oob_df <- do.call(rbind, random_oob_data)
+random_oob_df$iteration <- rep(1:100, each = nrow(random_oob_data[[1]]))
+
+# Calculate mean OOB error rates across iterations
+mean_random_oob <- random_oob_df |>
+    group_by(trees, class) |>
+    summarise(mean_error = mean(error), .groups = "drop")
+
+# Add random forest results to the plot
+ggplot2::ggplot() +
+    # Original data
+    ggplot2::geom_line(data = oob_df, ggplot2::aes(x = trees, y = error, color = class)) +
+    # Random forest results
+    ggplot2::geom_line(data = mean_random_oob, ggplot2::aes(x = trees, y = mean_error, color = class), linetype = "dashed") +
+    ggplot2::theme_minimal() +
+    ggplot2::labs(
+        title = "OOB error rate (Solid: Original, Dashed: Randomized)",
+        x = "Number of trees", y = "Error rate"
+    ) +
+    ggplot2::scale_color_manual(values = c(
+        "OOB" = "#4e4e4e",
+        "immigrant" = "#5d8566",
+        "resident" = "#c29007"
+    )) +
+    ggplot2::coord_cartesian(ylim = c(0, 100)) +
+    ggplot2::geom_hline(yintercept = 50, linetype = "dotted", color = "red") +
+    ggplot2::theme(
+        aspect.ratio = 1,
+        text = ggplot2::element_text(size = 12)
+    )

# time: 2024-08-29 15:07:29 UTC
# mode: r
+# Function to train a random forest with randomized labels
+train_random_forest <- function(train_df, test_df, num.eigenvectors) {
+    # Randomize training labels
+    train_df$random_pop <- sample(train_df$pop)
+
+    # Train random forest
+    rf <- randomForest::randomForest(
+        train_df[, (num.eigenvectors - 2):(2 + num.eigenvectors)],
+        y = as.factor(train_df$random_pop),
+        data = train_df,
+        xtest = test_df[, (num.eigenvectors - 2):(2 + num.eigenvectors)],
+        ytest = as.factor(test_df$pop),
+        ntree = 1000,
+        replace = FALSE,
+        proximity = TRUE
+    )
+
+    return(rf)
+}
+
+# Train 100 random forests with randomized labels
+set.seed(123) # For reproducibility
+random_forests <- replicate(5, train_random_forest(train_df, test_df, num.eigenvectors), simplify = FALSE)
+
+# Extract OOB error rates
+random_oob_data <- lapply(random_forests, function(rf) {
+    oob_sum(rf)$data
+})
+
+# Combine OOB error rates
+random_oob_df <- do.call(rbind, random_oob_data)
+random_oob_df$iteration <- rep(1:100, each = nrow(random_oob_data[[1]]))
+
+# Calculate mean OOB error rates across iterations
+mean_random_oob <- random_oob_df |>
+    dplyr::group_by(trees, class) |>
+    dplyr::summarise(mean_error = mean(error), .groups = "drop")
+
+# Add random forest results to the plot
+ggplot2::ggplot() +
+    # Original data
+    ggplot2::geom_line(data = oob_df, ggplot2::aes(x = trees, y = error, color = class)) +
+    # Random forest results
+    ggplot2::geom_line(data = mean_random_oob, ggplot2::aes(x = trees, y = mean_error, color = class), linetype = "dashed") +
+    ggplot2::theme_minimal() +
+    ggplot2::labs(
+        title = "OOB error rate (Solid: Original, Dashed: Randomized)",
+        x = "Number of trees", y = "Error rate"
+    ) +
+    ggplot2::scale_color_manual(values = c(
+        "OOB" = "#4e4e4e",
+        "immigrant" = "#5d8566",
+        "resident" = "#c29007"
+    )) +
+    ggplot2::coord_cartesian(ylim = c(0, 100)) +
+    ggplot2::geom_hline(yintercept = 50, linetype = "dotted", color = "red") +
+    ggplot2::theme(
+        aspect.ratio = 1,
+        text = ggplot2::element_text(size = 12)
+    )

# time: 2024-08-29 15:07:39 UTC
# mode: r
+mean_random_oob

# time: 2024-08-29 15:07:41 UTC
# mode: r
+random_oob_data

# time: 2024-08-29 15:07:43 UTC
# mode: r
+random_forests

# time: 2024-08-29 15:08:48 UTC
# mode: r
+# Extract OOB error rates
+random_oob_data <- lapply(random_forests, function(rf) {
+    oob_sum(rf)
+})

# time: 2024-08-29 15:08:50 UTC
# mode: r
+random_oob_data

# time: 2024-08-29 15:09:02 UTC
# mode: r
+# Extract OOB error rates
+random_oob_data <- lapply(random_forests, function(rf) {
+    oob_sum(rf)
+})

# time: 2024-08-29 15:09:03 UTC
# mode: r
+# Combine OOB error rates
+random_oob_df <- do.call(rbind, random_oob_data)

# time: 2024-08-29 15:09:04 UTC
# mode: r
+random_oob_df$iteration <- rep(1:100, each = nrow(random_oob_data[[1]]))

# time: 2024-08-29 15:09:46 UTC
# mode: r
+# Combine OOB error rates
+random_oob_df <- do.call(rbind, random_oob_data)

# time: 2024-08-29 15:09:47 UTC
# mode: r
+# Calculate mean OOB error rates across iterations
+mean_random_oob <- random_oob_df |>
+    dplyr::group_by(trees, class) |>
+    dplyr::summarise(mean_error = mean(error), .groups = "drop")

# time: 2024-08-29 15:09:49 UTC
# mode: r
+mean_random_oob

# time: 2024-08-29 15:10:04 UTC
# mode: r
+oob_df

# time: 2024-08-29 15:10:35 UTC
# mode: r
+# Combine OOB error rates
+random_oob_df <- do.call(rbind, random_oob_data)

# time: 2024-08-29 15:10:37 UTC
# mode: r
+# Calculate mean OOB error rates across iterations
+random_oob <- random_oob_df |>
+    dplyr::group_by(trees, class) |>
+    dplyr::summarise(mean_error = mean(error), .groups = "drop")

# time: 2024-08-29 15:10:45 UTC
# mode: r
+# Add random forest results to the plot
+ggplot2::ggplot() +
+    # Original data
+    ggplot2::geom_line(data = oob_df, ggplot2::aes(x = trees, y = error, color = class)) +
+    # Random forest results
+    ggplot2::geom_line(data = random_oob, ggplot2::aes(x = trees, y = error, color = class), linetype = "dashed") +
+    ggplot2::theme_minimal() +
+    ggplot2::labs(
+        title = "OOB error rate (Solid: Original, Dashed: Randomized)",
+        x = "Number of trees", y = "Error rate"
+    ) +
+    ggplot2::scale_color_manual(values = c(
+        "OOB" = "#4e4e4e",
+        "immigrant" = "#5d8566",
+        "resident" = "#c29007"
+    )) +
+    ggplot2::coord_cartesian(ylim = c(0, 100)) +
+    ggplot2::geom_hline(yintercept = 50, linetype = "dotted", color = "red") +
+    ggplot2::theme(
+        aspect.ratio = 1,
+        text = ggplot2::element_text(size = 12)
+    )

# time: 2024-08-29 15:10:54 UTC
# mode: r
+random_oob_df

# time: 2024-08-29 15:11:08 UTC
# mode: r
+# Combine OOB error rates
+random_oob_df <- do.call(rbind, random_oob_data)

# time: 2024-08-29 15:11:09 UTC
# mode: r
+# Add random forest results to the plot
+ggplot2::ggplot() +
+    # Original data
+    ggplot2::geom_line(data = oob_df, ggplot2::aes(x = trees, y = error, color = class)) +
+    # Random forest results
+    ggplot2::geom_line(data = random_oob, ggplot2::aes(x = trees, y = error, color = class), linetype = "dashed") +
+    ggplot2::theme_minimal() +
+    ggplot2::labs(
+        title = "OOB error rate (Solid: Original, Dashed: Randomized)",
+        x = "Number of trees", y = "Error rate"
+    ) +
+    ggplot2::scale_color_manual(values = c(
+        "OOB" = "#4e4e4e",
+        "immigrant" = "#5d8566",
+        "resident" = "#c29007"
+    )) +
+    ggplot2::coord_cartesian(ylim = c(0, 100)) +
+    ggplot2::geom_hline(yintercept = 50, linetype = "dotted", color = "red") +
+    ggplot2::theme(
+        aspect.ratio = 1,
+        text = ggplot2::element_text(size = 12)
+    )

# time: 2024-08-29 15:11:14 UTC
# mode: r
+random_oob_df

# time: 2024-08-29 15:11:25 UTC
# mode: r
+# Combine OOB error rates
+random_oob_df <- dplyr::as_tibble(do.call(rbind, random_oob_data))

# time: 2024-08-29 15:11:27 UTC
# mode: r
+random_oob_df

# time: 2024-08-29 15:11:37 UTC
# mode: r
+oob_df

# time: 2024-08-29 15:11:48 UTC
# mode: r
+# Add random forest results to the plot
+ggplot2::ggplot() +
+    # Original data
+    ggplot2::geom_line(data = oob_df, ggplot2::aes(x = trees, y = error, color = class)) +
+    # Random forest results
+    ggplot2::geom_line(data = random_oob_df, ggplot2::aes(x = trees, y = error, color = class), linetype = "dashed") +
+    ggplot2::theme_minimal() +
+    ggplot2::labs(
+        title = "OOB error rate (Solid: Original, Dashed: Randomized)",
+        x = "Number of trees", y = "Error rate"
+    ) +
+    ggplot2::scale_color_manual(values = c(
+        "OOB" = "#4e4e4e",
+        "immigrant" = "#5d8566",
+        "resident" = "#c29007"
+    )) +
+    ggplot2::coord_cartesian(ylim = c(0, 100)) +
+    ggplot2::geom_hline(yintercept = 50, linetype = "dotted", color = "red") +
+    ggplot2::theme(
+        aspect.ratio = 1,
+        text = ggplot2::element_text(size = 12)
+    )

# time: 2024-08-29 15:15:04 UTC
# mode: r
+# Function to train a random forest with optional randomized labels
+train_random_forest <- function(
+    train_df,
+    test_df,
+    num.eigenvectors,
+    randomize_labels = FALSE) {
+    if (randomize_labels) {
+        # Randomize training labels
+        train_df$random_pop <- sample(train_df$pop)
+        y_train <- as.factor(train_df$random_pop)
+    } else {
+        y_train <- as.factor(train_df$pop)
+    }
+
+    # Train random forest
+    rf <- randomForest::randomForest(
+        train_df[, (num.eigenvectors - 2):(2 + num.eigenvectors)],
+        y = y_train,
+        data = train_df,
+        xtest = test_df[, (num.eigenvectors - 2):(2 + num.eigenvectors)],
+        ytest = as.factor(test_df$pop),
+        ntree = 1000,
+        replace = FALSE,
+        proximity = TRUE
+    )
+
+    return(rf)
+}

# time: 2024-08-29 15:15:05 UTC
# mode: r
+# Train random forest
+output.forest <- train_random_forest(train_df, test_df, num.eigenvectors)

# time: 2024-08-29 15:15:09 UTC
# mode: r
+# Print confusion matrix
+rfPermute::confusionMatrix(output.forest)

# time: 2024-08-29 15:15:25 UTC
# mode: r
+oob_df <- dplyr::as_tibble(oob_sum(output.forest))

# time: 2024-08-29 15:15:31 UTC
# mode: r
+# Train 100 random forests with randomized labels
+set.seed(123) # For reproducibility
+random_forests <- replicate(5, train_random_forest(
+    train_df, test_df, num.eigenvectors,
+    randomize_labels = TRUE
+),
+simplify = FALSE
+)
+
+# Extract OOB error rates
+random_oob_data <- lapply(random_forests, function(rf) {
+    oob_sum(rf)
+})
+
+# Combine OOB error rates
+random_oob_df <- dplyr::as_tibble(do.call(rbind, random_oob_data))
+
+
+# Add random forest results to the plot
+ggplot2::ggplot() +
+    # Original data
+    ggplot2::geom_line(data = oob_df, ggplot2::aes(x = trees, y = error, color = class)) +
+    # Random forest results
+    ggplot2::geom_line(data = random_oob_df, ggplot2::aes(x = trees, y = error, color = class), linetype = "dashed") +
+    ggplot2::theme_minimal() +
+    ggplot2::labs(
+        title = "OOB error rate (Solid: Original, Dashed: Randomized)",
+        x = "Number of trees", y = "Error rate"
+    ) +
+    ggplot2::scale_color_manual(values = c(
+        "OOB" = "#4e4e4e",
+        "immigrant" = "#5d8566",
+        "resident" = "#c29007"
+    )) +
+    ggplot2::coord_cartesian(ylim = c(0, 100)) +
+    ggplot2::geom_hline(yintercept = 50, linetype = "dotted", color = "red") +
+    ggplot2::theme(
+        aspect.ratio = 1,
+        text = ggplot2::element_text(size = 12)
+    )

# time: 2024-08-29 15:18:31 UTC
# mode: r
+# Calculate mean error for original data
+mean_oob_df <- oob_df %>%
+    dplyr::group_by(trees, class) %>%
+    dplyr::summarise(mean_error = mean(error), .groups = "drop")

# time: 2024-08-29 15:18:39 UTC
# mode: r
+# Calculate mean error for original data
+mean_oob_df <- oob_df |>
+    dplyr::group_by(trees, class) |>
+    dplyr::summarise(mean_error = mean(error), .groups = "drop")

# time: 2024-08-29 15:18:42 UTC
# mode: r
+# Calculate mean error for randomized data
+mean_random_oob_df <- random_oob_df |>
+    dplyr::group_by(trees, class) |>
+    dplyr::summarise(mean_error = mean(error), .groups = "drop")

# time: 2024-08-29 15:18:44 UTC
# mode: r
+# Add random forest results to the plot
+ggplot2::ggplot() +
+    # Original data (individual lines with low opacity)
+    ggplot2::geom_line(data = oob_df, ggplot2::aes(x = trees, y = error, color = class, group = interaction(class, iteration)), alpha = 0.1) +
+    # Random forest results (individual lines with low opacity)
+    ggplot2::geom_line(data = random_oob_df, ggplot2::aes(x = trees, y = error, color = class, group = interaction(class, iteration)), alpha = 0.1, linetype = "dashed") +
+    # Smooth mean lines for original data
+    ggplot2::geom_smooth(data = mean_oob_df, ggplot2::aes(x = trees, y = mean_error, color = class), se = FALSE, size = 1.5) +
+    # Smooth mean lines for randomized data
+    ggplot2::geom_smooth(data = mean_random_oob_df, ggplot2::aes(x = trees, y = mean_error, color = class), se = FALSE, size = 1.5, linetype = "dashed") +
+    ggplot2::theme_minimal() +
+    ggplot2::labs(
+        title = "OOB error rate (Solid: Original, Dashed: Randomized)",
+        x = "Number of trees", y = "Error rate"
+    ) +
+    ggplot2::scale_color_manual(values = c(
+        "OOB" = "#4e4e4e",
+        "immigrant" = "#5d8566",
+        "resident" = "#c29007"
+    )) +
+    ggplot2::coord_cartesian(ylim = c(0, 100)) +
+    ggplot2::geom_hline(yintercept = 50, linetype = "dotted", color = "red") +
+    ggplot2::theme(
+        aspect.ratio = 1,
+        text = ggplot2::element_text(size = 12)
+    )

# time: 2024-08-29 15:19:06 UTC
# mode: r
+mean_random_oob_df

# time: 2024-08-29 15:19:52 UTC
# mode: r
+# Add random forest results to the plot
+ggplot2::ggplot() +
+    # Original data (individual lines with low opacity)
+    ggplot2::geom_line(data = oob_df, ggplot2::aes(x = trees, y = error, color = class), alpha = 0.1) +
+    # Random forest results (individual lines with low opacity)
+    ggplot2::geom_line(data = random_oob_df, ggplot2::aes(x = trees, y = error, color = class), alpha = 0.1, linetype = "dashed") +
+    # Smooth mean lines for original data
+    ggplot2::geom_smooth(data = mean_oob_df, ggplot2::aes(x = trees, y = mean_error, color = class), se = FALSE, size = 1.5) +
+    # Smooth mean lines for randomized data
+    ggplot2::geom_smooth(data = mean_random_oob_df, ggplot2::aes(x = trees, y = mean_error, color = class), se = FALSE, size = 1.5, linetype = "dashed") +
+    ggplot2::theme_minimal() +
+    ggplot2::labs(
+        title = "OOB error rate (Solid: Original, Dashed: Randomized)",
+        x = "Number of trees", y = "Error rate"
+    ) +
+    ggplot2::scale_color_manual(values = c(
+        "OOB" = "#4e4e4e",
+        "immigrant" = "#5d8566",
+        "resident" = "#c29007"
+    )) +
+    ggplot2::coord_cartesian(ylim = c(0, 100)) +
+    ggplot2::geom_hline(yintercept = 50, linetype = "dotted", color = "red") +
+    ggplot2::theme(
+        aspect.ratio = 1,
+        text = ggplot2::element_text(size = 12)
+    )

# time: 2024-08-29 15:21:34 UTC
# mode: r
+# Add random forest results to the plot
+ggplot2::ggplot() +
+    # Original data (individual lines with low opacity)
+    ggplot2::geom_line(
+        data = oob_df,
+        ggplot2::aes(x = trees, y = error, color = class), alpha = 0.1
+    ) +
+    # Random forest results (individual lines with low opacity)
+    ggplot2::geom_line(
+        data = random_oob_df,
+        ggplot2::aes(x = trees, y = error, color = class), alpha = 0.1, linetype = "dashed"
+    ) +
+    # Smooth mean lines for original data
+    ggplot2::geom_line(
+        data = mean_oob_df,
+        ggplot2::aes(x = trees, y = mean_error, color = class), size = 1.5
+    ) +
+    # Smooth mean lines for randomized data
+    ggplot2::geom_line(
+        data = mean_random_oob_df,
+        ggplot2::aes(x = trees, y = mean_error, color = class), size = 1.5,
+        linetype = "dashed"
+    ) +
+    ggplot2::theme_minimal() +
+    ggplot2::labs(
+        title = "OOB error rate (Solid: Original, Dashed: Randomized)",
+        x = "Number of trees", y = "Error rate"
+    ) +
+    ggplot2::scale_color_manual(values = c(
+        "OOB" = "#4e4e4e",
+        "immigrant" = "#5d8566",
+        "resident" = "#c29007"
+    )) +
+    ggplot2::coord_cartesian(ylim = c(0, 100)) +
+    ggplot2::geom_hline(yintercept = 50, linetype = "dotted", color = "red") +
+    ggplot2::theme(
+        aspect.ratio = 1,
+        text = ggplot2::element_text(size = 12)
+    )

# time: 2024-08-29 15:23:46 UTC
# mode: r
+# Train 100 random forests with original labels
+set.seed(123) # For reproducibility
+randf <- replicate(5, train_random_forest(
+    train_df, test_df, num.eigenvectors,
+    randomize_labels = FALSE
+),
+simplify = FALSE
+)
+
+# Extract OOB error rates
+oob_data <- lapply(randf, function(rf) {
+    oob_sum(rf)
+})
+
+oob_df <- dplyr::as_tibble(do.call(rbind, oob_data))

# time: 2024-08-29 15:23:52 UTC
# mode: r
+# Calculate mean error for original data
+mean_oob_df <- oob_df |>
+    dplyr::group_by(trees, class) |>
+    dplyr::summarise(mean_error = mean(error), .groups = "drop")
+
+# Calculate mean error for randomized data
+mean_random_oob_df <- random_oob_df |>
+    dplyr::group_by(trees, class) |>
+    dplyr::summarise(mean_error = mean(error), .groups = "drop")
+
+# Add random forest results to the plot
+ggplot2::ggplot() +
+    # Original data (individual lines with low opacity)
+    ggplot2::geom_line(
+        data = oob_df,
+        ggplot2::aes(x = trees, y = error, color = class), alpha = 0.1
+    ) +
+    # Random forest results (individual lines with low opacity)
+    ggplot2::geom_line(
+        data = random_oob_df,
+        ggplot2::aes(x = trees, y = error, color = class), alpha = 0.1, linetype = "dashed"
+    ) +
+    # Smooth mean lines for original data
+    ggplot2::geom_line(
+        data = mean_oob_df,
+        ggplot2::aes(x = trees, y = mean_error, color = class), size = 1.5
+    ) +
+    # Smooth mean lines for randomized data
+    ggplot2::geom_line(
+        data = mean_random_oob_df,
+        ggplot2::aes(x = trees, y = mean_error, color = class), size = 1.5,
+        linetype = "dashed"
+    ) +
+    ggplot2::theme_minimal() +
+    ggplot2::labs(
+        title = "OOB error rate (Solid: Original, Dashed: Randomized)",
+        x = "Number of trees", y = "Error rate"
+    ) +
+    ggplot2::scale_color_manual(values = c(
+        "OOB" = "#4e4e4e",
+        "immigrant" = "#5d8566",
+        "resident" = "#c29007"
+    )) +
+    ggplot2::coord_cartesian(ylim = c(0, 100)) +
+    ggplot2::geom_hline(yintercept = 50, linetype = "dotted", color = "red") +
+    ggplot2::theme(
+        aspect.ratio = 1,
+        text = ggplot2::element_text(size = 12)
+    )

# time: 2024-08-29 15:24:53 UTC
# mode: r
+# Function to train a random forest with optional randomized labels
+train_random_forest <- function(
+    train_df,
+    test_df,
+    num.eigenvectors,
+    randomize_labels = FALSE) {
+    if (randomize_labels) {
+        # Randomize training labels
+        train_df$random_pop <- sample(train_df$pop)
+        y_train <- as.factor(train_df$random_pop)
+    } else {
+        y_train <- as.factor(train_df$pop)
+    }
+
+    # Train random forest
+    rf <- randomForest::randomForest(
+        train_df[, (num.eigenvectors - 2):(2 + num.eigenvectors)],
+        y = y_train,
+        data = train_df,
+        xtest = test_df[, (num.eigenvectors - 2):(2 + num.eigenvectors)],
+        ytest = as.factor(test_df$pop),
+        ntree = 500,
+        replace = FALSE,
+        proximity = TRUE
+    )
+
+    return(rf)
+}

# time: 2024-08-29 15:25:08 UTC
# mode: r
+# Train 100 random forests with randomized labels
+set.seed(123) # For reproducibility
+randrandf <- replicate(20, train_random_forest(
+    train_df, test_df, num.eigenvectors,
+    randomize_labels = TRUE
+),
+simplify = FALSE
+)
+# Extract OOB error rates
+random_oob_data <- lapply(randrandf, function(rf) {
+    oob_sum(rf)
+})
+
+random_oob_df <- dplyr::as_tibble(do.call(rbind, random_oob_data))
+
+
+# Train 100 random forests with original labels
+set.seed(123) # For reproducibility
+randf <- replicate(20, train_random_forest(
+    train_df, test_df, num.eigenvectors,
+    randomize_labels = FALSE
+),
+simplify = FALSE
+)
+
+# Extract OOB error rates
+oob_data <- lapply(randf, function(rf) {
+    oob_sum(rf)
+})
+
+oob_df <- dplyr::as_tibble(do.call(rbind, oob_data))
+
+
+# Calculate mean error for original data
+mean_oob_df <- oob_df |>
+    dplyr::group_by(trees, class) |>
+    dplyr::summarise(mean_error = mean(error), .groups = "drop")
+
+# Calculate mean error for randomized data
+mean_random_oob_df <- random_oob_df |>
+    dplyr::group_by(trees, class) |>
+    dplyr::summarise(mean_error = mean(error), .groups = "drop")
+
+# Add random forest results to the plot
+ggplot2::ggplot() +
+    # Original data (individual lines with low opacity)
+    ggplot2::geom_line(
+        data = oob_df,
+        ggplot2::aes(x = trees, y = error, color = class), alpha = 0.1
+    ) +
+    # Random forest results (individual lines with low opacity)
+    ggplot2::geom_line(
+        data = random_oob_df,
+        ggplot2::aes(x = trees, y = error, color = class), alpha = 0.1, linetype = "dashed"
+    ) +
+    # Smooth mean lines for original data
+    ggplot2::geom_line(
+        data = mean_oob_df,
+        ggplot2::aes(x = trees, y = mean_error, color = class), size = 1.5
+    ) +
+    # Smooth mean lines for randomized data
+    ggplot2::geom_line(
+        data = mean_random_oob_df,
+        ggplot2::aes(x = trees, y = mean_error, color = class), size = 1.5,
+        linetype = "dashed"
+    ) +
+    ggplot2::theme_minimal() +
+    ggplot2::labs(
+        title = "OOB error rate (Solid: Original, Dashed: Randomized)",
+        x = "Number of trees", y = "Error rate"
+    ) +
+    ggplot2::scale_color_manual(values = c(
+        "OOB" = "#4e4e4e",
+        "immigrant" = "#5d8566",
+        "resident" = "#c29007"
+    )) +
+    ggplot2::coord_cartesian(ylim = c(0, 100)) +
+    ggplot2::geom_hline(yintercept = 50, linetype = "dotted", color = "red") +
+    ggplot2::theme(
+        aspect.ratio = 1,
+        text = ggplot2::element_text(size = 12)
+    )

# time: 2024-08-29 15:26:29 UTC
# mode: r
+# Subsample data to balance immigrant and resident samples
+min_samples <- min(length(immigrant.id), length(resident.id))
+all.id <- c(sample(immigrant.id, min_samples), sample(resident.id, min_samples))
+all.label <- ifelse(all.id %in% immigrant.id, "immigrant", "resident")
+
+# Split data into training and testing sets
+num_train <- round(length(all.id) * percent.train)
+train.id <- sample(all.id, num_train)
+test.id <- setdiff(all.id, train.id)
+
+train.label <- all.label[all.id %in% train.id]
+test.label <- all.label[all.id %in% test.id]
+
+# Check proportions
+prop_train <- table(train.label) / length(train.label)
+prop_test <- table(test.label) / length(test.label)
+
+# Verify no overlap between train and test sets
+if (length(intersect(train.id, test.id)) > 0) stop("Overlap between train and test sets")
+
+# Reduce dimensionality via pca
+num.threads <- 4
+pca <- SNPRelate::snpgdsPCA(all.maf, sample.id = train.id, num.thread = num.threads)
+
+tab <- data.frame(sample.id = pca$sample.id, pop = factor(train.label), EV1 = pca$eigenvect[, 1], EV2 = pca$eigenvect[, 2], stringsAsFactors = FALSE)
+
+
+# Calculate SNP loadings
+snp_loadings <- SNPRelate::snpgdsPCASNPLoading(pca, all.maf, num.thread = num.threads)
+
+# Calculate sample loadings for testing set using SNP loadings
+sample_loadings <- SNPRelate::snpgdsPCASampLoading(snp_loadings, all.maf, sample.id = test.id, num.thread = num.threads)
+
+# Extract list of SNPs used in PCA
+snpset.id <- unlist(all.maf)
+snp_list <- SNPRelate::snpgdsSNPList(all.maf)
+snp_rs_id <- snp_list$snp.id[pca$snp.id]
+snp_chrom <- snp_list$chromosome[pca$snp.id]
+snp_pos <- snp_list$position[pca$snp.id]
+
+# Create dataframe with one SNP per row
+snp_df <- data.frame(snp_rs_id, snp_chrom, snp_pos)
+
+# Transpose SNP loading table
+snp_loadings_transposed <- t(snp_loadings$snploading)
+
+# Match SNPs with loading vectors
+loading_mat <- data.frame(snp_df, snp_loadings_transposed[, 1:num.eigenvectors])
+
+# Write loading data frame to file
+
+# Create data frames, where sample is paired with eigenvectors
+train_eigenvects <- data.frame(pca$sample.id, pca$eigenvect[, 1:num.eigenvectors])
+test_eigenvects <- data.frame(sample_loadings$sample.id, sample_loadings$eigenvect[, 1:num.eigenvectors])
+
+# Ensure sample.id column is same for all data frames
+colnames(train_eigenvects)[1] <- "sample.id"
+colnames(test_eigenvects)[1] <- "sample.id"
+
+# Create a dataframe with 'sample.id' and 'pop' columns
+sample.info <- data.frame(sample.id = all.id, pop = all.label)
+
+# Merge data frames
+train_df <- merge(sample.info, train_eigenvects)
+test_df <- merge(sample.info, test_eigenvects)
+
+
+# Function to train a random forest with optional randomized labels
+train_random_forest <- function(
+    train_df,
+    test_df,
+    num.eigenvectors,
+    randomize_labels = FALSE) {
+    if (randomize_labels) {
+        # Randomize training labels
+        train_df$random_pop <- sample(train_df$pop)
+        y_train <- as.factor(train_df$random_pop)
+    } else {
+        y_train <- as.factor(train_df$pop)
+    }
+
+    # Train random forest
+    rf <- randomForest::randomForest(
+        train_df[, (num.eigenvectors - 2):(2 + num.eigenvectors)],
+        y = y_train,
+        data = train_df,
+        xtest = test_df[, (num.eigenvectors - 2):(2 + num.eigenvectors)],
+        ytest = as.factor(test_df$pop),
+        ntree = 500,
+        replace = FALSE,
+        proximity = TRUE
+    )
+
+    return(rf)
+}
+
+oob_sum <- function(...) { # This is so stupid omg
+    png(tempfile(fileext = ".png"))
+    sink("file")
+    xt <- summary(...)
+    dev.off()
+    sink()
+    return(xt$data)
+}
+
+
+# Print confusion matrix
+rfPermute::confusionMatrix(output.forest)
+rfPermute::plotPredictedProbs(output.forest, bins = 20, plot = TRUE)
+rfPermute::plotProximity(output.forest)
+
+
+# Train 100 random forests with randomized labels
+set.seed(123) # For reproducibility
+randrandf <- replicate(20, train_random_forest(
+    train_df, test_df, num.eigenvectors,
+    randomize_labels = TRUE
+),
+simplify = FALSE
+)
+# Extract OOB error rates
+random_oob_data <- lapply(randrandf, function(rf) {
+    oob_sum(rf)
+})
+
+random_oob_df <- dplyr::as_tibble(do.call(rbind, random_oob_data))
+
+
+# Train 100 random forests with original labels
+set.seed(123) # For reproducibility
+randf <- replicate(20, train_random_forest(
+    train_df, test_df, num.eigenvectors,
+    randomize_labels = FALSE
+),
+simplify = FALSE
+)
+
+# Extract OOB error rates
+oob_data <- lapply(randf, function(rf) {
+    oob_sum(rf)
+})
+
+oob_df <- dplyr::as_tibble(do.call(rbind, oob_data))
+
+
+# Calculate mean error for original data
+mean_oob_df <- oob_df |>
+    dplyr::group_by(trees, class) |>
+    dplyr::summarise(mean_error = mean(error), .groups = "drop")
+
+# Calculate mean error for randomized data
+mean_random_oob_df <- random_oob_df |>
+    dplyr::group_by(trees, class) |>
+    dplyr::summarise(mean_error = mean(error), .groups = "drop")
+
+# Add random forest results to the plot
+ggplot2::ggplot() +
+    # Original data (individual lines with low opacity)
+    ggplot2::geom_line(
+        data = oob_df,
+        ggplot2::aes(x = trees, y = error, color = class), alpha = 0.1
+    ) +
+    # Random forest results (individual lines with low opacity)
+    ggplot2::geom_line(
+        data = random_oob_df,
+        ggplot2::aes(x = trees, y = error, color = class), alpha = 0.1, linetype = "dashed"
+    ) +
+    # Smooth mean lines for original data
+    ggplot2::geom_line(
+        data = mean_oob_df,
+        ggplot2::aes(x = trees, y = mean_error, color = class), size = 1.5
+    ) +
+    # Smooth mean lines for randomized data
+    ggplot2::geom_line(
+        data = mean_random_oob_df,
+        ggplot2::aes(x = trees, y = mean_error, color = class), size = 1.5,
+        linetype = "dashed"
+    ) +
+    ggplot2::theme_minimal() +
+    ggplot2::labs(
+        title = "OOB error rate (Solid: Original, Dashed: Randomized)",
+        x = "Number of trees", y = "Error rate"
+    ) +
+    ggplot2::scale_color_manual(values = c(
+        "OOB" = "#4e4e4e",
+        "immigrant" = "#5d8566",
+        "resident" = "#c29007"
+    )) +
+    ggplot2::coord_cartesian(ylim = c(0, 100)) +
+    ggplot2::geom_hline(yintercept = 50, linetype = "dotted", color = "red") +
+    ggplot2::theme(
+        aspect.ratio = 1,
+        text = ggplot2::element_text(size = 12)
+    )

# time: 2024-08-29 15:43:43 UTC
# mode: r
+all.id <- gdsfmt::read.gdsn(gdsfmt::index.gdsn(all.maf, "sample.id"))
+
+# Subsample data to balance immigrant and resident samples
+min_samples <- min(length(immigrant.id), length(resident.id))
+all.id <- c(sample(immigrant.id, min_samples), sample(resident.id, min_samples))
+all.label <- ifelse(all.id %in% immigrant.id, "immigrant", "resident")
+
+# Split data into training and testing sets
+num_train <- round(length(all.id) * percent.train)
+train.id <- sample(all.id, num_train)
+test.id <- setdiff(all.id, train.id)
+
+train.label <- all.label[all.id %in% train.id]
+test.label <- all.label[all.id %in% test.id]
+
+# Check proportions
+prop_train <- table(train.label) / length(train.label)
+prop_test <- table(test.label) / length(test.label)
+
+# Verify no overlap between train and test sets
+if (length(intersect(train.id, test.id)) > 0) stop("Overlap between train and test sets")
+
+# Reduce dimensionality via pca
+num.threads <- 4
+pca <- SNPRelate::snpgdsPCA(all.maf, sample.id = train.id, num.thread = num.threads)
+
+tab <- data.frame(sample.id = pca$sample.id, pop = factor(train.label), EV1 = pca$eigenvect[, 1], EV2 = pca$eigenvect[, 2], stringsAsFactors = FALSE)
+
+
+# Calculate SNP loadings
+snp_loadings <- SNPRelate::snpgdsPCASNPLoading(pca, all.maf, num.thread = num.threads)
+
+# Calculate sample loadings for testing set using SNP loadings
+sample_loadings <- SNPRelate::snpgdsPCASampLoading(snp_loadings, all.maf, sample.id = test.id, num.thread = num.threads)
+
+# Extract list of SNPs used in PCA
+snpset.id <- unlist(all.maf)
+snp_list <- SNPRelate::snpgdsSNPList(all.maf)
+snp_rs_id <- snp_list$snp.id[pca$snp.id]
+snp_chrom <- snp_list$chromosome[pca$snp.id]
+snp_pos <- snp_list$position[pca$snp.id]
+
+# Create dataframe with one SNP per row
+snp_df <- data.frame(snp_rs_id, snp_chrom, snp_pos)
+
+# Transpose SNP loading table
+snp_loadings_transposed <- t(snp_loadings$snploading)
+
+# Match SNPs with loading vectors
+loading_mat <- data.frame(snp_df, snp_loadings_transposed[, 1:num.eigenvectors])
+
+# Write loading data frame to file
+
+# Create data frames, where sample is paired with eigenvectors
+train_eigenvects <- data.frame(pca$sample.id, pca$eigenvect[, 1:num.eigenvectors])
+test_eigenvects <- data.frame(sample_loadings$sample.id, sample_loadings$eigenvect[, 1:num.eigenvectors])
+
+# Ensure sample.id column is same for all data frames
+colnames(train_eigenvects)[1] <- "sample.id"
+colnames(test_eigenvects)[1] <- "sample.id"
+
+# Create a dataframe with 'sample.id' and 'pop' columns
+sample.info <- data.frame(sample.id = all.id, pop = all.label)
+
+# Merge data frames
+train_df <- merge(sample.info, train_eigenvects)
+test_df <- merge(sample.info, test_eigenvects)
+
+
+# Function to train a random forest with optional randomized labels
+train_random_forest <- function(
+    train_df,
+    test_df,
+    num.eigenvectors,
+    randomize_labels = FALSE) {
+    if (randomize_labels) {
+        # Randomize training labels
+        train_df$random_pop <- sample(train_df$pop)
+        y_train <- as.factor(train_df$random_pop)
+    } else {
+        y_train <- as.factor(train_df$pop)
+    }
+
+    # Train random forest
+    rf <- randomForest::randomForest(
+        train_df[, (num.eigenvectors - 2):(2 + num.eigenvectors)],
+        y = y_train,
+        data = train_df,
+        xtest = test_df[, (num.eigenvectors - 2):(2 + num.eigenvectors)],
+        ytest = as.factor(test_df$pop),
+        ntree = 500,
+        replace = FALSE,
+        proximity = TRUE
+    )
+
+    return(rf)
+}
+
+oob_sum <- function(...) { # This is so stupid omg
+    png(tempfile(fileext = ".png"))
+    sink("file")
+    xt <- summary(...)
+    dev.off()
+    sink()
+    return(xt$data)
+}
+
+
+# Print confusion matrix
+# rfPermute::confusionMatrix(output.forest)
+# rfPermute::plotPredictedProbs(output.forest, bins = 20, plot = TRUE)
+# rfPermute::plotProximity(output.forest)
+
+
+# Train 100 random forests with randomized labels
+set.seed(123) # For reproducibility
+randrandf <- replicate(20, train_random_forest(
+    train_df, test_df, num.eigenvectors,
+    randomize_labels = TRUE
+),
+simplify = FALSE
+)
+# Extract OOB error rates
+random_oob_data <- lapply(randrandf, function(rf) {
+    oob_sum(rf)
+})
+
+random_oob_df <- dplyr::as_tibble(do.call(rbind, random_oob_data))
+
+
+# Train 100 random forests with original labels
+set.seed(123) # For reproducibility
+randf <- replicate(20, train_random_forest(
+    train_df, test_df, num.eigenvectors,
+    randomize_labels = FALSE
+),
+simplify = FALSE
+)
+
+# Extract OOB error rates
+oob_data <- lapply(randf, function(rf) {
+    oob_sum(rf)
+})
+
+oob_df <- dplyr::as_tibble(do.call(rbind, oob_data))
+
+
+# Calculate mean error for original data
+mean_oob_df <- oob_df |>
+    dplyr::group_by(trees, class) |>
+    dplyr::summarise(mean_error = mean(error), .groups = "drop")
+
+# Calculate mean error for randomized data
+mean_random_oob_df <- random_oob_df |>
+    dplyr::group_by(trees, class) |>
+    dplyr::summarise(mean_error = mean(error), .groups = "drop")
+
+# Add random forest results to the plot
+ggplot2::ggplot() +
+    # Original data (individual lines with low opacity)
+    ggplot2::geom_line(
+        data = oob_df,
+        ggplot2::aes(x = trees, y = error, color = class), alpha = 0.1
+    ) +
+    # Random forest results (individual lines with low opacity)
+    ggplot2::geom_line(
+        data = random_oob_df,
+        ggplot2::aes(x = trees, y = error, color = class), alpha = 0.1, linetype = "dashed"
+    ) +
+    # Smooth mean lines for original data
+    ggplot2::geom_line(
+        data = mean_oob_df,
+        ggplot2::aes(x = trees, y = mean_error, color = class), size = 1.5
+    ) +
+    # Smooth mean lines for randomized data
+    ggplot2::geom_line(
+        data = mean_random_oob_df,
+        ggplot2::aes(x = trees, y = mean_error, color = class), size = 1.5,
+        linetype = "dashed"
+    ) +
+    ggplot2::theme_minimal() +
+    ggplot2::labs(
+        title = "OOB error rate (Solid: Original, Dashed: Randomized)",
+        x = "Number of trees", y = "Error rate"
+    ) +
+    ggplot2::scale_color_manual(values = c(
+        "OOB" = "#4e4e4e",
+        "immigrant" = "#5d8566",
+        "resident" = "#c29007"
+    )) +
+    ggplot2::coord_cartesian(ylim = c(0, 100)) +
+    ggplot2::geom_hline(yintercept = 50, linetype = "dotted", color = "red") +
+    ggplot2::theme(
+        aspect.ratio = 1,
+        text = ggplot2::element_text(size = 12)
+    )

# time: 2024-08-29 15:52:27 UTC
# mode: r
+# Function to split data into training and testing sets
+split_data <- function(all.id, all.label, percent.train) {
+    num_train <- round(length(all.id) * percent.train)
+    train.id <- sample(all.id, num_train)
+    test.id <- setdiff(all.id, train.id)
+    train.label <- all.label[all.id %in% train.id]
+    test.label <- all.label[all.id %in% test.id]
+
+    list(
+        train.id = train.id, test.id = test.id,
+        train.label = train.label, test.label = test.label
+    )
+}
+
+# Function to perform PCA and prepare data frames
+prepare_data <- function(all.maf, train.id, test.id, train.label, test.label, num.eigenvectors, num.threads) {
+    pca <- SNPRelate::snpgdsPCA(all.maf, sample.id = train.id, num.thread = num.threads)
+    snp_loadings <- SNPRelate::snpgdsPCASNPLoading(pca, all.maf, num.thread = num.threads)
+    sample_loadings <- SNPRelate::snpgdsPCASampLoading(snp_loadings, all.maf, sample.id = test.id, num.thread = num.threads)
+
+    train_eigenvects <- data.frame(sample.id = pca$sample.id, pca$eigenvect[, 1:num.eigenvectors])
+    test_eigenvects <- data.frame(sample.id = sample_loadings$sample.id, sample_loadings$eigenvect[, 1:num.eigenvectors])
+
+    train_df <- data.frame(sample.id = train.id, pop = train.label, train_eigenvects[, -1])
+    test_df <- data.frame(sample.id = test.id, pop = test.label, test_eigenvects[, -1])
+
+    list(train_df = train_df, test_df = test_df)
+}
+
+# Function to train a random forest with optional randomized labels
+train_random_forest <- function(train_df, test_df, num.eigenvectors, randomize_labels = FALSE) {
+    if (randomize_labels) {
+        train_df$pop <- sample(train_df$pop)
+    }
+
+    rf <- randomForest::randomForest(
+        train_df[, 3:(2 + num.eigenvectors)],
+        y = as.factor(train_df$pop),
+        data = train_df,
+        xtest = test_df[, 3:(2 + num.eigenvectors)],
+        ytest = as.factor(test_df$pop),
+        ntree = 500,
+        replace = FALSE,
+        proximity = TRUE
+    )
+
+    return(rf)
+}
+
+# Function to extract OOB error rates
+oob_sum <- function(...) {
+    png(tempfile(fileext = ".png"))
+    sink("file")
+    xt <- summary(...)
+    dev.off()
+    sink()
+    return(xt$data)
+}
+
+# Function to run a single iteration
+run_iteration <- function(all.id, all.label, all.maf, percent.train, num.eigenvectors, num.threads, randomize_labels) {
+    # Split data
+    split <- split_data(all.id, all.label, percent.train)
+
+    # Prepare data
+    data <- prepare_data(all.maf, split$train.id, split$test.id, split$train.label, split$test.label, num.eigenvectors, num.threads)
+
+    # Train random forest
+    rf <- train_random_forest(data$train_df, data$test_df, num.eigenvectors, randomize_labels)
+
+    # Extract OOB error rates
+    oob_data <- oob_sum(rf)
+
+    return(oob_data)
+}
+
+# Main function to run multiple iterations
+run_analysis <- function(all.id, all.label, all.maf, percent.train, num.eigenvectors, num.threads, n_iterations = 20) {
+    set.seed(123) # For reproducibility
+
+    # Run iterations with original labels
+    original_results <- replicate(n_iterations,
+        run_iteration(all.id, all.label, all.maf, percent.train, num.eigenvectors, num.threads, FALSE),
+        simplify = FALSE
+    )
+
+    # Run iterations with randomized labels
+    random_results <- replicate(n_iterations,
+        run_iteration(all.id, all.label, all.maf, percent.train, num.eigenvectors, num.threads, TRUE),
+        simplify = FALSE
+    )
+
+    # Combine results
+    oob_df <- dplyr::as_tibble(do.call(rbind, original_results))
+    random_oob_df <- dplyr::as_tibble(do.call(rbind, random_results))
+
+    list(oob_df = oob_df, random_oob_df = random_oob_df)
+}
+
+# Run the analysis
+results <- run_analysis(all.id, all.label, all.maf, percent.train, num.eigenvectors, num.threads)
+
+# Calculate mean error for original data
+mean_oob_df <- results$oob_df |>
+    dplyr::group_by(trees, class) |>
+    dplyr::summarise(mean_error = mean(error), .groups = "drop")
+
+# Calculate mean error for randomized data
+mean_random_oob_df <- results$random_oob_df |>
+    dplyr::group_by(trees, class) |>
+    dplyr::summarise(mean_error = mean(error), .groups = "drop")
+
+# Create the plot
+ggplot2::ggplot() +
+    ggplot2::geom_line(data = results$oob_df, ggplot2::aes(x = trees, y = error, color = class), alpha = 0.1) +
+    ggplot2::geom_line(data = results$random_oob_df, ggplot2::aes(x = trees, y = error, color = class), alpha = 0.1, linetype = "dashed") +
+    ggplot2::geom_line(data = mean_oob_df, ggplot2::aes(x = trees, y = mean_error, color = class), size = 1.5) +
+    ggplot2::geom_line(data = mean_random_oob_df, ggplot2::aes(x = trees, y = mean_error, color = class), size = 1.5, linetype = "dashed") +
+    ggplot2::theme_minimal() +
+    ggplot2::labs(title = "OOB error rate (Solid: Original, Dashed: Randomized)", x = "Number of trees", y = "Error rate") +
+    ggplot2::scale_color_manual(values = c("OOB" = "#4e4e4e", "immigrant" = "#5d8566", "resident" = "#c29007")) +
+    ggplot2::coord_cartesian(ylim = c(0, 100)) +
+    ggplot2::geom_hline(yintercept = 50, linetype = "dotted", color = "red") +
+    ggplot2::theme(aspect.ratio = 1, text = ggplot2::element_text(size = 12))

# time: 2024-08-29 16:06:13 UTC
# mode: r
+rfPermute::plotProximity(output.forest)

# time: 2024-08-29 16:06:18 UTC
# mode: r
+rfPermute::plotPredictedProbs(output.forest, bins = 20, plot = TRUE)

# time: 2024-08-29 16:06:23 UTC
# mode: r
+rfPermute::confusionMatrix(output.forest)

# time: 2024-08-29 16:06:25 UTC
# mode: r
+rfPermute::plotPredictedProbs(output.forest, bins = 20, plot = TRUE)

# time: 2024-08-29 16:06:33 UTC
# mode: r
+summary(output.forest)

# time: 2024-08-29 16:07:49 UTC
# mode: r
+ggplot2::ggplot() +
+    ggplot2::geom_line(data = results$oob_df, ggplot2::aes(x = trees, y = error, color = class), alpha = 0.1) +
+    ggplot2::geom_line(data = results$random_oob_df, ggplot2::aes(x = trees, y = error, color = class), alpha = 0.1, linetype = "dashed") +
+    ggplot2::geom_line(data = mean_oob_df, ggplot2::aes(x = trees, y = mean_error, color = class), size = 1.5) +
+    ggplot2::geom_line(data = mean_random_oob_df, ggplot2::aes(x = trees, y = mean_error, color = class), size = 1.5, linetype = "dashed") +
+    ggplot2::theme_minimal() +
+    ggplot2::labs(title = "OOB error rate (Solid: Original, Dashed: Randomized)", x = "Number of trees", y = "Percent correct") +
+    ggplot2::scale_color_manual(values = c("OOB" = "#4e4e4e", "immigrant" = "#5d8566", "resident" = "#c29007")) +
+    ggplot2::coord_cartesian(ylim = c(0, 100)) +
+    ggplot2::geom_hline(yintercept = 50, linetype = "dotted", color = "red") +
+    ggplot2::theme(aspect.ratio = 1, text = ggplot2::element_text(size = 12)) +
+    ggplot2::scale_y_continuous(labels = scales::percent_format())

# time: 2024-08-29 16:10:03 UTC
# mode: r
+ggplot2::ggplot() +
+    ggplot2::geom_line(data = results$oob_df, ggplot2::aes(
+        x = trees, y = error, color = class
+    ), alpha = 0.1) +
+    ggplot2::geom_line(data = results$random_oob_df, ggplot2::aes(
+        x = trees, y = error, color = class
+    ), alpha = 0.1, linetype = "dashed") +
+    ggplot2::geom_line(data = mean_oob_df, ggplot2::aes(
+        x = trees, y = mean_error, color = class
+    ), size = 1.5) +
+    ggplot2::geom_line(data = mean_random_oob_df, ggplot2::aes(
+        x = trees, y = mean_error, color = class
+    ), size = 1.5, linetype = "dashed") +
+    ggplot2::theme_minimal() +
+    ggplot2::labs(
+        title = "OOB error rate (Solid: Original, Dashed: Randomized)",
+        x = "Number of trees", y = "Percent correct"
+    ) +
+    ggplot2::scale_color_manual(values = c(
+        "OOB" = "#4e4e4e",
+        "immigrant" = "#5d8566", "resident" = "#c29007"
+    )) +
+    ggplot2::coord_cartesian(ylim = c(0, 100)) +
+    ggplot2::geom_hline(yintercept = 50, linetype = "dotted", color = "red") +
+    ggplot2::theme(aspect.ratio = 1, text = ggplot2::element_text(size = 12))

# time: 2024-08-29 16:10:23 UTC
# mode: r
+# Create the plot
+ggplot2::ggplot() +
+    ggplot2::geom_line(data = results$oob_df, ggplot2::aes(
+        x = trees, y = error, color = class
+    ), alpha = 0.1) +
+    ggplot2::geom_line(data = results$random_oob_df, ggplot2::aes(
+        x = trees, y = error, color = class
+    ), alpha = 0.1, linetype = "dashed") +
+    ggplot2::geom_line(data = mean_oob_df, ggplot2::aes(
+        x = trees, y = mean_error, color = class
+    ), size = 1.5) +
+    ggplot2::geom_line(data = mean_random_oob_df, ggplot2::aes(
+        x = trees, y = mean_error, color = class
+    ), size = 1.5, linetype = "dashed") +
+    ggplot2::theme_minimal() +
+    ggplot2::labs(
+        title = "OOB error rate (Solid: Original, Dashed: Randomized)",
+        x = "Number of trees", y = "Percent correct"
+    ) +
+    ggplot2::scale_color_manual(values = c(
+        "OOB" = "#4e4e4e",
+        "immigrant" = "#5d8566", "resident" = "#c29007"
+    )) +
+    ggplot2::coord_cartesian(ylim = c(0, 100)) +
+    ggplot2::geom_hline(yintercept = 50, linetype = "dotted", color = "red") +
+    ggplot2::theme(aspect.ratio = 1, text = ggplot2::element_text(size = 12)) +
+    ggplot2::scale_y_continuous(labels = scales::percent_format(scale=1))

# time: 2024-08-29 16:13:26 UTC
# mode: r
+rfPermute::varImpPlot(output.forest)

# time: 2024-08-29 16:14:19 UTC
# mode: r
+rfPermute::plotTrace(output.forest)

# time: 2024-08-29 16:15:06 UTC
# mode: r
+# Create the plot
+ggplot2::ggplot() +
+    ggplot2::geom_line(data = results$oob_df, ggplot2::aes(
+        x = trees, y = error, color = class
+    ), alpha = 0.1) +
+    ggplot2::geom_line(data = results$random_oob_df, ggplot2::aes(
+        x = trees, y = error, color = class
+    ), alpha = 0.1, linetype = "dashed") +
+    ggplot2::geom_line(data = mean_oob_df, ggplot2::aes(
+        x = trees, y = mean_error, color = class
+    ), size = 1.5) +
+    ggplot2::geom_line(data = mean_random_oob_df, ggplot2::aes(
+        x = trees, y = mean_error, color = class
+    ), size = 1.5, linetype = "dashed") +
+    ggplot2::theme_minimal() +
+    ggplot2::labs(
+        title = "OOB error rate (Solid: Original, Dashed: Randomized)",
+        x = "Number of trees", y = "Percent correct"
+    ) +
+    ggplot2::scale_color_manual(values = c(
+        "OOB" = "#4e4e4e",
+        "immigrant" = "#5d8566", "resident" = "#c29007"
+    )) +
+    ggplot2::coord_cartesian(ylim = c(0, 100)) +
+    ggplot2::geom_hline(yintercept = 50, linetype = "dotted", color = "red") +
+    ggplot2::theme(aspect.ratio = 1, text = ggplot2::element_text(size = 12)) +
+    ggplot2::scale_y_continuous(labels = scales::percent_format(scale=1))

# time: 2024-08-29 16:15:18 UTC
# mode: r
+# Create the plot
+
+oobplot = 
+ggplot2::ggplot() +
+    ggplot2::geom_line(data = results$oob_df, ggplot2::aes(
+        x = trees, y = error, color = class
+    ), alpha = 0.1) +
+    ggplot2::geom_line(data = results$random_oob_df, ggplot2::aes(
+        x = trees, y = error, color = class
+    ), alpha = 0.1, linetype = "dashed") +
+    ggplot2::geom_line(data = mean_oob_df, ggplot2::aes(
+        x = trees, y = mean_error, color = class
+    ), size = 1.5) +
+    ggplot2::geom_line(data = mean_random_oob_df, ggplot2::aes(
+        x = trees, y = mean_error, color = class
+    ), size = 1.5, linetype = "dashed") +
+    ggplot2::theme_minimal() +
+    ggplot2::labs(
+        title = "OOB error rate (Solid: Original, Dashed: Randomized)",
+        x = "Number of trees", y = "Percent correct"
+    ) +
+    ggplot2::scale_color_manual(values = c(
+        "OOB" = "#4e4e4e",
+        "immigrant" = "#5d8566", "resident" = "#c29007"
+    )) +
+    ggplot2::coord_cartesian(ylim = c(0, 100)) +
+    ggplot2::geom_hline(yintercept = 50, linetype = "dotted", color = "red") +
+    ggplot2::theme(aspect.ratio = 1, text = ggplot2::element_text(size = 12)) +
+    ggplot2::scale_y_continuous(labels = scales::percent_format(scale=1))

# time: 2024-08-29 16:15:26 UTC
# mode: r
+# Save the plot
+ggplot2::ggsave(file.path(config$path$output, "oob_plot.png"), oobplot, width = 8, height = 6)

# time: 2024-08-29 16:15:42 UTC
# mode: r
+config$path$output

# time: 2024-08-29 16:16:16 UTC
# mode: r
+# Save the plot
+ggplot2::ggsave(file.path(config$path$figures, "oob_plot.png"), oobplot, width = 8, height = 6)

# time: 2024-08-29 16:16:38 UTC
# mode: r
+# Save the plot
+ggplot2::ggsave(file.path(config$path$figures, "oob_plot.jpg"), plot =
+    oobplot,
+    width = 8, 
+    height = 6
+)

# time: 2024-08-30 10:05:08 UTC
# mode: r
+# Import configuration file
+config <- config::get()
+
+# parameters
+percent.train <- 0.7
+num.threads <- 44
+num.eigenvectors <- 20
+set.seed(555)
+
+# Input files
+immigrant.info <- file.path(config$path$data, "600K_immigrants.fam")
+immigrant.vcf <- file.path(config$path$data, "600K_immigrants.vcf")
+resident.vcf <- file.path(config$path$data, "600K_residents.vcf")
+
+immigrant.gds <- file.path(config$path$data, "immigrant_recode.gds")
+resident.gds <- file.path(config$path$data, "resident_recode.gds")
+all.gds <- file.path(config$path$data, "all_recode.gds")
+
+# remove any existing gds files before creating new ones
+file.remove(list.files(path = config$path$data, pattern = "*.gds$", full.names = TRUE))
+SNPRelate::snpgdsVCF2GDS(immigrant.vcf, immigrant.gds, method = "biallelic.only")
+SNPRelate::snpgdsVCF2GDS(resident.vcf, resident.gds, method = "biallelic.only")
+gdsfmt::showfile.gds(closeall = TRUE)
+# Append the immigrant and resident data
+SNPRelate::snpgdsCombineGeno(c(immigrant.gds, resident.gds), all.gds)
+
+# Load all the data
+all.maf <- SNPRelate::snpgdsOpen(all.gds)
+
+# Load labels
+immigrant.id <- gdsfmt::read.gdsn(gdsfmt::index.gdsn(
+    SNPRelate::snpgdsOpen(immigrant.gds),
+    "sample.id"
+))
+resident.id <- gdsfmt::read.gdsn(gdsfmt::index.gdsn(
+    SNPRelate::snpgdsOpen(resident.gds),
+    "sample.id"
+))
+
+all.id <- gdsfmt::read.gdsn(gdsfmt::index.gdsn(all.maf, "sample.id"))
+
+
+# Function to split data into training and testing sets
+split_data <- function(all.id, all.label, percent.train) {
+    num_train <- round(length(all.id) * percent.train)
+    train.id <- sample(all.id, num_train)
+    test.id <- setdiff(all.id, train.id)
+    train.label <- all.label[all.id %in% train.id]
+    test.label <- all.label[all.id %in% test.id]
+
+    list(
+        train.id = train.id, test.id = test.id,
+        train.label = train.label, test.label = test.label
+    )
+}
+
+# Function to perform PCA and prepare data frames
+prepare_data <- function(
+    all.maf, train.id, test.id,
+    train.label, test.label, num.eigenvectors, num.threads) {
+    pca <- SNPRelate::snpgdsPCA(
+        all.maf,
+        sample.id = train.id, num.thread = num.threads
+    )
+    snp_loadings <- SNPRelate::snpgdsPCASNPLoading(
+        pca, all.maf,
+        num.thread = num.threads
+    )
+    sample_loadings <- SNPRelate::snpgdsPCASampLoading(
+        snp_loadings, all.maf,
+        sample.id = test.id, num.thread = num.threads
+    )
+
+    train_eigenvects <- data.frame(
+        sample.id = pca$sample.id, pca$eigenvect[, 1:num.eigenvectors]
+    )
+    test_eigenvects <- data.frame(
+        sample.id = sample_loadings$sample.id, sample_loadings$eigenvect[, 1:num.eigenvectors]
+    )
+
+    train_df <- data.frame(sample.id = train.id, pop = train.label, train_eigenvects[, -1])
+    test_df <- data.frame(sample.id = test.id, pop = test.label, test_eigenvects[, -1])
+
+    list(train_df = train_df, test_df = test_df)
+}
+
+# Function to train a random forest with optional randomized labels
+train_random_forest <- function(
+    train_df, test_df, num.eigenvectors, randomize_labels = FALSE
+    ) {
+    if (randomize_labels) {
+        train_df$pop <- sample(train_df$pop)
+    }
+
+    rf <- randomForest::randomForest(
+        train_df[, 3:(2 + num.eigenvectors)],
+        y = as.factor(train_df$pop),
+        data = train_df,
+        xtest = test_df[, 3:(2 + num.eigenvectors)],
+        ytest = as.factor(test_df$pop),
+        ntree = 500,
+        replace = FALSE,
+        proximity = TRUE
+    )
+
+    return(rf)
+}
+
+# Function to extract OOB error rates
+oob_sum <- function(...) {
+    png(tempfile(fileext = ".png"))
+    sink("file")
+    xt <- summary(...)
+    dev.off()
+    sink()
+    return(xt$data)
+}
+
+# Function to run a single iteration
+run_iteration <- function(
+    all.id, all.label, all.maf, percent.train, num.eigenvectors, num.threads, randomize_labels) {
+    # Split data
+    split <- split_data(all.id, all.label, percent.train)
+
+    # Prepare data
+    data <- prepare_data(
+        all.maf, split$train.id, split$test.id, split$train.label, split$test.label, 
+        num.eigenvectors, num.threads
+    )
+
+    # Train random forest
+    rf <- train_random_forest(data$train_df, data$test_df, num.eigenvectors, randomize_labels)
+
+    # Extract OOB error rates
+    oob_data <- oob_sum(rf)
+
+    return(oob_data)
+}
+
+# Main function to run multiple iterations
+run_analysis <- function(
+    all.id, all.label, all.maf, percent.train, 
+    num.eigenvectors, num.threads, 
+    n_iterations = 5) {
+    set.seed(123) # For reproducibility
+
+    # Run iterations with original labels
+    original_results <- replicate(n_iterations,
+        run_iteration(all.id, all.label, all.maf, percent.train, num.eigenvectors, num.threads, FALSE),
+        simplify = FALSE
+    )
+
+    # Run iterations with randomized labels
+    random_results <- replicate(n_iterations,
+        run_iteration(all.id, all.label, all.maf, percent.train, num.eigenvectors, num.threads, TRUE),
+        simplify = FALSE
+    )
+
+    # Combine results
+    oob_df <- dplyr::as_tibble(do.call(rbind, original_results))
+    random_oob_df <- dplyr::as_tibble(do.call(rbind, random_results))
+
+    list(oob_df = oob_df, random_oob_df = random_oob_df)
+}
+
+# Run the analysis
+results <- run_analysis(all.id, all.label, all.maf, percent.train, num.eigenvectors, num.threads)
+
+# Calculate mean error for original data
+mean_oob_df <- results$oob_df |>
+    dplyr::group_by(trees, class) |>
+    dplyr::summarise(mean_error = mean(error), .groups = "drop")
+
+# Calculate mean error for randomized data
+mean_random_oob_df <- results$random_oob_df |>
+    dplyr::group_by(trees, class) |>
+    dplyr::summarise(mean_error = mean(error), .groups = "drop")
+
+# Create the plot
+
+oobplot =
+    ggplot2::ggplot() +
+    ggplot2::geom_line(data = results$oob_df, ggplot2::aes(
+        x = trees, y = error, color = class
+    ), alpha = 0.1) +
+    ggplot2::geom_line(data = results$random_oob_df, ggplot2::aes(
+        x = trees, y = error, color = class
+    ), alpha = 0.1, linetype = "dashed") +
+    ggplot2::geom_line(data = mean_oob_df, ggplot2::aes(
+        x = trees, y = mean_error, color = class
+    ), size = 1.5) +
+    ggplot2::geom_line(data = mean_random_oob_df, ggplot2::aes(
+        x = trees, y = mean_error, color = class
+    ), size = 1.5, linetype = "dashed") +
+    ggplot2::theme_minimal() +
+    ggplot2::labs(
+        title = "OOB error rate (Solid: Original, Dashed: Randomized)",
+        x = "Number of trees", y = "Percent correct"
+    ) +
+    ggplot2::scale_color_manual(values = c(
+        "OOB" = "#4e4e4e",
+        "immigrant" = "#5d8566", "resident" = "#c29007"
+    )) +
+    ggplot2::coord_cartesian(ylim = c(0, 100)) +
+    ggplot2::geom_hline(yintercept = 50, linetype = "dotted", color = "red") +
+    ggplot2::theme(aspect.ratio = 1, text = ggplot2::element_text(size = 12)) +
+    ggplot2::scale_y_continuous(labels = scales::percent_format(scale = 1))
+
+# Save the plot
+ggplot2::ggsave(file.path(config$path$figures, "oob_plot.jpg"),
+    plot =
+        oobplot,
+    width = 8,
+    height = 6
+)
+
+
+# rfPermute::confusionMatrix(output.forest)
+# rfPermute::plotPredictedProbs(output.forest, bins = 20, plot = TRUE)
+# rfPermute::plotProximity(output.forest)
+# summary(output.forest)
+# rfPermute::plotTrace(output.forest)

# time: 2024-08-30 10:09:16 UTC
# mode: r
+# Import configuration file
+config <- config::get()
+
+# parameters
+percent.train <- 0.7
+num.threads <- 44
+num.eigenvectors <- 20
+set.seed(555)
+
+# Input files
+immigrant.info <- file.path(config$path$data, "600K_immigrants.fam")
+immigrant.vcf <- file.path(config$path$data, "600K_immigrants.vcf")
+resident.vcf <- file.path(config$path$data, "600K_residents.vcf")
+
+immigrant.gds <- file.path(config$path$data, "immigrant_recode.gds")
+resident.gds <- file.path(config$path$data, "resident_recode.gds")
+all.gds <- file.path(config$path$data, "all_recode.gds")
+
+# remove any existing gds files before creating new ones
+file.remove(list.files(path = config$path$data, pattern = "*.gds$", full.names = TRUE))
+SNPRelate::snpgdsVCF2GDS(immigrant.vcf, immigrant.gds, method = "biallelic.only")
+SNPRelate::snpgdsVCF2GDS(resident.vcf, resident.gds, method = "biallelic.only")
+gdsfmt::showfile.gds(closeall = TRUE)
+# Append the immigrant and resident data
+SNPRelate::snpgdsCombineGeno(c(immigrant.gds, resident.gds), all.gds)
+
+# Load all the data
+all.maf <- SNPRelate::snpgdsOpen(all.gds)
+
+# Load labels
+immigrant.id <- gdsfmt::read.gdsn(gdsfmt::index.gdsn(
+    SNPRelate::snpgdsOpen(immigrant.gds),
+    "sample.id"
+))
+resident.id <- gdsfmt::read.gdsn(gdsfmt::index.gdsn(
+    SNPRelate::snpgdsOpen(resident.gds),
+    "sample.id"
+))
+
+all.id <- gdsfmt::read.gdsn(gdsfmt::index.gdsn(all.maf, "sample.id"))
+all.label <- ifelse(all.id %in% immigrant.id, "immigrant", "resident")
+
+
+# Function to split data into training and testing sets
+split_data <- function(all.id, all.label, percent.train) {
+    num_train <- round(length(all.id) * percent.train)
+    train.id <- sample(all.id, num_train)
+    test.id <- setdiff(all.id, train.id)
+    train.label <- all.label[all.id %in% train.id]
+    test.label <- all.label[all.id %in% test.id]
+
+    list(
+        train.id = train.id, test.id = test.id,
+        train.label = train.label, test.label = test.label
+    )
+}
+
+# Function to perform PCA and prepare data frames
+prepare_data <- function(
+    all.maf, train.id, test.id,
+    train.label, test.label, num.eigenvectors, num.threads) {
+    pca <- SNPRelate::snpgdsPCA(
+        all.maf,
+        sample.id = train.id, num.thread = num.threads
+    )
+    snp_loadings <- SNPRelate::snpgdsPCASNPLoading(
+        pca, all.maf,
+        num.thread = num.threads
+    )
+    sample_loadings <- SNPRelate::snpgdsPCASampLoading(
+        snp_loadings, all.maf,
+        sample.id = test.id, num.thread = num.threads
+    )
+
+    train_eigenvects <- data.frame(
+        sample.id = pca$sample.id, pca$eigenvect[, 1:num.eigenvectors]
+    )
+    test_eigenvects <- data.frame(
+        sample.id = sample_loadings$sample.id, sample_loadings$eigenvect[, 1:num.eigenvectors]
+    )
+
+    train_df <- data.frame(sample.id = train.id, pop = train.label, train_eigenvects[, -1])
+    test_df <- data.frame(sample.id = test.id, pop = test.label, test_eigenvects[, -1])
+
+    list(train_df = train_df, test_df = test_df)
+}
+
+# Function to train a random forest with optional randomized labels
+train_random_forest <- function(
+    train_df, test_df, num.eigenvectors, randomize_labels = FALSE
+    ) {
+    if (randomize_labels) {
+        train_df$pop <- sample(train_df$pop)
+    }
+
+    rf <- randomForest::randomForest(
+        train_df[, 3:(2 + num.eigenvectors)],
+        y = as.factor(train_df$pop),
+        data = train_df,
+        xtest = test_df[, 3:(2 + num.eigenvectors)],
+        ytest = as.factor(test_df$pop),
+        ntree = 500,
+        replace = FALSE,
+        proximity = TRUE
+    )
+
+    return(rf)
+}
+
+# Function to extract OOB error rates
+oob_sum <- function(...) {
+    png(tempfile(fileext = ".png"))
+    sink("file")
+    xt <- summary(...)
+    dev.off()
+    sink()
+    return(xt$data)
+}
+
+# Function to run a single iteration
+run_iteration <- function(
+    all.id, all.label, all.maf, percent.train, num.eigenvectors, num.threads, randomize_labels) {
+    # Split data
+    split <- split_data(all.id, all.label, percent.train)
+
+    # Prepare data
+    data <- prepare_data(
+        all.maf, split$train.id, split$test.id, split$train.label, split$test.label, 
+        num.eigenvectors, num.threads
+    )
+
+    # Train random forest
+    rf <- train_random_forest(data$train_df, data$test_df, num.eigenvectors, randomize_labels)
+
+    # Extract OOB error rates
+    oob_data <- oob_sum(rf)
+
+    return(oob_data)
+}
+
+# Main function to run multiple iterations
+run_analysis <- function(
+    all.id, all.label, all.maf, percent.train, 
+    num.eigenvectors, num.threads, 
+    n_iterations = 5) {
+    set.seed(123) # For reproducibility
+
+    # Run iterations with original labels
+    original_results <- replicate(n_iterations,
+        run_iteration(all.id, all.label, all.maf, percent.train, num.eigenvectors, num.threads, FALSE),
+        simplify = FALSE
+    )
+
+    # Run iterations with randomized labels
+    random_results <- replicate(n_iterations,
+        run_iteration(all.id, all.label, all.maf, percent.train, num.eigenvectors, num.threads, TRUE),
+        simplify = FALSE
+    )
+
+    # Combine results
+    oob_df <- dplyr::as_tibble(do.call(rbind, original_results))
+    random_oob_df <- dplyr::as_tibble(do.call(rbind, random_results))
+
+    list(oob_df = oob_df, random_oob_df = random_oob_df)
+}
+
+# Run the analysis
+results <- run_analysis(all.id, all.label, all.maf, percent.train, num.eigenvectors, num.threads)
+
+# Calculate mean error for original data
+mean_oob_df <- results$oob_df |>
+    dplyr::group_by(trees, class) |>
+    dplyr::summarise(mean_error = mean(error), .groups = "drop")
+
+# Calculate mean error for randomized data
+mean_random_oob_df <- results$random_oob_df |>
+    dplyr::group_by(trees, class) |>
+    dplyr::summarise(mean_error = mean(error), .groups = "drop")
+
+# Create the plot
+
+oobplot =
+    ggplot2::ggplot() +
+    ggplot2::geom_line(data = results$oob_df, ggplot2::aes(
+        x = trees, y = error, color = class
+    ), alpha = 0.1) +
+    ggplot2::geom_line(data = results$random_oob_df, ggplot2::aes(
+        x = trees, y = error, color = class
+    ), alpha = 0.1, linetype = "dashed") +
+    ggplot2::geom_line(data = mean_oob_df, ggplot2::aes(
+        x = trees, y = mean_error, color = class
+    ), size = 1.5) +
+    ggplot2::geom_line(data = mean_random_oob_df, ggplot2::aes(
+        x = trees, y = mean_error, color = class
+    ), size = 1.5, linetype = "dashed") +
+    ggplot2::theme_minimal() +
+    ggplot2::labs(
+        title = "OOB error rate (Solid: Original, Dashed: Randomized)",
+        x = "Number of trees", y = "Percent correct"
+    ) +
+    ggplot2::scale_color_manual(values = c(
+        "OOB" = "#4e4e4e",
+        "immigrant" = "#5d8566", "resident" = "#c29007"
+    )) +
+    ggplot2::coord_cartesian(ylim = c(0, 100)) +
+    ggplot2::geom_hline(yintercept = 50, linetype = "dotted", color = "red") +
+    ggplot2::theme(aspect.ratio = 1, text = ggplot2::element_text(size = 12)) +
+    ggplot2::scale_y_continuous(labels = scales::percent_format(scale = 1))
+
+# Save the plot
+ggplot2::ggsave(file.path(config$path$figures, "oob_plot.jpg"),
+    plot =
+        oobplot,
+    width = 8,
+    height = 6
+)
+
+
+# rfPermute::confusionMatrix(output.forest)
+# rfPermute::plotPredictedProbs(output.forest, bins = 20, plot = TRUE)
+# rfPermute::plotProximity(output.forest)
+# summary(output.forest)
+# rfPermute::plotTrace(output.forest)

# time: 2024-08-30 10:11:22 UTC
# mode: r
+results

# time: 2024-08-30 10:11:33 UTC
# mode: r
+original_results <- replicate(n_iterations,
+        run_iteration(all.id, all.label, all.maf, percent.train, num.eigenvectors, num.threads, FALSE),
+        simplify = FALSE
+    )

# time: 2024-08-30 10:12:17 UTC
# mode: r
+all.id

# time: 2024-08-30 10:12:22 UTC
# mode: r
+all.label

# time: 2024-08-30 10:12:42 UTC
# mode: r
+split_data(all.id, all.label, percent.train)

# time: 2024-08-30 10:14:57 UTC
# mode: r
+source("/home/nilomr/projects/greti-genomics/main.R", encoding = "UTF-8")

# time: 2024-08-30 10:19:23 UTC
# mode: r
+renv::install('vscdebugger')

# time: 2024-08-30 10:19:53 UTC
# mode: r
+renv::install('vscDebugger')

# time: 2024-08-30 10:22:37 UTC
# mode: r
+renv::paths$library()

# time: 2024-08-30 10:35:38 UTC
# mode: r
+source("/home/nilomr/projects/greti-genomics/0-read-data.R", encoding = "UTF-8")

# time: 2024-08-30 10:36:46 UTC
# mode: r
+# Import configuration file
+config <- config::get()
+
+# parameters
+percent.train <- 0.7
+num.threads <- 44
+num.eigenvectors <- 20
+set.seed(555)
+
+# Input files
+immigrant.info <- file.path(config$path$data, "600K_immigrants.fam")
+immigrant.vcf <- file.path(config$path$data, "600K_immigrants.vcf")
+resident.vcf <- file.path(config$path$data, "600K_residents.vcf")
+
+immigrant.gds <- file.path(config$path$data, "immigrant_recode.gds")
+resident.gds <- file.path(config$path$data, "resident_recode.gds")
+all.gds <- file.path(config$path$data, "all_recode.gds")
+
+
+# Load all the data
+all.maf <- SNPRelate::snpgdsOpen(all.gds)
+
+# Load labels
+immigrant.id <- gdsfmt::read.gdsn(gdsfmt::index.gdsn(
+    SNPRelate::snpgdsOpen(immigrant.gds),
+    "sample.id"
+))
+resident.id <- gdsfmt::read.gdsn(gdsfmt::index.gdsn(
+    SNPRelate::snpgdsOpen(resident.gds),
+    "sample.id"
+))
+
+all.id <- gdsfmt::read.gdsn(gdsfmt::index.gdsn(all.maf, "sample.id"))
+all.label <- ifelse(all.id %in% immigrant.id, "immigrant", "resident")
+
+
+# Function to split data into training and testing sets
+# TODO balence so that .5 per class
+split_data <- function(all.id, all.label, percent.train) {
+    num_train <- round(length(all.id) * percent.train)
+    train.id <- sample(all.id, num_train)
+    test.id <- setdiff(all.id, train.id)
+    train.label <- all.label[all.id %in% train.id]
+    test.label <- all.label[all.id %in% test.id]
+
+    list(
+        train.id = train.id, test.id = test.id,
+        train.label = train.label, test.label = test.label
+    )
+}
+
+# Function to perform PCA and prepare data frames
+prepare_data <- function(
+    all.maf, train.id, test.id,
+    train.label, test.label, num.eigenvectors, num.threads) {
+    pca <- SNPRelate::snpgdsPCA(
+        all.maf,
+        sample.id = train.id, num.thread = num.threads
+    )
+    snp_loadings <- SNPRelate::snpgdsPCASNPLoading(
+        pca, all.maf,
+        num.thread = num.threads
+    )
+    sample_loadings <- SNPRelate::snpgdsPCASampLoading(
+        snp_loadings, all.maf,
+        sample.id = test.id, num.thread = num.threads
+    )
+
+    train_eigenvects <- data.frame(
+        sample.id = pca$sample.id, pca$eigenvect[, 1:num.eigenvectors]
+    )
+    test_eigenvects <- data.frame(
+        sample.id = sample_loadings$sample.id, sample_loadings$eigenvect[, 1:num.eigenvectors]
+    )
+
+    train_df <- data.frame(sample.id = train.id, pop = train.label, train_eigenvects[, -1])
+    test_df <- data.frame(sample.id = test.id, pop = test.label, test_eigenvects[, -1])
+
+    list(train_df = train_df, test_df = test_df)
+}
+
+# Function to train a random forest with optional randomized labels
+train_random_forest <- function(
+    train_df, test_df, num.eigenvectors, randomize_labels = FALSE) {
+    if (randomize_labels) {
+        train_df$pop <- sample(train_df$pop)
+    }
+
+    rf <- randomForest::randomForest(
+        train_df[, 3:(2 + num.eigenvectors)],
+        y = as.factor(train_df$pop),
+        data = train_df,
+        xtest = test_df[, 3:(2 + num.eigenvectors)],
+        ytest = as.factor(test_df$pop),
+        ntree = 500,
+        replace = FALSE,
+        proximity = TRUE
+    )
+
+    return(rf)
+}
+
+
+
+# Function to run a single iteration
+run_iteration <- function(
+    all.id, all.label, all.maf, percent.train, num.eigenvectors, num.threads, randomize_labels) {
+    # Split data
+    split <- split_data(all.id, all.label, percent.train)
+
+    # Prepare data
+    data <- prepare_data(
+        all.maf, split$train.id, split$test.id, split$train.label, split$test.label,
+        num.eigenvectors, num.threads
+    )
+
+    # Train random forest
+    rf <- train_random_forest(data$train_df, data$test_df, num.eigenvectors, randomize_labels)
+
+    # Extract OOB error rates
+    oob_data <- rfPermute::plotTrace(rf, plot = FALSE)$data
+
+    return(oob_data)
+}
+
+# Main function to run multiple iterations
+run_analysis <- function(
+    all.id, all.label, all.maf, percent.train,
+    num.eigenvectors, num.threads,
+    n_iterations = 5) {
+    set.seed(123) # For reproducibility
+
+    # Run iterations with original labels
+    original_results <- replicate(n_iterations,
+        run_iteration(all.id, all.label, all.maf, percent.train, num.eigenvectors, num.threads, FALSE),
+        simplify = FALSE
+    )
+
+    # Run iterations with randomized labels
+    random_results <- replicate(n_iterations,
+        run_iteration(all.id, all.label, all.maf, percent.train, num.eigenvectors, num.threads, TRUE),
+        simplify = FALSE
+    )
+
+    # Combine results
+    oob_df <- dplyr::as_tibble(do.call(rbind, original_results))
+    random_oob_df <- dplyr::as_tibble(do.call(rbind, random_results))
+
+    list(oob_df = oob_df, random_oob_df = random_oob_df)
+}
+
+# Run the analysis
+results <- run_analysis(all.id, all.label, all.maf, percent.train, num.eigenvectors, num.threads)
+
+# Calculate mean error for original data
+mean_oob_df <- results$oob_df |>
+    dplyr::group_by(trees, class) |>
+    dplyr::summarise(mean_error = mean(error), .groups = "drop")
+
+# Calculate mean error for randomized data
+mean_random_oob_df <- results$random_oob_df |>
+    dplyr::group_by(trees, class) |>
+    dplyr::summarise(mean_error = mean(error), .groups = "drop")
+
+# Create the plot
+
+oobplot =
+    ggplot2::ggplot() +
+    ggplot2::geom_line(data = results$oob_df, ggplot2::aes(
+        x = trees, y = error, color = class
+    ), alpha = 0.1) +
+    ggplot2::geom_line(data = results$random_oob_df, ggplot2::aes(
+        x = trees, y = error, color = class
+    ), alpha = 0.1, linetype = "dashed") +
+    ggplot2::geom_line(data = mean_oob_df, ggplot2::aes(
+        x = trees, y = mean_error, color = class
+    ), size = 1.5) +
+    ggplot2::geom_line(data = mean_random_oob_df, ggplot2::aes(
+        x = trees, y = mean_error, color = class
+    ), size = 1.5, linetype = "dashed") +
+    ggplot2::theme_minimal() +
+    ggplot2::labs(
+        title = "OOB error rate (Solid: Original, Dashed: Randomized)",
+        x = "Number of trees", y = "Percent correct"
+    ) +
+    ggplot2::scale_color_manual(values = c(
+        "OOB" = "#4e4e4e",
+        "immigrant" = "#5d8566", "resident" = "#c29007"
+    )) +
+    ggplot2::coord_cartesian(ylim = c(0, 100)) +
+    ggplot2::geom_hline(yintercept = 50, linetype = "dotted", color = "red") +
+    ggplot2::theme(aspect.ratio = 1, text = ggplot2::element_text(size = 12)) +
+    ggplot2::scale_y_continuous(labels = scales::percent_format(scale = 1))
+
+# Save the plot
+ggplot2::ggsave(file.path(config$path$figures, "oob_plot.jpg"),
+    plot =
+        oobplot,
+    width = 8,
+    height = 6
+)
+
+
+# rfPermute::confusionMatrix(output.forest)
+# rfPermute::plotPredictedProbs(output.forest, bins = 20, plot = TRUE)
+# rfPermute::plotProximity(output.forest)
+# summary(output.forest)
+# rfPermute::plotTrace(output.forest)

# time: 2024-08-30 10:41:05 UTC
# mode: r
+# Create the plot
+
+oobplot =
+    ggplot2::ggplot() +
+    ggplot2::geom_line(data = results$oob_df, ggplot2::aes(
+        x = trees, y = error, color = class
+    ), alpha = 0.1) +
+    ggplot2::geom_line(data = results$random_oob_df, ggplot2::aes(
+        x = trees, y = error, color = class
+    ), alpha = 0.1, linetype = "dashed") +
+    ggplot2::geom_line(data = mean_oob_df, ggplot2::aes(
+        x = trees, y = mean_error, color = class
+    ), linewidth = 1.5) +
+    ggplot2::geom_line(data = mean_random_oob_df, ggplot2::aes(
+        x = trees, y = mean_error, color = class
+    ), linewidth = 1.5, linetype = "dashed") +
+    ggplot2::theme_minimal() +
+    ggplot2::labs(
+        title = "OOB error rate (Solid: Original, Dashed: Randomized)",
+        x = "Number of trees", y = "Percent correct"
+    ) +
+    ggplot2::scale_color_manual(values = c(
+        "OOB" = "#4e4e4e",
+        "immigrant" = "#5d8566", "resident" = "#c29007"
+    )) +
+    ggplot2::coord_cartesian(ylim = c(0, 100)) +
+    ggplot2::geom_hline(yintercept = 50, linetype = "dotted", color = "red") +
+    ggplot2::theme(aspect.ratio = 1, text = ggplot2::element_text(linewidth = 12)) +
+    ggplot2::scale_y_continuous(labels = scales::percent_format(scale = 1))

# time: 2024-08-30 10:41:18 UTC
# mode: r
+# Create the plot
+
+oobplot <-
+    ggplot2::ggplot() +
+    ggplot2::geom_line(data = results$oob_df, ggplot2::aes(
+        x = trees, y = error, color = class
+    ), alpha = 0.1) +
+    ggplot2::geom_line(data = results$random_oob_df, ggplot2::aes(
+        x = trees, y = error, color = class
+    ), alpha = 0.1, linetype = "dashed") +
+    ggplot2::geom_line(data = mean_oob_df, ggplot2::aes(
+        x = trees, y = mean_error, color = class
+    ), linewidth = 1.5) +
+    ggplot2::geom_line(data = mean_random_oob_df, ggplot2::aes(
+        x = trees, y = mean_error, color = class
+    ), linewidth = 1.5, linetype = "dashed") +
+    ggplot2::theme_minimal() +
+    ggplot2::labs(
+        title = "OOB error rate (Solid: Original, Dashed: Randomized)",
+        x = "Number of trees", y = "Percent correct"
+    ) +
+    ggplot2::scale_color_manual(values = c(
+        "OOB" = "#4e4e4e",
+        "immigrant" = "#5d8566", "resident" = "#c29007"
+    )) +
+    ggplot2::coord_cartesian(ylim = c(0, 100)) +
+    ggplot2::geom_hline(yintercept = 50, linetype = "dotted", color = "red") +
+    ggplot2::theme(aspect.ratio = 1, text = ggplot2::element_text(size = 12)) +
+    ggplot2::scale_y_continuous(labels = scales::percent_format(scale = 1))

# time: 2024-08-30 10:41:20 UTC
# mode: r
+# Save the plot
+ggplot2::ggsave(file.path(config$path$figures, "oob_plot.jpg"),
+    plot =
+        oobplot,
+    width = 8,
+    height = 6
+)

# time: 2024-08-30 10:42:01 UTC
# mode: r
+results

# time: 2024-08-30 10:42:37 UTC
# mode: r
+# Create the plot
+
+oobplot <-
+    ggplot2::ggplot() +
+    ggplot2::geom_line(data = results$oob_df, ggplot2::aes(
+        x = trees, y = error, color = class
+    ), alpha = 0.1) +
+    ggplot2::geom_line(data = results$random_oob_df, ggplot2::aes(
+        x = trees, y = error, color = class
+    ), alpha = 0.1, linetype = "dashed") +
+    ggplot2::geom_line(data = mean_oob_df, ggplot2::aes(
+        x = trees, y = mean_error, color = class
+    ), linewidth = 1.5) +
+    ggplot2::geom_line(data = mean_random_oob_df, ggplot2::aes(
+        x = trees, y = mean_error, color = class
+    ), linewidth = 1.5, linetype = "dashed") +
+    ggplot2::theme_minimal() +
+    ggplot2::labs(
+        title = "OOB error rate (Solid: Original, Dashed: Randomized)",
+        x = "Number of trees", y = "Percent correct"
+    ) +
+    ggplot2::scale_color_manual(values = c(
+        "OOB" = "#4e4e4e",
+        "immigrant" = "#5d8566", "resident" = "#c29007"
+    )) +
+    ggplot2::coord_cartesian(ylim = c(0, 100)) +
+    ggplot2::geom_hline(yintercept = 50, linetype = "dotted", color = "red") +
+    ggplot2::theme(aspect.ratio = 1, text = ggplot2::element_text(size = 12))

# time: 2024-08-30 10:42:38 UTC
# mode: r
+#ggplot2::scale_y_continuous(labels = scales::percent_format(scale = 1))
+
+# Save the plot
+ggplot2::ggsave(file.path(config$path$figures, "oob_plot.jpg"),
+    plot =
+        oobplot,
+    width = 8,
+    height = 6
+)

# time: 2024-08-30 10:42:49 UTC
# mode: r
+mean_oob_df

# time: 2024-08-30 10:42:55 UTC
# mode: r
+mean_random_oob_df

# time: 2024-08-30 10:43:46 UTC
# mode: r
+all.label

# time: 2024-08-30 10:43:53 UTC
# mode: r
+all.id

# time: 2024-08-30 10:46:18 UTC
# mode: r
+# Import configuration file
+config <- config::get()
+
+# parameters
+percent.train <- 0.7
+num.threads <- 44
+num.eigenvectors <- 20
+set.seed(555)
+
+# Input files
+immigrant.info <- file.path(config$path$data, "600K_immigrants.fam")
+immigrant.vcf <- file.path(config$path$data, "600K_immigrants.vcf")
+resident.vcf <- file.path(config$path$data, "600K_residents.vcf")
+
+immigrant.gds <- file.path(config$path$data, "immigrant_recode.gds")
+resident.gds <- file.path(config$path$data, "resident_recode.gds")
+all.gds <- file.path(config$path$data, "all_recode.gds")
+
+
+# Load all the data
+all.maf <- SNPRelate::snpgdsOpen(all.gds)
+
+# Load labels
+immigrant.id <- gdsfmt::read.gdsn(gdsfmt::index.gdsn(
+    SNPRelate::snpgdsOpen(immigrant.gds),
+    "sample.id"
+))
+resident.id <- gdsfmt::read.gdsn(gdsfmt::index.gdsn(
+    SNPRelate::snpgdsOpen(resident.gds),
+    "sample.id"
+))
+
+all.id <- gdsfmt::read.gdsn(gdsfmt::index.gdsn(all.maf, "sample.id"))
+all.label <- ifelse(all.id %in% immigrant.id, "immigrant", "resident")
+
+
+# Function to split data into training and testing sets
+# TODO balence so that there are .5 per class in both train and test sets
+subset_by_class <- function(ids, labels, num_samples_per_class) {
+    subset_ids <- c()
+    for (class_label in unique(labels)) {
+        class_ids <- ids[labels == class_label]
+        class_ids <- sample(class_ids, num_samples_per_class)
+        subset_ids <- c(subset_ids, class_ids)
+    }
+    subset_ids
+}
+split_data <- function(all.id, all.label, percent.train) {
+    num_train <- round(length(all.id) * percent.train)
+    train.id <- sample(all.id, num_train)
+    test.id <- setdiff(all.id, train.id)
+    
+    # Balance the train and test sets
+    train.label <- all.label[all.id %in% train.id]
+    test.label <- all.label[all.id %in% test.id]
+    
+    # Calculate the number of samples per class
+    num_samples_per_class <- min(table(train.label), table(test.label))
+    
+    # Subset the train and test sets to have equal number of samples per class
+    train.id <- subset_by_class(train.id, train.label, num_samples_per_class)
+    test.id <- subset_by_class(test.id, test.label, num_samples_per_class)
+    
+    list(
+        train.id = train.id, test.id = test.id,
+        train.label = train.label, test.label = test.label
+    )
+}
+
+
+# Function to perform PCA and prepare data frames
+prepare_data <- function(
+    all.maf, train.id, test.id,
+    train.label, test.label, num.eigenvectors, num.threads) {
+    pca <- SNPRelate::snpgdsPCA(
+        all.maf,
+        sample.id = train.id, num.thread = num.threads
+    )
+    snp_loadings <- SNPRelate::snpgdsPCASNPLoading(
+        pca, all.maf,
+        num.thread = num.threads
+    )
+    sample_loadings <- SNPRelate::snpgdsPCASampLoading(
+        snp_loadings, all.maf,
+        sample.id = test.id, num.thread = num.threads
+    )
+
+    train_eigenvects <- data.frame(
+        sample.id = pca$sample.id, pca$eigenvect[, 1:num.eigenvectors]
+    )
+    test_eigenvects <- data.frame(
+        sample.id = sample_loadings$sample.id, sample_loadings$eigenvect[, 1:num.eigenvectors]
+    )
+
+    train_df <- data.frame(sample.id = train.id, pop = train.label, train_eigenvects[, -1])
+    test_df <- data.frame(sample.id = test.id, pop = test.label, test_eigenvects[, -1])
+
+    list(train_df = train_df, test_df = test_df)
+}
+
+# Function to train a random forest with optional randomized labels
+train_random_forest <- function(
+    train_df, test_df, num.eigenvectors, randomize_labels = FALSE) {
+    if (randomize_labels) {
+        train_df$pop <- sample(train_df$pop)
+    }
+
+    rf <- randomForest::randomForest(
+        train_df[, 3:(2 + num.eigenvectors)],
+        y = as.factor(train_df$pop),
+        data = train_df,
+        xtest = test_df[, 3:(2 + num.eigenvectors)],
+        ytest = as.factor(test_df$pop),
+        ntree = 500,
+        replace = FALSE,
+        proximity = TRUE
+    )
+
+    return(rf)
+}
+
+
+
+# Function to run a single iteration
+run_iteration <- function(
+    all.id, all.label, all.maf, percent.train, num.eigenvectors, num.threads, randomize_labels) {
+    # Split data
+    split <- split_data(all.id, all.label, percent.train)
+
+    # Prepare data
+    data <- prepare_data(
+        all.maf, split$train.id, split$test.id, split$train.label, split$test.label,
+        num.eigenvectors, num.threads
+    )
+
+    # Train random forest
+    rf <- train_random_forest(data$train_df, data$test_df, num.eigenvectors, randomize_labels)
+
+    # Extract OOB error rates
+    oob_data <- rfPermute::plotTrace(rf, plot = FALSE)$data
+
+    return(oob_data)
+}
+
+# Main function to run multiple iterations
+run_analysis <- function(
+    all.id, all.label, all.maf, percent.train,
+    num.eigenvectors, num.threads,
+    n_iterations = 5) {
+    set.seed(123) # For reproducibility
+
+    # Run iterations with original labels
+    original_results <- replicate(n_iterations,
+        run_iteration(all.id, all.label, all.maf, percent.train, num.eigenvectors, num.threads, FALSE),
+        simplify = FALSE
+    )
+
+    # Run iterations with randomized labels
+    random_results <- replicate(n_iterations,
+        run_iteration(all.id, all.label, all.maf, percent.train, num.eigenvectors, num.threads, TRUE),
+        simplify = FALSE
+    )
+
+    # Combine results
+    oob_df <- dplyr::as_tibble(do.call(rbind, original_results))
+    random_oob_df <- dplyr::as_tibble(do.call(rbind, random_results))
+
+    list(oob_df = oob_df, random_oob_df = random_oob_df)
+}
+
+# Run the analysis
+results <- run_analysis(all.id, all.label, all.maf, percent.train, num.eigenvectors, num.threads)
+
+# Calculate mean error for original data
+mean_oob_df <- results$oob_df |>
+    dplyr::group_by(trees, class) |>
+    dplyr::summarise(mean_error = mean(error), .groups = "drop")
+
+# Calculate mean error for randomized data
+mean_random_oob_df <- results$random_oob_df |>
+    dplyr::group_by(trees, class) |>
+    dplyr::summarise(mean_error = mean(error), .groups = "drop")
+
+# Create the plot
+
+oobplot <-
+    ggplot2::ggplot() +
+    ggplot2::geom_line(data = results$oob_df, ggplot2::aes(
+        x = trees, y = error, color = class
+    ), alpha = 0.1) +
+    ggplot2::geom_line(data = results$random_oob_df, ggplot2::aes(
+        x = trees, y = error, color = class
+    ), alpha = 0.1, linetype = "dashed") +
+    ggplot2::geom_line(data = mean_oob_df, ggplot2::aes(
+        x = trees, y = mean_error, color = class
+    ), linewidth = 1.5) +
+    ggplot2::geom_line(data = mean_random_oob_df, ggplot2::aes(
+        x = trees, y = mean_error, color = class
+    ), linewidth = 1.5, linetype = "dashed") +
+    ggplot2::theme_minimal() +
+    ggplot2::labs(
+        title = "OOB error rate (Solid: Original, Dashed: Randomized)",
+        x = "Number of trees", y = "Percent correct"
+    ) +
+    ggplot2::scale_color_manual(values = c(
+        "OOB" = "#4e4e4e",
+        "immigrant" = "#5d8566", "resident" = "#c29007"
+    )) +
+    ggplot2::coord_cartesian(ylim = c(0, 100)) +
+    ggplot2::geom_hline(yintercept = 50, linetype = "dotted", color = "red") +
+    ggplot2::theme(aspect.ratio = 1, text = ggplot2::element_text(size = 12)) +
+    ggplot2::scale_y_continuous(labels = scales::percent_format(scale = 1))
+
+# Save the plot
+ggplot2::ggsave(file.path(config$path$figures, "oob_plot.jpg"),
+    plot =
+        oobplot,
+    width = 8,
+    height = 6
+)
+
+
+# rfPermute::confusionMatrix(output.forest)
+# rfPermute::plotPredictedProbs(output.forest, bins = 20, plot = TRUE)
+# rfPermute::plotProximity(output.forest)
+# summary(output.forest)
+# rfPermute::plotTrace(output.forest)

# time: 2024-08-30 10:56:37 UTC
# mode: r
+# Import configuration file
+config <- config::get()
+
+# parameters
+percent.train <- 0.7
+num.threads <- 44
+num.eigenvectors <- 20
+set.seed(555)
+
+# Input files
+immigrant.info <- file.path(config$path$data, "600K_immigrants.fam")
+immigrant.vcf <- file.path(config$path$data, "600K_immigrants.vcf")
+resident.vcf <- file.path(config$path$data, "600K_residents.vcf")
+
+immigrant.gds <- file.path(config$path$data, "immigrant_recode.gds")
+resident.gds <- file.path(config$path$data, "resident_recode.gds")
+all.gds <- file.path(config$path$data, "all_recode.gds")
+
+
+# Load all the data
+all.maf <- SNPRelate::snpgdsOpen(all.gds)
+
+# Load labels
+immigrant.id <- gdsfmt::read.gdsn(gdsfmt::index.gdsn(
+    SNPRelate::snpgdsOpen(immigrant.gds),
+    "sample.id"
+))
+resident.id <- gdsfmt::read.gdsn(gdsfmt::index.gdsn(
+    SNPRelate::snpgdsOpen(resident.gds),
+    "sample.id"
+))
+
+all.id <- gdsfmt::read.gdsn(gdsfmt::index.gdsn(all.maf, "sample.id"))
+all.label <- ifelse(all.id %in% immigrant.id, "immigrant", "resident")
+
+
+# Function to split data into training and testing sets
+# TODO balence so that there are .5 per class in both train and test sets
+split_data <- function(all.id, all.label, percent.train) {
+    num_train <- round(length(all.id) * percent.train)
+    train.id <- sample(all.id, num_train)
+    test.id <- setdiff(all.id, train.id)
+    train.label <- all.label[all.id %in% train.id]
+    test.label <- all.label[all.id %in% test.id]
+
+    list(
+        train.id = train.id, test.id = test.id,
+        train.label = train.label, test.label = test.label
+    )
+}
+
+# Function to perform PCA and prepare data frames
+prepare_data <- function(all.maf, train.id, test.id, train.label, test.label, num.eigenvectors, num.threads) {
+    print(paste("Number of train.id:", length(train.id)))
+    print(paste("Number of train.label:", length(train.label)))
+
+    pca <- SNPRelate::snpgdsPCA(all.maf, sample.id = train.id, num.thread = num.threads)
+
+    print(paste("Number of pca$sample.id:", length(pca$sample.id)))
+
+    snp_loadings <- SNPRelate::snpgdsPCASNPLoading(pca, all.maf, num.thread = num.threads)
+    sample_loadings <- SNPRelate::snpgdsPCASampLoading(snp_loadings, all.maf, sample.id = test.id, num.thread = num.threads)
+
+    train_eigenvects <- data.frame(sample.id = pca$sample.id, pca$eigenvect[, 1:num.eigenvectors])
+    test_eigenvects <- data.frame(sample.id = sample_loadings$sample.id, sample_loadings$eigenvect[, 1:num.eigenvectors])
+
+    print(paste("Number of rows in train_eigenvects:", nrow(train_eigenvects)))
+
+    train_df <- data.frame(sample.id = train.id, pop = train.label, train_eigenvects[, -1])
+    test_df <- data.frame(sample.id = test.id, pop = test.label, test_eigenvects[, -1])
+
+    list(train_df = train_df, test_df = test_df)
+}
+
+# Function to train a random forest with optional randomized labels
+train_random_forest <- function(
+    train_df, test_df, num.eigenvectors, randomize_labels = FALSE) {
+    if (randomize_labels) {
+        train_df$pop <- sample(train_df$pop)
+    }
+
+    rf <- randomForest::randomForest(
+        train_df[, 3:(2 + num.eigenvectors)],
+        y = as.factor(train_df$pop),
+        data = train_df,
+        xtest = test_df[, 3:(2 + num.eigenvectors)],
+        ytest = as.factor(test_df$pop),
+        ntree = 500,
+        replace = FALSE,
+        proximity = TRUE
+    )
+
+    return(rf)
+}
+
+
+
+# Function to run a single iteration
+run_iteration <- function(
+    all.id, all.label, all.maf, percent.train, num.eigenvectors, num.threads, randomize_labels) {
+    # Split data
+    split <- split_data(all.id, all.label, percent.train)
+
+    # Prepare data
+    data <- prepare_data(
+        all.maf, split$train.id, split$test.id, split$train.label, split$test.label,
+        num.eigenvectors, num.threads
+    )
+
+    # Train random forest
+    rf <- train_random_forest(data$train_df, data$test_df, num.eigenvectors, randomize_labels)
+
+    # Extract OOB error rates
+    oob_data <- rfPermute::plotTrace(rf, plot = FALSE)$data
+
+    return(oob_data)
+}
+
+# Main function to run multiple iterations
+run_analysis <- function(
+    all.id, all.label, all.maf, percent.train,
+    num.eigenvectors, num.threads,
+    n_iterations = 5) {
+    set.seed(123) # For reproducibility
+
+    # Run iterations with original labels
+    original_results <- replicate(n_iterations,
+        run_iteration(all.id, all.label, all.maf, percent.train, num.eigenvectors, num.threads, FALSE),
+        simplify = FALSE
+    )
+
+    # Run iterations with randomized labels
+    random_results <- replicate(n_iterations,
+        run_iteration(all.id, all.label, all.maf, percent.train, num.eigenvectors, num.threads, TRUE),
+        simplify = FALSE
+    )
+
+    # Combine results
+    oob_df <- dplyr::as_tibble(do.call(rbind, original_results))
+    random_oob_df <- dplyr::as_tibble(do.call(rbind, random_results))
+
+    list(oob_df = oob_df, random_oob_df = random_oob_df)
+}
+
+# Run the analysis
+results <- run_analysis(all.id, all.label, all.maf, percent.train, num.eigenvectors, num.threads)
+
+# Calculate mean error for original data
+mean_oob_df <- results$oob_df |>
+    dplyr::group_by(trees, class) |>
+    dplyr::summarise(mean_error = mean(error), .groups = "drop")
+
+# Calculate mean error for randomized data
+mean_random_oob_df <- results$random_oob_df |>
+    dplyr::group_by(trees, class) |>
+    dplyr::summarise(mean_error = mean(error), .groups = "drop")
+
+# Create the plot
+
+oobplot <-
+    ggplot2::ggplot() +
+    ggplot2::geom_line(data = results$oob_df, ggplot2::aes(
+        x = trees, y = error, color = class
+    ), alpha = 0.1) +
+    ggplot2::geom_line(data = results$random_oob_df, ggplot2::aes(
+        x = trees, y = error, color = class
+    ), alpha = 0.1, linetype = "dashed") +
+    ggplot2::geom_line(data = mean_oob_df, ggplot2::aes(
+        x = trees, y = mean_error, color = class
+    ), linewidth = 1.5) +
+    ggplot2::geom_line(data = mean_random_oob_df, ggplot2::aes(
+        x = trees, y = mean_error, color = class
+    ), linewidth = 1.5, linetype = "dashed") +
+    ggplot2::theme_minimal() +
+    ggplot2::labs(
+        title = "OOB error rate (Solid: Original, Dashed: Randomized)",
+        x = "Number of trees", y = "Percent correct"
+    ) +
+    ggplot2::scale_color_manual(values = c(
+        "OOB" = "#4e4e4e",
+        "immigrant" = "#5d8566", "resident" = "#c29007"
+    )) +
+    ggplot2::coord_cartesian(ylim = c(0, 100)) +
+    ggplot2::geom_hline(yintercept = 50, linetype = "dotted", color = "red") +
+    ggplot2::theme(aspect.ratio = 1, text = ggplot2::element_text(size = 12)) +
+    ggplot2::scale_y_continuous(labels = scales::percent_format(scale = 1))
+
+# Save the plot
+ggplot2::ggsave(file.path(config$path$figures, "oob_plot.jpg"),
+    plot =
+        oobplot,
+    width = 8,
+    height = 6
+)
+
+
+# rfPermute::confusionMatrix(output.forest)
+# rfPermute::plotPredictedProbs(output.forest, bins = 20, plot = TRUE)
+# rfPermute::plotProximity(output.forest)
+# summary(output.forest)
+# rfPermute::plotTrace(output.forest)

# time: 2024-08-30 10:57:54 UTC
# mode: r
+# Import configuration file
+config <- config::get()
+
+# parameters
+percent.train <- 0.7
+num.threads <- 44
+num.eigenvectors <- 20
+set.seed(555)
+
+# Input files
+immigrant.info <- file.path(config$path$data, "600K_immigrants.fam")
+immigrant.vcf <- file.path(config$path$data, "600K_immigrants.vcf")
+resident.vcf <- file.path(config$path$data, "600K_residents.vcf")
+
+immigrant.gds <- file.path(config$path$data, "immigrant_recode.gds")
+resident.gds <- file.path(config$path$data, "resident_recode.gds")
+all.gds <- file.path(config$path$data, "all_recode.gds")
+
+
+# Load all the data
+all.maf <- SNPRelate::snpgdsOpen(all.gds, allow.duplicate = TRUE)
+
+# Load labels
+immigrant.id <- gdsfmt::read.gdsn(gdsfmt::index.gdsn(
+    SNPRelate::snpgdsOpen(immigrant.gds),
+    "sample.id"
+))
+resident.id <- gdsfmt::read.gdsn(gdsfmt::index.gdsn(
+    SNPRelate::snpgdsOpen(resident.gds),
+    "sample.id"
+))
+
+all.id <- gdsfmt::read.gdsn(gdsfmt::index.gdsn(all.maf, "sample.id"))
+all.label <- ifelse(all.id %in% immigrant.id, "immigrant", "resident")
+
+
+# Function to split data into training and testing sets
+# TODO balence so that there are .5 per class in both train and test sets
+split_data <- function(all.id, all.label, percent.train) {
+    num_train <- round(length(all.id) * percent.train)
+    train.id <- sample(all.id, num_train)
+    test.id <- setdiff(all.id, train.id)
+    train.label <- all.label[all.id %in% train.id]
+    test.label <- all.label[all.id %in% test.id]
+
+    list(
+        train.id = train.id, test.id = test.id,
+        train.label = train.label, test.label = test.label
+    )
+}
+
+# Function to perform PCA and prepare data frames
+prepare_data <- function(all.maf, train.id, test.id, train.label, test.label, num.eigenvectors, num.threads) {
+    print(paste("Number of train.id:", length(train.id)))
+    print(paste("Number of train.label:", length(train.label)))
+
+    pca <- SNPRelate::snpgdsPCA(all.maf, sample.id = train.id, num.thread = num.threads)
+
+    print(paste("Number of pca$sample.id:", length(pca$sample.id)))
+
+    snp_loadings <- SNPRelate::snpgdsPCASNPLoading(pca, all.maf, num.thread = num.threads)
+    sample_loadings <- SNPRelate::snpgdsPCASampLoading(snp_loadings, all.maf, sample.id = test.id, num.thread = num.threads)
+
+    train_eigenvects <- data.frame(sample.id = pca$sample.id, pca$eigenvect[, 1:num.eigenvectors])
+    test_eigenvects <- data.frame(sample.id = sample_loadings$sample.id, sample_loadings$eigenvect[, 1:num.eigenvectors])
+
+    print(paste("Number of rows in train_eigenvects:", nrow(train_eigenvects)))
+
+    train_df <- data.frame(sample.id = train.id, pop = train.label, train_eigenvects[, -1])
+    test_df <- data.frame(sample.id = test.id, pop = test.label, test_eigenvects[, -1])
+
+    list(train_df = train_df, test_df = test_df)
+}
+
+# Function to train a random forest with optional randomized labels
+train_random_forest <- function(
+    train_df, test_df, num.eigenvectors, randomize_labels = FALSE) {
+    if (randomize_labels) {
+        train_df$pop <- sample(train_df$pop)
+    }
+
+    rf <- randomForest::randomForest(
+        train_df[, 3:(2 + num.eigenvectors)],
+        y = as.factor(train_df$pop),
+        data = train_df,
+        xtest = test_df[, 3:(2 + num.eigenvectors)],
+        ytest = as.factor(test_df$pop),
+        ntree = 500,
+        replace = FALSE,
+        proximity = TRUE
+    )
+
+    return(rf)
+}
+
+
+
+# Function to run a single iteration
+run_iteration <- function(
+    all.id, all.label, all.maf, percent.train, num.eigenvectors, num.threads, randomize_labels) {
+    # Split data
+    split <- split_data(all.id, all.label, percent.train)
+
+    # Prepare data
+    data <- prepare_data(
+        all.maf, split$train.id, split$test.id, split$train.label, split$test.label,
+        num.eigenvectors, num.threads
+    )
+
+    # Train random forest
+    rf <- train_random_forest(data$train_df, data$test_df, num.eigenvectors, randomize_labels)
+
+    # Extract OOB error rates
+    oob_data <- rfPermute::plotTrace(rf, plot = FALSE)$data
+
+    return(oob_data)
+}
+
+# Main function to run multiple iterations
+run_analysis <- function(
+    all.id, all.label, all.maf, percent.train,
+    num.eigenvectors, num.threads,
+    n_iterations = 5) {
+    set.seed(123) # For reproducibility
+
+    # Run iterations with original labels
+    original_results <- replicate(n_iterations,
+        run_iteration(all.id, all.label, all.maf, percent.train, num.eigenvectors, num.threads, FALSE),
+        simplify = FALSE
+    )
+
+    # Run iterations with randomized labels
+    random_results <- replicate(n_iterations,
+        run_iteration(all.id, all.label, all.maf, percent.train, num.eigenvectors, num.threads, TRUE),
+        simplify = FALSE
+    )
+
+    # Combine results
+    oob_df <- dplyr::as_tibble(do.call(rbind, original_results))
+    random_oob_df <- dplyr::as_tibble(do.call(rbind, random_results))
+
+    list(oob_df = oob_df, random_oob_df = random_oob_df)
+}
+
+# Run the analysis
+results <- run_analysis(all.id, all.label, all.maf, percent.train, num.eigenvectors, num.threads)
+
+# Calculate mean error for original data
+mean_oob_df <- results$oob_df |>
+    dplyr::group_by(trees, class) |>
+    dplyr::summarise(mean_error = mean(error), .groups = "drop")
+
+# Calculate mean error for randomized data
+mean_random_oob_df <- results$random_oob_df |>
+    dplyr::group_by(trees, class) |>
+    dplyr::summarise(mean_error = mean(error), .groups = "drop")
+
+# Create the plot
+
+oobplot <-
+    ggplot2::ggplot() +
+    ggplot2::geom_line(data = results$oob_df, ggplot2::aes(
+        x = trees, y = error, color = class
+    ), alpha = 0.1) +
+    ggplot2::geom_line(data = results$random_oob_df, ggplot2::aes(
+        x = trees, y = error, color = class
+    ), alpha = 0.1, linetype = "dashed") +
+    ggplot2::geom_line(data = mean_oob_df, ggplot2::aes(
+        x = trees, y = mean_error, color = class
+    ), linewidth = 1.5) +
+    ggplot2::geom_line(data = mean_random_oob_df, ggplot2::aes(
+        x = trees, y = mean_error, color = class
+    ), linewidth = 1.5, linetype = "dashed") +
+    ggplot2::theme_minimal() +
+    ggplot2::labs(
+        title = "OOB error rate (Solid: Original, Dashed: Randomized)",
+        x = "Number of trees", y = "Percent correct"
+    ) +
+    ggplot2::scale_color_manual(values = c(
+        "OOB" = "#4e4e4e",
+        "immigrant" = "#5d8566", "resident" = "#c29007"
+    )) +
+    ggplot2::coord_cartesian(ylim = c(0, 100)) +
+    ggplot2::geom_hline(yintercept = 50, linetype = "dotted", color = "red") +
+    ggplot2::theme(aspect.ratio = 1, text = ggplot2::element_text(size = 12)) +
+    ggplot2::scale_y_continuous(labels = scales::percent_format(scale = 1))
+
+# Save the plot
+ggplot2::ggsave(file.path(config$path$figures, "oob_plot.jpg"),
+    plot =
+        oobplot,
+    width = 8,
+    height = 6
+)
+
+
+# rfPermute::confusionMatrix(output.forest)
+# rfPermute::plotPredictedProbs(output.forest, bins = 20, plot = TRUE)
+# rfPermute::plotProximity(output.forest)
+# summary(output.forest)
+# rfPermute::plotTrace(output.forest)

# time: 2024-08-30 11:01:13 UTC
# mode: r
+# Calculate mean error for original data
+mean_oob_df <- results$oob_df |>
+    dplyr::group_by(trees, class) |>
+    dplyr::summarise(mean_error = mean(error), .groups = "drop")
+
+# Calculate mean error for randomized data
+mean_random_oob_df <- results$random_oob_df |>
+    dplyr::group_by(trees, class) |>
+    dplyr::summarise(mean_error = mean(error), .groups = "drop")
+
+# Create the plot
+
+oobplot <-
+    ggplot2::ggplot() +
+    ggplot2::geom_line(data = results$oob_df, ggplot2::aes(
+        x = trees, y = error, color = class
+    ), alpha = 0.1) +
+    ggplot2::geom_line(data = results$random_oob_df, ggplot2::aes(
+        x = trees, y = error, color = class
+    ), alpha = 0.1, linetype = "dashed") +
+    ggplot2::geom_line(data = mean_oob_df, ggplot2::aes(
+        x = trees, y = mean_error, color = class
+    ), linewidth = 1.5) +
+    ggplot2::geom_line(data = mean_random_oob_df, ggplot2::aes(
+        x = trees, y = mean_error, color = class
+    ), linewidth = 1.5, linetype = "dashed") +
+    ggplot2::theme_minimal() +
+    ggplot2::labs(
+        title = "OOB error rate (Solid: Original, Dashed: Randomized)",
+        x = "Number of trees", y = "Percent correct"
+    ) +
+    ggplot2::scale_color_manual(values = c(
+        "OOB" = "#4e4e4e",
+        "immigrant" = "#5d8566", "resident" = "#c29007"
+    )) +
+    ggplot2::coord_cartesian(ylim = c(0, 100)) +
+    ggplot2::geom_hline(yintercept = 50, linetype = "dotted", color = "red") +
+    ggplot2::theme(aspect.ratio = 1, text = ggplot2::element_text(size = 12)) +
+    ggplot2::scale_y_continuous(labels = scales::percent_format(scale = 1))
+
+# Save the plot
+ggplot2::ggsave(file.path(config$path$figures, "oob_plot.jpg"),
+    plot =
+        oobplot,
+    width = 8,
+    height = 6
+)

# time: 2024-08-30 11:05:33 UTC
# mode: r
+# Import configuration file
+config <- config::get()
+
+# parameters
+percent.train <- 0.7
+num.threads <- 44
+num.eigenvectors <- 20
+set.seed(555)
+
+# Input files
+immigrant.info <- file.path(config$path$data, "600K_immigrants.fam")
+immigrant.vcf <- file.path(config$path$data, "600K_immigrants.vcf")
+resident.vcf <- file.path(config$path$data, "600K_residents.vcf")
+
+immigrant.gds <- file.path(config$path$data, "immigrant_recode.gds")
+resident.gds <- file.path(config$path$data, "resident_recode.gds")
+all.gds <- file.path(config$path$data, "all_recode.gds")
+
+
+# Load all the data
+all.maf <- SNPRelate::snpgdsOpen(all.gds, allow.duplicate = TRUE)
+
+# Load labels
+immigrant.id <- gdsfmt::read.gdsn(gdsfmt::index.gdsn(
+    SNPRelate::snpgdsOpen(immigrant.gds),
+    "sample.id"
+))
+resident.id <- gdsfmt::read.gdsn(gdsfmt::index.gdsn(
+    SNPRelate::snpgdsOpen(resident.gds),
+    "sample.id"
+))
+
+all.id <- gdsfmt::read.gdsn(gdsfmt::index.gdsn(all.maf, "sample.id"))
+all.label <- ifelse(all.id %in% immigrant.id, "immigrant", "resident")
+
+
+# Function to split data into training and testing sets
+# TODO balence so that there are .5 per class in both train and test sets
+split_data <- function(all.id, all.label, percent.train) {
+    # Split data by class
+    class_0 <- all.id[all.label == 0]
+    class_1 <- all.id[all.label == 1]
+
+    # Determine the size of the smaller class
+    min_class_size <- min(length(class_0), length(class_1))
+
+    # Undersample the majority class
+    if (length(class_0) > min_class_size) {
+        class_0 <- sample(class_0, min_class_size)
+    } else {
+        class_1 <- sample(class_1, min_class_size)
+    }
+
+    # Combine the balanced classes
+    balanced_ids <- c(class_0, class_1)
+    balanced_labels <- c(rep(0, length(class_0)), rep(1, length(class_1)))
+
+    # Calculate the number of samples for training
+    num_train <- round(length(balanced_ids) * percent.train)
+
+    # Ensure equal representation of both classes in train and test sets
+    num_train_per_class <- num_train %/% 2
+
+    # Randomly select samples for training from each class
+    train_indices <- c(
+        sample(1:min_class_size, num_train_per_class),
+        sample((min_class_size + 1):(2 * min_class_size), num_train_per_class)
+    )
+
+    train.id <- balanced_ids[train_indices]
+    test.id <- balanced_ids[-train_indices]
+    train.label <- balanced_labels[train_indices]
+    test.label <- balanced_labels[-train_indices]
+
+    list(
+        train.id = train.id, test.id = test.id,
+        train.label = train.label, test.label = test.label
+    )
+}
+# Function to perform PCA and prepare data frames
+prepare_data <- function(all.maf, train.id, test.id, train.label, test.label, num.eigenvectors, num.threads) {
+    print(paste("Number of train.id:", length(train.id)))
+    print(paste("Number of train.label:", length(train.label)))
+
+    pca <- SNPRelate::snpgdsPCA(all.maf, sample.id = train.id, num.thread = num.threads)
+
+    print(paste("Number of pca$sample.id:", length(pca$sample.id)))
+
+    snp_loadings <- SNPRelate::snpgdsPCASNPLoading(pca, all.maf, num.thread = num.threads)
+    sample_loadings <- SNPRelate::snpgdsPCASampLoading(snp_loadings, all.maf, sample.id = test.id, num.thread = num.threads)
+
+    train_eigenvects <- data.frame(sample.id = pca$sample.id, pca$eigenvect[, 1:num.eigenvectors])
+    test_eigenvects <- data.frame(sample.id = sample_loadings$sample.id, sample_loadings$eigenvect[, 1:num.eigenvectors])
+
+    print(paste("Number of rows in train_eigenvects:", nrow(train_eigenvects)))
+
+    train_df <- data.frame(sample.id = train.id, pop = train.label, train_eigenvects[, -1])
+    test_df <- data.frame(sample.id = test.id, pop = test.label, test_eigenvects[, -1])
+
+    list(train_df = train_df, test_df = test_df)
+}
+
+# Function to train a random forest with optional randomized labels
+train_random_forest <- function(
+    train_df, test_df, num.eigenvectors, randomize_labels = FALSE) {
+    if (randomize_labels) {
+        train_df$pop <- sample(train_df$pop)
+    }
+
+    rf <- randomForest::randomForest(
+        train_df[, 3:(2 + num.eigenvectors)],
+        y = as.factor(train_df$pop),
+        data = train_df,
+        xtest = test_df[, 3:(2 + num.eigenvectors)],
+        ytest = as.factor(test_df$pop),
+        ntree = 500,
+        replace = FALSE,
+        proximity = TRUE
+    )
+
+    return(rf)
+}
+
+
+
+# Function to run a single iteration
+run_iteration <- function(
+    all.id, all.label, all.maf, percent.train, num.eigenvectors, num.threads, randomize_labels) {
+    # Split data
+    split <- split_data(all.id, all.label, percent.train)
+
+    # Prepare data
+    data <- prepare_data(
+        all.maf, split$train.id, split$test.id, split$train.label, split$test.label,
+        num.eigenvectors, num.threads
+    )
+
+    # Train random forest
+    rf <- train_random_forest(data$train_df, data$test_df, num.eigenvectors, randomize_labels)
+
+    # Extract OOB error rates
+    oob_data <- rfPermute::plotTrace(rf, plot = FALSE)$data
+
+    return(oob_data)
+}
+
+# Main function to run multiple iterations
+run_analysis <- function(
+    all.id, all.label, all.maf, percent.train,
+    num.eigenvectors, num.threads,
+    n_iterations = 1) {
+    set.seed(123) # For reproducibility
+
+    # Run iterations with original labels
+    original_results <- replicate(n_iterations,
+        run_iteration(all.id, all.label, all.maf, percent.train, num.eigenvectors, num.threads, FALSE),
+        simplify = FALSE
+    )
+
+    # Run iterations with randomized labels
+    random_results <- replicate(n_iterations,
+        run_iteration(all.id, all.label, all.maf, percent.train, num.eigenvectors, num.threads, TRUE),
+        simplify = FALSE
+    )
+
+    # Combine results
+    oob_df <- dplyr::as_tibble(do.call(rbind, original_results))
+    random_oob_df <- dplyr::as_tibble(do.call(rbind, random_results))
+
+    list(oob_df = oob_df, random_oob_df = random_oob_df)
+}
+
+# Run the analysis
+results <- run_analysis(all.id, all.label, all.maf, percent.train, num.eigenvectors, num.threads)
+
+# Calculate mean error for original data
+mean_oob_df <- results$oob_df |>
+    dplyr::group_by(trees, class) |>
+    dplyr::summarise(mean_error = mean(error), .groups = "drop")
+
+# Calculate mean error for randomized data
+mean_random_oob_df <- results$random_oob_df |>
+    dplyr::group_by(trees, class) |>
+    dplyr::summarise(mean_error = mean(error), .groups = "drop")
+
+# Create the plot
+
+oobplot <-
+    ggplot2::ggplot() +
+    ggplot2::geom_line(data = results$oob_df, ggplot2::aes(
+        x = trees, y = error, color = class
+    ), alpha = 0.1) +
+    ggplot2::geom_line(data = results$random_oob_df, ggplot2::aes(
+        x = trees, y = error, color = class
+    ), alpha = 0.1, linetype = "dashed") +
+    ggplot2::geom_line(data = mean_oob_df, ggplot2::aes(
+        x = trees, y = mean_error, color = class
+    ), linewidth = 1.5) +
+    ggplot2::geom_line(data = mean_random_oob_df, ggplot2::aes(
+        x = trees, y = mean_error, color = class
+    ), linewidth = 1.5, linetype = "dashed") +
+    ggplot2::theme_minimal() +
+    ggplot2::labs(
+        title = "OOB error rate (Solid: Original, Dashed: Randomized)",
+        x = "Number of trees", y = "Percent correct"
+    ) +
+    ggplot2::scale_color_manual(values = c(
+        "OOB" = "#4e4e4e",
+        "immigrant" = "#5d8566", "resident" = "#c29007"
+    )) +
+    ggplot2::coord_cartesian(ylim = c(0, 100)) +
+    ggplot2::geom_hline(yintercept = 50, linetype = "dotted", color = "red") +
+    ggplot2::theme(aspect.ratio = 1, text = ggplot2::element_text(size = 12)) +
+    ggplot2::scale_y_continuous(labels = scales::percent_format(scale = 1))
+
+# Save the plot
+ggplot2::ggsave(file.path(config$path$figures, "oob_plot.jpg"),
+    plot =
+        oobplot,
+    width = 8,
+    height = 6
+)
+
+
+# rfPermute::confusionMatrix(output.forest)
+# rfPermute::plotPredictedProbs(output.forest, bins = 20, plot = TRUE)
+# rfPermute::plotProximity(output.forest)
+# summary(output.forest)
+# rfPermute::plotTrace(output.forest)

# time: 2024-08-30 11:05:54 UTC
# mode: r
+factor(all.label)

# time: 2024-08-30 11:06:03 UTC
# mode: r
+labels <- unique(all.label)

# time: 2024-08-30 11:06:06 UTC
# mode: r
+labels

# time: 2024-08-30 11:06:17 UTC
# mode: r
+# Import configuration file
+config <- config::get()
+
+# parameters
+percent.train <- 0.7
+num.threads <- 44
+num.eigenvectors <- 20
+set.seed(555)
+
+# Input files
+immigrant.info <- file.path(config$path$data, "600K_immigrants.fam")
+immigrant.vcf <- file.path(config$path$data, "600K_immigrants.vcf")
+resident.vcf <- file.path(config$path$data, "600K_residents.vcf")
+
+immigrant.gds <- file.path(config$path$data, "immigrant_recode.gds")
+resident.gds <- file.path(config$path$data, "resident_recode.gds")
+all.gds <- file.path(config$path$data, "all_recode.gds")
+
+
+# Load all the data
+all.maf <- SNPRelate::snpgdsOpen(all.gds, allow.duplicate = TRUE)
+
+# Load labels
+immigrant.id <- gdsfmt::read.gdsn(gdsfmt::index.gdsn(
+    SNPRelate::snpgdsOpen(immigrant.gds),
+    "sample.id"
+))
+resident.id <- gdsfmt::read.gdsn(gdsfmt::index.gdsn(
+    SNPRelate::snpgdsOpen(resident.gds),
+    "sample.id"
+))
+
+all.id <- gdsfmt::read.gdsn(gdsfmt::index.gdsn(all.maf, "sample.id"))
+all.label <- ifelse(all.id %in% immigrant.id, "immigrant", "resident")
+
+
+# Function to split data into training and testing sets
+# TODO balence so that there are .5 per class in both train and test sets
+split_data <- function(all.id, all.label, percent.train) {
+    # Split data by class
+    labels <- unique(all.label)
+    class_0 <- all.id[all.label == labels[1]]
+    class_1 <- all.id[all.label == labels[2]]
+
+    # Determine the size of the smaller class
+    min_class_size <- min(length(class_0), length(class_1))
+
+    # Undersample the majority class
+    if (length(class_0) > min_class_size) {
+        class_0 <- sample(class_0, min_class_size)
+    } else {
+        class_1 <- sample(class_1, min_class_size)
+    }
+
+    # Combine the balanced classes
+    balanced_ids <- c(class_0, class_1)
+    balanced_labels <- c(rep(0, length(class_0)), rep(1, length(class_1)))
+
+    # Calculate the number of samples for training
+    num_train <- round(length(balanced_ids) * percent.train)
+
+    # Ensure equal representation of both classes in train and test sets
+    num_train_per_class <- num_train %/% 2
+
+    # Randomly select samples for training from each class
+    train_indices <- c(
+        sample(1:min_class_size, num_train_per_class),
+        sample((min_class_size + 1):(2 * min_class_size), num_train_per_class)
+    )
+
+    train.id <- balanced_ids[train_indices]
+    test.id <- balanced_ids[-train_indices]
+    train.label <- balanced_labels[train_indices]
+    test.label <- balanced_labels[-train_indices]
+
+    list(
+        train.id = train.id, test.id = test.id,
+        train.label = train.label, test.label = test.label
+    )
+}
+# Function to perform PCA and prepare data frames
+prepare_data <- function(all.maf, train.id, test.id, train.label, test.label, num.eigenvectors, num.threads) {
+    print(paste("Number of train.id:", length(train.id)))
+    print(paste("Number of train.label:", length(train.label)))
+
+    pca <- SNPRelate::snpgdsPCA(all.maf, sample.id = train.id, num.thread = num.threads)
+
+    print(paste("Number of pca$sample.id:", length(pca$sample.id)))
+
+    snp_loadings <- SNPRelate::snpgdsPCASNPLoading(pca, all.maf, num.thread = num.threads)
+    sample_loadings <- SNPRelate::snpgdsPCASampLoading(snp_loadings, all.maf, sample.id = test.id, num.thread = num.threads)
+
+    train_eigenvects <- data.frame(sample.id = pca$sample.id, pca$eigenvect[, 1:num.eigenvectors])
+    test_eigenvects <- data.frame(sample.id = sample_loadings$sample.id, sample_loadings$eigenvect[, 1:num.eigenvectors])
+
+    print(paste("Number of rows in train_eigenvects:", nrow(train_eigenvects)))
+
+    train_df <- data.frame(sample.id = train.id, pop = train.label, train_eigenvects[, -1])
+    test_df <- data.frame(sample.id = test.id, pop = test.label, test_eigenvects[, -1])
+
+    list(train_df = train_df, test_df = test_df)
+}
+
+# Function to train a random forest with optional randomized labels
+train_random_forest <- function(
+    train_df, test_df, num.eigenvectors, randomize_labels = FALSE) {
+    if (randomize_labels) {
+        train_df$pop <- sample(train_df$pop)
+    }
+
+    rf <- randomForest::randomForest(
+        train_df[, 3:(2 + num.eigenvectors)],
+        y = as.factor(train_df$pop),
+        data = train_df,
+        xtest = test_df[, 3:(2 + num.eigenvectors)],
+        ytest = as.factor(test_df$pop),
+        ntree = 500,
+        replace = FALSE,
+        proximity = TRUE
+    )
+
+    return(rf)
+}
+
+
+
+# Function to run a single iteration
+run_iteration <- function(
+    all.id, all.label, all.maf, percent.train, num.eigenvectors, num.threads, randomize_labels) {
+    # Split data
+    split <- split_data(all.id, all.label, percent.train)
+
+    # Prepare data
+    data <- prepare_data(
+        all.maf, split$train.id, split$test.id, split$train.label, split$test.label,
+        num.eigenvectors, num.threads
+    )
+
+    # Train random forest
+    rf <- train_random_forest(data$train_df, data$test_df, num.eigenvectors, randomize_labels)
+
+    # Extract OOB error rates
+    oob_data <- rfPermute::plotTrace(rf, plot = FALSE)$data
+
+    return(oob_data)
+}
+
+# Main function to run multiple iterations
+run_analysis <- function(
+    all.id, all.label, all.maf, percent.train,
+    num.eigenvectors, num.threads,
+    n_iterations = 1) {
+    set.seed(123) # For reproducibility
+
+    # Run iterations with original labels
+    original_results <- replicate(n_iterations,
+        run_iteration(all.id, all.label, all.maf, percent.train, num.eigenvectors, num.threads, FALSE),
+        simplify = FALSE
+    )
+
+    # Run iterations with randomized labels
+    random_results <- replicate(n_iterations,
+        run_iteration(all.id, all.label, all.maf, percent.train, num.eigenvectors, num.threads, TRUE),
+        simplify = FALSE
+    )
+
+    # Combine results
+    oob_df <- dplyr::as_tibble(do.call(rbind, original_results))
+    random_oob_df <- dplyr::as_tibble(do.call(rbind, random_results))
+
+    list(oob_df = oob_df, random_oob_df = random_oob_df)
+}
+
+# Run the analysis
+results <- run_analysis(all.id, all.label, all.maf, percent.train, num.eigenvectors, num.threads)
+
+# Calculate mean error for original data
+mean_oob_df <- results$oob_df |>
+    dplyr::group_by(trees, class) |>
+    dplyr::summarise(mean_error = mean(error), .groups = "drop")
+
+# Calculate mean error for randomized data
+mean_random_oob_df <- results$random_oob_df |>
+    dplyr::group_by(trees, class) |>
+    dplyr::summarise(mean_error = mean(error), .groups = "drop")
+
+# Create the plot
+
+oobplot <-
+    ggplot2::ggplot() +
+    ggplot2::geom_line(data = results$oob_df, ggplot2::aes(
+        x = trees, y = error, color = class
+    ), alpha = 0.1) +
+    ggplot2::geom_line(data = results$random_oob_df, ggplot2::aes(
+        x = trees, y = error, color = class
+    ), alpha = 0.1, linetype = "dashed") +
+    ggplot2::geom_line(data = mean_oob_df, ggplot2::aes(
+        x = trees, y = mean_error, color = class
+    ), linewidth = 1.5) +
+    ggplot2::geom_line(data = mean_random_oob_df, ggplot2::aes(
+        x = trees, y = mean_error, color = class
+    ), linewidth = 1.5, linetype = "dashed") +
+    ggplot2::theme_minimal() +
+    ggplot2::labs(
+        title = "OOB error rate (Solid: Original, Dashed: Randomized)",
+        x = "Number of trees", y = "Percent correct"
+    ) +
+    ggplot2::scale_color_manual(values = c(
+        "OOB" = "#4e4e4e",
+        "immigrant" = "#5d8566", "resident" = "#c29007"
+    )) +
+    ggplot2::coord_cartesian(ylim = c(0, 100)) +
+    ggplot2::geom_hline(yintercept = 50, linetype = "dotted", color = "red") +
+    ggplot2::theme(aspect.ratio = 1, text = ggplot2::element_text(size = 12)) +
+    ggplot2::scale_y_continuous(labels = scales::percent_format(scale = 1))
+
+# Save the plot
+ggplot2::ggsave(file.path(config$path$figures, "oob_plot.jpg"),
+    plot =
+        oobplot,
+    width = 8,
+    height = 6
+)
+
+
+# rfPermute::confusionMatrix(output.forest)
+# rfPermute::plotPredictedProbs(output.forest, bins = 20, plot = TRUE)
+# rfPermute::plotProximity(output.forest)
+# summary(output.forest)
+# rfPermute::plotTrace(output.forest)

# time: 2024-08-30 11:08:38 UTC
# mode: r
+mean_random_oob_df

# time: 2024-08-30 11:14:21 UTC
# mode: r
+# Import configuration file
+config <- config::get()
+
+# parameters
+percent.train <- 0.7
+num.threads <- 44
+num.eigenvectors <- 20
+set.seed(555)
+
+# Input files
+immigrant.info <- file.path(config$path$data, "600K_immigrants.fam")
+immigrant.vcf <- file.path(config$path$data, "600K_immigrants.vcf")
+resident.vcf <- file.path(config$path$data, "600K_residents.vcf")
+
+immigrant.gds <- file.path(config$path$data, "immigrant_recode.gds")
+resident.gds <- file.path(config$path$data, "resident_recode.gds")
+all.gds <- file.path(config$path$data, "all_recode.gds")
+
+
+# Load all the data
+all.maf <- SNPRelate::snpgdsOpen(all.gds, allow.duplicate = TRUE)
+
+# Load labels
+immigrant.id <- gdsfmt::read.gdsn(gdsfmt::index.gdsn(
+    SNPRelate::snpgdsOpen(immigrant.gds),
+    "sample.id"
+))
+resident.id <- gdsfmt::read.gdsn(gdsfmt::index.gdsn(
+    SNPRelate::snpgdsOpen(resident.gds),
+    "sample.id"
+))
+
+all.id <- gdsfmt::read.gdsn(gdsfmt::index.gdsn(all.maf, "sample.id"))
+all.label <- ifelse(all.id %in% immigrant.id, "immigrant", "resident")
+
+
+# Function to split data into training and testing sets
+# TODO balence so that there are .5 per class in both train and test sets
+split_data <- function(all.id, all.label, percent.train) {
+    # Split data by class
+    labels <- unique(all.label)
+    class_0 <- all.id[all.label == labels[1]]
+    class_1 <- all.id[all.label == labels[2]]
+
+    # Determine the size of the smaller class
+    min_class_size <- min(length(class_0), length(class_1))
+
+    # Undersample the majority class
+    if (length(class_0) > min_class_size) {
+        class_0 <- sample(class_0, min_class_size)
+    } else {
+        class_1 <- sample(class_1, min_class_size)
+    }
+
+    # Combine the balanced classes
+    balanced_ids <- c(class_0, class_1)
+    balanced_labels <- c(rep(labels[1], length(class_0)), rep(labels[2], length(class_1)))
+
+    # Calculate the number of samples for training
+    num_train <- round(length(balanced_ids) * percent.train)
+
+    # Ensure equal representation of both classes in train and test sets
+    num_train_per_class <- num_train %/% 2
+
+    # Randomly select samples for training from each class
+    train_indices <- c(
+        sample(1:min_class_size, num_train_per_class),
+        sample((min_class_size + 1):(2 * min_class_size), num_train_per_class)
+    )
+
+    train.id <- balanced_ids[train_indices]
+    test.id <- balanced_ids[-train_indices]
+    train.label <- balanced_labels[train_indices]
+    test.label <- balanced_labels[-train_indices]
+
+    list(
+        train.id = train.id, test.id = test.id,
+        train.label = train.label, test.label = test.label
+    )
+}
+# Function to perform PCA and prepare data frames
+prepare_data <- function(all.maf, train.id, test.id, train.label, test.label, num.eigenvectors, num.threads) {
+    pca <- SNPRelate::snpgdsPCA(all.maf, sample.id = train.id, num.thread = num.threads)
+    snp_loadings <- SNPRelate::snpgdsPCASNPLoading(pca, all.maf, num.thread = num.threads)
+    sample_loadings <- SNPRelate::snpgdsPCASampLoading(snp_loadings, all.maf, sample.id = test.id, num.thread = num.threads)
+    train_eigenvects <- data.frame(sample.id = pca$sample.id, pca$eigenvect[, 1:num.eigenvectors])
+    test_eigenvects <- data.frame(sample.id = sample_loadings$sample.id, sample_loadings$eigenvect[, 1:num.eigenvectors])
+    train_df <- data.frame(sample.id = train.id, pop = train.label, train_eigenvects[, -1])
+    test_df <- data.frame(sample.id = test.id, pop = test.label, test_eigenvects[, -1])
+    list(train_df = train_df, test_df = test_df)
+}
+
+# Function to train a random forest with optional randomized labels
+train_random_forest <- function(
+    train_df, test_df, num.eigenvectors, randomize_labels = FALSE) {
+    if (randomize_labels) {
+        train_df$pop <- sample(train_df$pop)
+    }
+
+    rf <- randomForest::randomForest(
+        train_df[, 3:(2 + num.eigenvectors)],
+        y = as.factor(train_df$pop),
+        data = train_df,
+        xtest = test_df[, 3:(2 + num.eigenvectors)],
+        ytest = as.factor(test_df$pop),
+        ntree = 500,
+        replace = FALSE,
+        proximity = TRUE
+    )
+
+    return(rf)
+}
+
+
+
+# Function to run a single iteration
+# Function to run a single iteration
+run_iteration <- function(
+    all.id, all.label, all.maf, percent.train, num.eigenvectors, num.threads, randomize_labels) {
+    # Split data
+    split <- split_data(all.id, all.label, percent.train)
+
+    # Prepare data
+    data <- prepare_data(
+        all.maf, split$train.id, split$test.id, split$train.label, split$test.label,
+        num.eigenvectors, num.threads
+    )
+
+    # Train random forest
+    rf <- train_random_forest(data$train_df, data$test_df, num.eigenvectors, randomize_labels)
+
+    # Extract OOB error rates
+    oob_data <- rfPermute::plotTrace(rf, plot = FALSE)$data
+
+    # Return both OOB data and the random forest model
+    return(list(oob_data = oob_data, rf_model = rf))
+}
+
+# Main function to run multiple iterations
+run_analysis <- function(
+    all.id, all.label, all.maf, percent.train,
+    num.eigenvectors, num.threads,
+    n_iterations = 1) {
+    set.seed(123) # For reproducibility
+
+    # Run iterations with original labels
+    original_results <- replicate(n_iterations,
+        run_iteration(all.id, all.label, all.maf, percent.train, num.eigenvectors, num.threads, FALSE),
+        simplify = FALSE
+    )
+
+    # Run iterations with randomized labels
+    random_results <- replicate(n_iterations,
+        run_iteration(all.id, all.label, all.maf, percent.train, num.eigenvectors, num.threads, TRUE),
+        simplify = FALSE
+    )
+
+    # Separate OOB data and RF models
+    original_oob <- lapply(original_results, function(x) x$oob_data)
+    original_rf_models <- lapply(original_results, function(x) x$rf_model)
+
+    random_oob <- lapply(random_results, function(x) x$oob_data)
+    random_rf_models <- lapply(random_results, function(x) x$rf_model)
+
+    # Combine OOB results
+    oob_df <- dplyr::as_tibble(do.call(rbind, original_oob))
+    random_oob_df <- dplyr::as_tibble(do.call(rbind, random_oob))
+
+    list(
+        oob_df = oob_df,
+        random_oob_df = random_oob_df,
+        original_rf_models = original_rf_models,
+        random_rf_models = random_rf_models
+    )
+}
+
+# Run the analysis
+results <- run_analysis(all.id, all.label, all.maf, percent.train, num.eigenvectors, num.threads)
+
+# Calculate mean error for original data
+mean_oob_df <- results$oob_df |>
+    dplyr::group_by(trees, class) |>
+    dplyr::summarise(mean_error = mean(error), .groups = "drop")
+
+# Calculate mean error for randomized data
+mean_random_oob_df <- results$random_oob_df |>
+    dplyr::group_by(trees, class) |>
+    dplyr::summarise(mean_error = mean(error), .groups = "drop")
+
+# Create the plot
+
+oobplot <-
+    ggplot2::ggplot() +
+    ggplot2::geom_line(data = results$oob_df, ggplot2::aes(
+        x = trees, y = error, color = class
+    ), alpha = 0.1) +
+    ggplot2::geom_line(data = results$random_oob_df, ggplot2::aes(
+        x = trees, y = error, color = class
+    ), alpha = 0.1, linetype = "dashed") +
+    ggplot2::geom_line(data = mean_oob_df, ggplot2::aes(
+        x = trees, y = mean_error, color = class
+    ), linewidth = 1.5) +
+    ggplot2::geom_line(data = mean_random_oob_df, ggplot2::aes(
+        x = trees, y = mean_error, color = class
+    ), linewidth = 1.5, linetype = "dashed") +
+    ggplot2::theme_minimal() +
+    ggplot2::labs(
+        title = "OOB error rate (Solid: Original, Dashed: Randomized)",
+        x = "Number of trees", y = "Percent correct"
+    ) +
+    ggplot2::scale_color_manual(values = c(
+        "OOB" = "#4e4e4e",
+        "immigrant" = "#5d8566", "resident" = "#c29007"
+    )) +
+    ggplot2::coord_cartesian(ylim = c(0, 100)) +
+    ggplot2::geom_hline(yintercept = 50, linetype = "dotted", color = "red") +
+    ggplot2::theme(aspect.ratio = 1, text = ggplot2::element_text(size = 12)) +
+    ggplot2::scale_y_continuous(labels = scales::percent_format(scale = 1))
+
+# Save the plot
+ggplot2::ggsave(file.path(config$path$figures, "oob_plot.jpg"),
+    plot =
+        oobplot,
+    width = 8,
+    height = 6
+)
+
+
+# rfPermute::confusionMatrix(output.forest)
+# rfPermute::plotPredictedProbs(output.forest, bins = 20, plot = TRUE)
+# rfPermute::plotProximity(output.forest)
+# summary(output.forest)
+# rfPermute::plotTrace(output.forest)

# time: 2024-08-30 11:16:23 UTC
# mode: r
+# Example of how you might calculate summary statistics across replicates
+calculate_summary_stats <- function(rf_models) {
+    # Extract variable importance from each model
+    var_imp_list <- lapply(rf_models, function(model) model$importance)
+    
+    # Combine variable importance across all models
+    var_imp_df <- do.call(rbind, var_imp_list)
+    
+    # Calculate mean and standard deviation of variable importance
+    var_imp_summary <- data.frame(
+        mean_importance = apply(var_imp_df, 2, mean),
+        sd_importance = apply(var_imp_df, 2, sd)
+    )
+    
+    # Extract and summarize other relevant statistics as needed
+    # For example, you might want to look at average OOB error rate
+    oob_error_rates <- sapply(rf_models, function(model) model$err.rate[nrow(model$err.rate), "OOB"])
+    mean_oob_error <- mean(oob_error_rates)
+    sd_oob_error <- sd(oob_error_rates)
+    
+    # Return the summary statistics
+    list(
+        var_imp_summary = var_imp_summary,
+        mean_oob_error = mean_oob_error,
+        sd_oob_error = sd_oob_error
+    )
+}
+
+# Calculate summary statistics for original and randomized models
+original_summary <- calculate_summary_stats(results$original_rf_models)
+random_summary <- calculate_summary_stats(results$random_rf_models)

# time: 2024-08-30 11:16:25 UTC
# mode: r
+random_summary

# time: 2024-08-30 11:16:35 UTC
# mode: r
+original_summary

# time: 2024-08-30 11:18:24 UTC
# mode: r
+# Calculate summary statistics for the random and original models
+
+
+
+randomForest::MDSplot(results$original_rf_models[[1]], all.label)

# time: 2024-08-30 11:18:34 UTC
# mode: r
+results$original_rf_models[[1]]

# time: 2024-08-30 11:18:58 UTC
# mode: r
+# rfPermute::confusionMatrix(output.forest)
+# rfPermute::plotPredictedProbs(output.forest, bins = 20, plot = TRUE)
+# rfPermute::plotProximity(output.forest)
+# summary(output.forest)
+# rfPermute::plotTrace(output.forest)
+
+results$original_rf_models[[1]]$labels

# time: 2024-08-30 11:19:07 UTC
# mode: r
+results

# time: 2024-08-30 11:22:11 UTC
# mode: r
+# Calculate summary statistics for the random and original models
+
+
+
+randomForest::MDSplot(results$original_rf_models[[1]])

# time: 2024-08-30 11:23:28 UTC
# mode: r
+# Import configuration file
+config <- config::get()
+
+# parameters
+percent.train <- 0.7
+num.threads <- 44
+num.eigenvectors <- 20
+set.seed(555)
+
+# Input files
+immigrant.info <- file.path(config$path$data, "600K_immigrants.fam")
+immigrant.vcf <- file.path(config$path$data, "600K_immigrants.vcf")
+resident.vcf <- file.path(config$path$data, "600K_residents.vcf")
+
+immigrant.gds <- file.path(config$path$data, "immigrant_recode.gds")
+resident.gds <- file.path(config$path$data, "resident_recode.gds")
+all.gds <- file.path(config$path$data, "all_recode.gds")
+
+
+# Load all the data
+all.maf <- SNPRelate::snpgdsOpen(all.gds, allow.duplicate = TRUE)
+
+# Load labels
+immigrant.id <- gdsfmt::read.gdsn(gdsfmt::index.gdsn(
+    SNPRelate::snpgdsOpen(immigrant.gds),
+    "sample.id"
+))
+resident.id <- gdsfmt::read.gdsn(gdsfmt::index.gdsn(
+    SNPRelate::snpgdsOpen(resident.gds),
+    "sample.id"
+))
+
+all.id <- gdsfmt::read.gdsn(gdsfmt::index.gdsn(all.maf, "sample.id"))
+all.label <- ifelse(all.id %in% immigrant.id, "immigrant", "resident")
+
+
+# Function to split data into training and testing sets
+# TODO balence so that there are .5 per class in both train and test sets
+split_data <- function(all.id, all.label, percent.train) {
+    # Split data by class
+    labels <- unique(all.label)
+    class_0 <- all.id[all.label == labels[1]]
+    class_1 <- all.id[all.label == labels[2]]
+
+    # Determine the size of the smaller class
+    min_class_size <- min(length(class_0), length(class_1))
+
+    # Undersample the majority class
+    if (length(class_0) > min_class_size) {
+        class_0 <- sample(class_0, min_class_size)
+    } else {
+        class_1 <- sample(class_1, min_class_size)
+    }
+
+    # Combine the balanced classes
+    balanced_ids <- c(class_0, class_1)
+    balanced_labels <- c(rep(labels[1], length(class_0)), rep(labels[2], length(class_1)))
+
+    # Calculate the number of samples for training
+    num_train <- round(length(balanced_ids) * percent.train)
+
+    # Ensure equal representation of both classes in train and test sets
+    num_train_per_class <- num_train %/% 2
+
+    # Randomly select samples for training from each class
+    train_indices <- c(
+        sample(1:min_class_size, num_train_per_class),
+        sample((min_class_size + 1):(2 * min_class_size), num_train_per_class)
+    )
+
+    train.id <- balanced_ids[train_indices]
+    test.id <- balanced_ids[-train_indices]
+    train.label <- balanced_labels[train_indices]
+    test.label <- balanced_labels[-train_indices]
+
+    list(
+        train.id = train.id, test.id = test.id,
+        train.label = train.label, test.label = test.label
+    )
+}
+# Function to perform PCA and prepare data frames
+prepare_data <- function(all.maf, train.id, test.id, train.label, test.label, num.eigenvectors, num.threads) {
+    pca <- SNPRelate::snpgdsPCA(all.maf, sample.id = train.id, num.thread = num.threads)
+    snp_loadings <- SNPRelate::snpgdsPCASNPLoading(pca, all.maf, num.thread = num.threads)
+    sample_loadings <- SNPRelate::snpgdsPCASampLoading(snp_loadings, all.maf, sample.id = test.id, num.thread = num.threads)
+    train_eigenvects <- data.frame(sample.id = pca$sample.id, pca$eigenvect[, 1:num.eigenvectors])
+    test_eigenvects <- data.frame(sample.id = sample_loadings$sample.id, sample_loadings$eigenvect[, 1:num.eigenvectors])
+    train_df <- data.frame(sample.id = train.id, pop = train.label, train_eigenvects[, -1])
+    test_df <- data.frame(sample.id = test.id, pop = test.label, test_eigenvects[, -1])
+    list(train_df = train_df, test_df = test_df)
+}
+
+# Function to train a random forest with optional randomized labels
+train_random_forest <- function(
+    train_df, test_df, num.eigenvectors, randomize_labels = FALSE) {
+    if (randomize_labels) {
+        train_df$pop <- sample(train_df$pop)
+    }
+
+    rf <- randomForest::randomForest(
+        train_df[, 3:(2 + num.eigenvectors)],
+        y = as.factor(train_df$pop),
+        data = train_df,
+        xtest = test_df[, 3:(2 + num.eigenvectors)],
+        ytest = as.factor(test_df$pop),
+        ntree = 500,
+        replace = FALSE,
+        proximity = TRUE
+    )
+
+    return(rf)
+}
+
+# Function to run a single iteration
+run_iteration <- function(
+    all.id, all.label, all.maf, percent.train, num.eigenvectors, num.threads, randomize_labels) {
+    # Split data
+    split <- split_data(all.id, all.label, percent.train)
+
+    # Prepare data
+    data <- prepare_data(
+        all.maf, split$train.id, split$test.id, split$train.label, split$test.label,
+        num.eigenvectors, num.threads
+    )
+
+    # Train random forest
+    rf <- train_random_forest(data$train_df, data$test_df, num.eigenvectors, randomize_labels)
+
+    # Extract OOB error rates
+    oob_data <- rfPermute::plotTrace(rf, plot = FALSE)$data
+
+    # Return both OOB data, the random forest model, and the train/test labels
+    return(list(oob_data = oob_data, rf_model = rf, train_labels = split$train.label, test_labels = split$test.label))
+}
+
+# Main function to run multiple iterations
+run_analysis <- function(
+    all.id, all.label, all.maf, percent.train,
+    num.eigenvectors, num.threads,
+    n_iterations = 1) {
+    set.seed(123) # For reproducibility
+
+    # Run iterations with original labels
+    original_results <- replicate(n_iterations,
+        run_iteration(all.id, all.label, all.maf, percent.train, 
+        num.eigenvectors, num.threads, FALSE),
+        simplify = FALSE
+    )
+
+    # Run iterations with randomized labels
+    random_results <- replicate(n_iterations,
+        run_iteration(all.id, all.label, all.maf, percent.train, 
+        num.eigenvectors, num.threads, TRUE),
+        simplify = FALSE
+    )
+
+    # Separate OOB data, RF models, and train/test labels
+    original_oob <- lapply(original_results, function(x) x$oob_data)
+    original_rf_models <- lapply(original_results, function(x) x$rf_model)
+    original_train_labels <- lapply(original_results, function(x) x$train_labels)
+    original_test_labels <- lapply(original_results, function(x) x$test_labels)
+
+    random_oob <- lapply(random_results, function(x) x$oob_data)
+    random_rf_models <- lapply(random_results, function(x) x$rf_model)
+    random_train_labels <- lapply(random_results, function(x) x$train_labels)
+    random_test_labels <- lapply(random_results, function(x) x$test_labels)
+
+    # Combine OOB results
+    oob_df <- dplyr::as_tibble(do.call(rbind, original_oob))
+    random_oob_df <- dplyr::as_tibble(do.call(rbind, random_oob))
+
+    list(
+        oob_df = oob_df,
+        random_oob_df = random_oob_df,
+        original_rf_models = original_rf_models,
+        random_rf_models = random_rf_models,
+        original_train_labels = original_train_labels,
+        original_test_labels = original_test_labels,
+        random_train_labels = random_train_labels,
+        random_test_labels = random_test_labels
+    )
+}
+
+# Run the analysis
+results <- run_analysis(all.id, all.label, all.maf, percent.train, num.eigenvectors, num.threads)

# time: 2024-08-30 11:23:54 UTC
# mode: r
+# Calculate summary statistics for the random and original models
+
+
+
+randomForest::MDSplot(results$original_rf_models[[1]], results$original_train_labels[[1]])

# time: 2024-08-30 11:24:02 UTC
# mode: r
+results$original_train_labels[[1]]

# time: 2024-08-30 11:24:08 UTC
# mode: r
+# Calculate summary statistics for the random and original models
+
+
+
+randomForest::MDSplot(results$original_rf_models[[1]], results$original_test_labels[[1]])

# time: 2024-08-30 11:25:28 UTC
# mode: r
+results$original_train_labels[[1]]

# time: 2024-08-30 11:25:37 UTC
# mode: r
+results$original_rf_models[[1]]

# time: 2024-08-30 11:27:10 UTC
# mode: r
+results

# time: 2024-08-30 11:30:05 UTC
# mode: r
+# Import configuration file
+config <- config::get()
+
+# parameters
+percent.train <- 0.7
+num.threads <- 44
+num.eigenvectors <- 20
+set.seed(555)
+
+# Input files
+immigrant.info <- file.path(config$path$data, "600K_immigrants.fam")
+immigrant.vcf <- file.path(config$path$data, "600K_immigrants.vcf")
+resident.vcf <- file.path(config$path$data, "600K_residents.vcf")
+
+immigrant.gds <- file.path(config$path$data, "immigrant_recode.gds")
+resident.gds <- file.path(config$path$data, "resident_recode.gds")
+all.gds <- file.path(config$path$data, "all_recode.gds")
+
+
+# Load all the data
+all.maf <- SNPRelate::snpgdsOpen(all.gds, allow.duplicate = TRUE)
+
+# Load labels
+immigrant.id <- gdsfmt::read.gdsn(gdsfmt::index.gdsn(
+    SNPRelate::snpgdsOpen(immigrant.gds),
+    "sample.id"
+))
+resident.id <- gdsfmt::read.gdsn(gdsfmt::index.gdsn(
+    SNPRelate::snpgdsOpen(resident.gds),
+    "sample.id"
+))
+
+all.id <- gdsfmt::read.gdsn(gdsfmt::index.gdsn(all.maf, "sample.id"))
+all.label <- ifelse(all.id %in% immigrant.id, "immigrant", "resident")
+
+
+# Function to split data into training and testing sets
+# TODO balence so that there are .5 per class in both train and test sets
+split_data <- function(all.id, all.label, percent.train) {
+    # Split data by class
+    labels <- unique(all.label)
+    class_0 <- all.id[all.label == labels[1]]
+    class_1 <- all.id[all.label == labels[2]]
+
+    # Determine the size of the smaller class
+    min_class_size <- min(length(class_0), length(class_1))
+
+    # Undersample the majority class
+    if (length(class_0) > min_class_size) {
+        class_0 <- sample(class_0, min_class_size)
+    } else {
+        class_1 <- sample(class_1, min_class_size)
+    }
+
+    # Combine the balanced classes
+    balanced_ids <- c(class_0, class_1)
+    balanced_labels <- c(rep(labels[1], length(class_0)), rep(labels[2], length(class_1)))
+
+    # Calculate the number of samples for training
+    num_train <- round(length(balanced_ids) * percent.train)
+
+    # Ensure equal representation of both classes in train and test sets
+    num_train_per_class <- num_train %/% 2
+
+    # Randomly select samples for training from each class
+    train_indices <- c(
+        sample(1:min_class_size, num_train_per_class),
+        sample((min_class_size + 1):(2 * min_class_size), num_train_per_class)
+    )
+
+    train.id <- balanced_ids[train_indices]
+    test.id <- balanced_ids[-train_indices]
+    train.label <- balanced_labels[train_indices]
+    test.label <- balanced_labels[-train_indices]
+
+    list(
+        train.id = train.id, test.id = test.id,
+        train.label = train.label, test.label = test.label
+    )
+}
+# Function to perform PCA and prepare data frames
+prepare_data <- function(all.maf, train.id, test.id, train.label, test.label, num.eigenvectors, num.threads) {
+    pca <- SNPRelate::snpgdsPCA(all.maf, sample.id = train.id, num.thread = num.threads)
+    snp_loadings <- SNPRelate::snpgdsPCASNPLoading(pca, all.maf, num.thread = num.threads)
+    sample_loadings <- SNPRelate::snpgdsPCASampLoading(snp_loadings, all.maf, sample.id = test.id, num.thread = num.threads)
+    train_eigenvects <- data.frame(sample.id = pca$sample.id, pca$eigenvect[, 1:num.eigenvectors])
+    test_eigenvects <- data.frame(sample.id = sample_loadings$sample.id, sample_loadings$eigenvect[, 1:num.eigenvectors])
+    train_df <- data.frame(sample.id = train.id, pop = train.label, train_eigenvects[, -1])
+    test_df <- data.frame(sample.id = test.id, pop = test.label, test_eigenvects[, -1])
+    list(train_df = train_df, test_df = test_df)
+}
+
+# Function to train a random forest with optional randomized labels
+train_random_forest <- function(
+    train_df, test_df, num.eigenvectors, randomize_labels = FALSE) {
+    if (randomize_labels) {
+        train_df$pop <- sample(train_df$pop)
+    }
+
+    return(randomForest::randomForest(
+        train_df[, 3:(2 + num.eigenvectors)],
+        y = as.factor(train_df$pop),
+        data = train_df,
+        xtest = test_df[, 3:(2 + num.eigenvectors)],
+        ytest = as.factor(test_df$pop),
+        ntree = 500,
+        replace = FALSE,
+        proximity = TRUE
+    ))
+}
+
+# Function to run a single iteration
+run_iteration <- function(
+    all.id, all.label, all.maf, percent.train, num.eigenvectors, num.threads, randomize_labels) {
+    # Split data
+    split <- split_data(all.id, all.label, percent.train)
+
+    # Prepare data
+    data <- prepare_data(
+        all.maf, split$train.id, split$test.id, split$train.label, split$test.label,
+        num.eigenvectors, num.threads
+    )
+
+    # Train random forest
+    rf <- train_random_forest(data$train_df, data$test_df, num.eigenvectors, randomize_labels)
+
+    # Extract OOB error rates
+    oob_data <- rfPermute::plotTrace(rf, plot = FALSE)$data
+
+    # Return both OOB data, the random forest model, and the train/test labels
+    return(list(oob_data = oob_data, rf_model = rf, train_labels = split$train.label, test_labels = split$test.label))
+}
+
+# Main function to run multiple iterations
+run_analysis <- function(
+    all.id, all.label, all.maf, percent.train,
+    num.eigenvectors, num.threads,
+    n_iterations = 1) {
+    set.seed(123) # For reproducibility
+
+    # Run iterations with original labels
+    original_results <- replicate(n_iterations,
+        run_iteration(all.id, all.label, all.maf, percent.train, 
+        num.eigenvectors, num.threads, FALSE),
+        simplify = FALSE
+    )
+
+    # Run iterations with randomized labels
+    random_results <- replicate(n_iterations,
+        run_iteration(all.id, all.label, all.maf, percent.train, 
+        num.eigenvectors, num.threads, TRUE),
+        simplify = FALSE
+    )
+
+    # Separate OOB data, RF models, and train/test labels
+    original_oob <- lapply(original_results, function(x) x$oob_data)
+    original_rf_models <- lapply(original_results, function(x) x$rf_model)
+    original_train_labels <- lapply(original_results, function(x) x$train_labels)
+    original_test_labels <- lapply(original_results, function(x) x$test_labels)
+
+    random_oob <- lapply(random_results, function(x) x$oob_data)
+    random_rf_models <- lapply(random_results, function(x) x$rf_model)
+    random_train_labels <- lapply(random_results, function(x) x$train_labels)
+    random_test_labels <- lapply(random_results, function(x) x$test_labels)
+
+    # Combine OOB results
+    oob_df <- dplyr::as_tibble(do.call(rbind, original_oob))
+    random_oob_df <- dplyr::as_tibble(do.call(rbind, random_oob))
+
+    list(
+        oob_df = oob_df,
+        random_oob_df = random_oob_df,
+        original_rf_models = original_rf_models,
+        random_rf_models = random_rf_models,
+        original_train_labels = original_train_labels,
+        original_test_labels = original_test_labels,
+        random_train_labels = random_train_labels,
+        random_test_labels = random_test_labels
+    )
+}
+
+# Run the analysis
+results <- run_analysis(all.id, all.label, all.maf, percent.train, num.eigenvectors, num.threads)

# time: 2024-08-30 11:30:34 UTC
# mode: r
+rfPermute::confusionMatrix(results$original_rf_models[[1]])

# time: 2024-08-30 11:30:37 UTC
# mode: r
+# Calculate summary statistics for the random and original models
+
+
+
+randomForest::MDSplot(results$original_rf_models[[1]], results$original_train_labels[[1]])

# time: 2024-08-30 11:30:53 UTC
# mode: r
+# rfPermute::plotPredictedProbs(output.forest, bins = 20, plot = TRUE)
+# rfPermute::plotProximity(output.forest)
+# summary(output.forest)
+# rfPermute::plotTrace(output.forest)
+
+results$original_rf_models[[1]]$importance

# time: 2024-08-30 11:31:22 UTC
# mode: r
+results$original_rf_models[[1]]$confusion

# time: 2024-08-30 11:34:31 UTC
# mode: r
+all.maf

# time: 2024-08-30 11:34:45 UTC
# mode: r
+# Import configuration file
+config <- config::get()
+
+# parameters
+percent.train <- 0.7
+num.threads <- 44
+num.eigenvectors <- 20
+set.seed(555)
+
+# Input files
+immigrant.info <- file.path(config$path$data, "600K_immigrants.fam")
+immigrant.vcf <- file.path(config$path$data, "600K_immigrants.vcf")
+resident.vcf <- file.path(config$path$data, "600K_residents.vcf")
+
+immigrant.gds <- file.path(config$path$data, "immigrant_recode.gds")
+resident.gds <- file.path(config$path$data, "resident_recode.gds")
+all.gds <- file.path(config$path$data, "all_recode.gds")
+
+
+# Load all the data
+all.maf <- SNPRelate::snpgdsOpen(all.gds, allow.duplicate = TRUE)
+
+# Load labels
+immigrant.id <- gdsfmt::read.gdsn(gdsfmt::index.gdsn(
+    SNPRelate::snpgdsOpen(immigrant.gds),
+    "sample.id"
+))
+resident.id <- gdsfmt::read.gdsn(gdsfmt::index.gdsn(
+    SNPRelate::snpgdsOpen(resident.gds),
+    "sample.id"
+))
+
+all.id <- gdsfmt::read.gdsn(gdsfmt::index.gdsn(all.maf, "sample.id"))
+all.label <- ifelse(all.id %in% immigrant.id, "immigrant", "resident")
+
+
+# Function to split data into training and testing sets
+# TODO balence so that there are .5 per class in both train and test sets
+split_data <- function(all.id, all.label, percent.train) {
+    # Split data by class
+    labels <- unique(all.label)
+    class_0 <- all.id[all.label == labels[1]]
+    class_1 <- all.id[all.label == labels[2]]
+
+    # Determine the size of the smaller class
+    min_class_size <- min(length(class_0), length(class_1))
+
+    # Undersample the majority class
+    if (length(class_0) > min_class_size) {
+        class_0 <- sample(class_0, min_class_size)
+    } else {
+        class_1 <- sample(class_1, min_class_size)
+    }
+
+    # Combine the balanced classes
+    balanced_ids <- c(class_0, class_1)
+    balanced_labels <- c(rep(labels[1], length(class_0)), rep(labels[2], length(class_1)))
+
+    # Calculate the number of samples for training
+    num_train <- round(length(balanced_ids) * percent.train)
+
+    # Ensure equal representation of both classes in train and test sets
+    num_train_per_class <- num_train %/% 2
+
+    # Randomly select samples for training from each class
+    train_indices <- c(
+        sample(1:min_class_size, num_train_per_class),
+        sample((min_class_size + 1):(2 * min_class_size), num_train_per_class)
+    )
+
+    train.id <- balanced_ids[train_indices]
+    test.id <- balanced_ids[-train_indices]
+    train.label <- balanced_labels[train_indices]
+    test.label <- balanced_labels[-train_indices]
+
+    list(
+        train.id = train.id, test.id = test.id,
+        train.label = train.label, test.label = test.label
+    )
+}
+# Function to perform PCA and prepare data frames
+prepare_data <- function(all.maf, train.id, test.id, train.label, test.label, num.eigenvectors, num.threads) {
+    pca <- SNPRelate::snpgdsPCA(all.maf, sample.id = train.id, num.thread = num.threads)
+    snp_loadings <- SNPRelate::snpgdsPCASNPLoading(pca, all.maf, num.thread = num.threads)
+    sample_loadings <- SNPRelate::snpgdsPCASampLoading(snp_loadings, all.maf, sample.id = test.id, num.thread = num.threads)
+    train_eigenvects <- data.frame(sample.id = pca$sample.id, pca$eigenvect[, 1:num.eigenvectors])
+    test_eigenvects <- data.frame(sample.id = sample_loadings$sample.id, sample_loadings$eigenvect[, 1:num.eigenvectors])
+    train_df <- data.frame(sample.id = train.id, pop = train.label, train_eigenvects[, -1])
+    test_df <- data.frame(sample.id = test.id, pop = test.label, test_eigenvects[, -1])
+    list(train_df = train_df, test_df = test_df)
+}
+
+# Function to train a random forest with optional randomized labels
+train_random_forest <- function(
+    train_df, test_df, num.eigenvectors, randomize_labels = FALSE) {
+    if (randomize_labels) {
+        train_df$pop <- sample(train_df$pop)
+    }
+
+    return(randomForest::randomForest(
+        train_df[, 3:(2 + num.eigenvectors)],
+        y = as.factor(train_df$pop),
+        data = train_df,
+        xtest = test_df[, 3:(2 + num.eigenvectors)],
+        ytest = as.factor(test_df$pop),
+        ntree = 500,
+        replace = FALSE,
+        proximity = TRUE
+    ))
+}
+
+# Function to run a single iteration
+run_iteration <- function(
+    all.id, all.label, all.maf, percent.train, num.eigenvectors, num.threads, randomize_labels) {
+    # Split data
+    split <- split_data(all.id, all.label, percent.train)
+
+    # Prepare data
+    data <- prepare_data(
+        all.maf, split$train.id, split$test.id, split$train.label, split$test.label,
+        num.eigenvectors, num.threads
+    )
+
+    # Train random forest
+    rf <- train_random_forest(data$train_df, data$test_df, num.eigenvectors, randomize_labels)
+
+    # Extract OOB error rates
+    oob_data <- rfPermute::plotTrace(rf, plot = FALSE)$data
+
+    # Return both OOB data, the random forest model, and the train/test labels
+    return(list(oob_data = oob_data, rf_model = rf, train_labels = split$train.label, test_labels = split$test.label))
+}
+
+# Main function to run multiple iterations
+run_analysis <- function(
+    all.id, all.label, all.maf, percent.train,
+    num.eigenvectors, num.threads,
+    n_iterations = 1) {
+    set.seed(123) # For reproducibility
+
+    # Run iterations with original labels
+    original_results <- replicate(n_iterations,
+        run_iteration(all.id, all.label, all.maf, percent.train, 
+        num.eigenvectors, num.threads, FALSE),
+        simplify = FALSE
+    )
+
+    # Run iterations with randomized labels
+    random_results <- replicate(n_iterations,
+        run_iteration(all.id, all.label, all.maf, percent.train, 
+        num.eigenvectors, num.threads, TRUE),
+        simplify = FALSE
+    )
+
+    # Separate OOB data, RF models, and train/test labels
+    original_oob <- lapply(original_results, function(x) x$oob_data)
+    original_rf_models <- lapply(original_results, function(x) x$rf_model)
+    original_train_labels <- lapply(original_results, function(x) x$train_labels)
+    original_test_labels <- lapply(original_results, function(x) x$test_labels)
+
+    random_oob <- lapply(random_results, function(x) x$oob_data)
+    random_rf_models <- lapply(random_results, function(x) x$rf_model)
+    random_train_labels <- lapply(random_results, function(x) x$train_labels)
+    random_test_labels <- lapply(random_results, function(x) x$test_labels)
+
+    # Combine OOB results
+    oob_df <- dplyr::as_tibble(do.call(rbind, original_oob))
+    random_oob_df <- dplyr::as_tibble(do.call(rbind, random_oob))
+
+    list(
+        oob_df = oob_df,
+        random_oob_df = random_oob_df,
+        original_rf_models = original_rf_models,
+        random_rf_models = random_rf_models,
+        original_train_labels = original_train_labels,
+        original_test_labels = original_test_labels,
+        random_train_labels = random_train_labels,
+        random_test_labels = random_test_labels
+    )
+}
+
+# Run the analysis
+results <- run_analysis(all.id, all.label, all.maf, percent.train, num.eigenvectors, num.threads)
+
+# Calculate mean error for original data
+mean_oob_df <- results$oob_df |>
+    dplyr::group_by(trees, class) |>
+    dplyr::summarise(mean_error = mean(error), .groups = "drop")
+
+# Calculate mean error for randomized data
+mean_random_oob_df <- results$random_oob_df |>
+    dplyr::group_by(trees, class) |>
+    dplyr::summarise(mean_error = mean(error), .groups = "drop")
+
+# Create the plot
+
+oobplot <-
+    ggplot2::ggplot() +
+    ggplot2::geom_line(data = results$oob_df, ggplot2::aes(
+        x = trees, y = error, color = class
+    ), alpha = 0.1) +
+    ggplot2::geom_line(data = results$random_oob_df, ggplot2::aes(
+        x = trees, y = error, color = class
+    ), alpha = 0.1, linetype = "dashed") +
+    ggplot2::geom_line(data = mean_oob_df, ggplot2::aes(
+        x = trees, y = mean_error, color = class
+    ), linewidth = 1.5) +
+    ggplot2::geom_line(data = mean_random_oob_df, ggplot2::aes(
+        x = trees, y = mean_error, color = class
+    ), linewidth = 1.5, linetype = "dashed") +
+    ggplot2::theme_minimal() +
+    ggplot2::labs(
+        title = "OOB error rate (Solid: Original, Dashed: Randomized)",
+        x = "Number of trees", y = "Percent correct"
+    ) +
+    ggplot2::scale_color_manual(values = c(
+        "OOB" = "#4e4e4e",
+        "immigrant" = "#5d8566", "resident" = "#c29007"
+    )) +
+    ggplot2::coord_cartesian(ylim = c(0, 100)) +
+    ggplot2::geom_hline(yintercept = 50, linetype = "dotted", color = "red") +
+    ggplot2::theme(aspect.ratio = 1, text = ggplot2::element_text(size = 12)) +
+    ggplot2::scale_y_continuous(labels = scales::percent_format(scale = 1))
+
+# Save the plot
+ggplot2::ggsave(file.path(config$path$figures, "oob_plot.jpg"),
+    plot =
+        oobplot,
+    width = 8,
+    height = 6
+)
+
+
+# Calculate summary statistics for the random and original models
+
+
+
+randomForest::MDSplot(results$original_rf_models[[1]], results$original_train_labels[[1]])
+
+rfPermute::confusionMatrix(results$original_rf_models[[1]])
+# rfPermute::plotPredictedProbs(output.forest, bins = 20, plot = TRUE)
+# rfPermute::plotProximity(output.forest)
+# summary(output.forest)
+# rfPermute::plotTrace(output.forest)
+
+results$original_rf_models[[1]]$importance
+results$original_rf_models[[1]]$confusion

# time: 2024-08-30 11:37:00 UTC
# mode: r
+head(all.maf)

# time: 2024-08-30 13:14:58 UTC
# mode: r
+# Import configuration file
+config <- config::get()
+
+# parameters
+percent.train <- 0.7
+num.threads <- 44
+num.eigenvectors <- 5
+set.seed(555)
+
+# Input files
+immigrant.info <- file.path(config$path$data, "600K_immigrants.fam")
+immigrant.vcf <- file.path(config$path$data, "600K_immigrants.vcf")
+resident.vcf <- file.path(config$path$data, "600K_residents.vcf")
+
+immigrant.gds <- file.path(config$path$data, "immigrant_recode.gds")
+resident.gds <- file.path(config$path$data, "resident_recode.gds")
+all.gds <- file.path(config$path$data, "all_recode.gds")
+
+
+# Load all the data
+all.maf <- SNPRelate::snpgdsOpen(all.gds, allow.duplicate = TRUE)
+
+# Load labels
+immigrant.id <- gdsfmt::read.gdsn(gdsfmt::index.gdsn(
+    SNPRelate::snpgdsOpen(immigrant.gds),
+    "sample.id"
+))
+resident.id <- gdsfmt::read.gdsn(gdsfmt::index.gdsn(
+    SNPRelate::snpgdsOpen(resident.gds),
+    "sample.id"
+))
+
+all.id <- gdsfmt::read.gdsn(gdsfmt::index.gdsn(all.maf, "sample.id"))
+all.label <- ifelse(all.id %in% immigrant.id, "immigrant", "resident")
+
+
+# Function to split data into training and testing sets
+# TODO balence so that there are .5 per class in both train and test sets
+split_data <- function(all.id, all.label, percent.train) {
+    # Split data by class
+    labels <- unique(all.label)
+    class_0 <- all.id[all.label == labels[1]]
+    class_1 <- all.id[all.label == labels[2]]
+
+    # Determine the size of the smaller class
+    min_class_size <- min(length(class_0), length(class_1))
+
+    # Undersample the majority class
+    if (length(class_0) > min_class_size) {
+        class_0 <- sample(class_0, min_class_size)
+    } else {
+        class_1 <- sample(class_1, min_class_size)
+    }
+
+    # Combine the balanced classes
+    balanced_ids <- c(class_0, class_1)
+    balanced_labels <- c(rep(labels[1], length(class_0)), rep(labels[2], length(class_1)))
+
+    # Calculate the number of samples for training
+    num_train <- round(length(balanced_ids) * percent.train)
+
+    # Ensure equal representation of both classes in train and test sets
+    num_train_per_class <- num_train %/% 2
+
+    # Randomly select samples for training from each class
+    train_indices <- c(
+        sample(1:min_class_size, num_train_per_class),
+        sample((min_class_size + 1):(2 * min_class_size), num_train_per_class)
+    )
+
+    train.id <- balanced_ids[train_indices]
+    test.id <- balanced_ids[-train_indices]
+    train.label <- balanced_labels[train_indices]
+    test.label <- balanced_labels[-train_indices]
+
+    list(
+        train.id = train.id, test.id = test.id,
+        train.label = train.label, test.label = test.label
+    )
+}
+# Function to perform PCA and prepare data frames
+prepare_data <- function(all.maf, train.id, test.id, train.label, test.label, num.eigenvectors, num.threads) {
+    pca <- SNPRelate::snpgdsPCA(all.maf, sample.id = train.id, num.thread = num.threads)
+    snp_loadings <- SNPRelate::snpgdsPCASNPLoading(pca, all.maf, num.thread = num.threads)
+    sample_loadings <- SNPRelate::snpgdsPCASampLoading(snp_loadings, all.maf, sample.id = test.id, num.thread = num.threads)
+    train_eigenvects <- data.frame(sample.id = pca$sample.id, pca$eigenvect[, 1:num.eigenvectors])
+    test_eigenvects <- data.frame(sample.id = sample_loadings$sample.id, sample_loadings$eigenvect[, 1:num.eigenvectors])
+    train_df <- data.frame(sample.id = train.id, pop = train.label, train_eigenvects[, -1])
+    test_df <- data.frame(sample.id = test.id, pop = test.label, test_eigenvects[, -1])
+    list(train_df = train_df, test_df = test_df)
+}
+
+# Function to train a random forest with optional randomized labels
+train_random_forest <- function(
+    train_df, test_df, num.eigenvectors, randomize_labels = FALSE) {
+    if (randomize_labels) {
+        train_df$pop <- sample(train_df$pop)
+    }
+
+    return(randomForest::randomForest(
+        train_df[, 3:(2 + num.eigenvectors)],
+        y = as.factor(train_df$pop),
+        data = train_df,
+        xtest = test_df[, 3:(2 + num.eigenvectors)],
+        ytest = as.factor(test_df$pop),
+        ntree = 500,
+        replace = FALSE,
+        proximity = TRUE
+    ))
+}
+
+# Function to run a single iteration
+run_iteration <- function(
+    all.id, all.label, all.maf, percent.train, num.eigenvectors, num.threads, randomize_labels) {
+    # Split data
+    split <- split_data(all.id, all.label, percent.train)
+
+    # Prepare data
+    data <- prepare_data(
+        all.maf, split$train.id, split$test.id, split$train.label, split$test.label,
+        num.eigenvectors, num.threads
+    )
+
+    # Train random forest
+    rf <- train_random_forest(data$train_df, data$test_df, num.eigenvectors, randomize_labels)
+
+    # Extract OOB error rates
+    oob_data <- rfPermute::plotTrace(rf, plot = FALSE)$data
+
+    # Return both OOB data, the random forest model, and the train/test labels
+    return(list(oob_data = oob_data, rf_model = rf, train_labels = split$train.label, test_labels = split$test.label))
+}
+
+# Main function to run multiple iterations
+run_analysis <- function(
+    all.id, all.label, all.maf, percent.train,
+    num.eigenvectors, num.threads,
+    n_iterations = 1) {
+    set.seed(123) # For reproducibility
+
+    # Run iterations with original labels
+    original_results <- replicate(n_iterations,
+        run_iteration(
+            all.id, all.label, all.maf, percent.train,
+            num.eigenvectors, num.threads, FALSE
+        ),
+        simplify = FALSE
+    )
+
+    # Run iterations with randomized labels
+    random_results <- replicate(n_iterations,
+        run_iteration(
+            all.id, all.label, all.maf, percent.train,
+            num.eigenvectors, num.threads, TRUE
+        ),
+        simplify = FALSE
+    )
+
+    # Separate OOB data, RF models, and train/test labels
+    original_oob <- lapply(original_results, function(x) x$oob_data)
+    original_rf_models <- lapply(original_results, function(x) x$rf_model)
+    original_train_labels <- lapply(original_results, function(x) x$train_labels)
+    original_test_labels <- lapply(original_results, function(x) x$test_labels)
+
+    random_oob <- lapply(random_results, function(x) x$oob_data)
+    random_rf_models <- lapply(random_results, function(x) x$rf_model)
+    random_train_labels <- lapply(random_results, function(x) x$train_labels)
+    random_test_labels <- lapply(random_results, function(x) x$test_labels)
+
+    # Combine OOB results
+    oob_df <- dplyr::as_tibble(do.call(rbind, original_oob))
+    random_oob_df <- dplyr::as_tibble(do.call(rbind, random_oob))
+
+    list(
+        oob_df = oob_df,
+        random_oob_df = random_oob_df,
+        original_rf_models = original_rf_models,
+        random_rf_models = random_rf_models,
+        original_train_labels = original_train_labels,
+        original_test_labels = original_test_labels,
+        random_train_labels = random_train_labels,
+        random_test_labels = random_test_labels
+    )
+}
+
+# Run the analysis
+results <- run_analysis(all.id, all.label, all.maf, percent.train, num.eigenvectors, num.threads)
+
+# Calculate mean error for original data
+mean_oob_df <- results$oob_df |>
+    dplyr::group_by(trees, class) |>
+    dplyr::summarise(mean_error = mean(error), .groups = "drop")
+
+# Calculate mean error for randomized data
+mean_random_oob_df <- results$random_oob_df |>
+    dplyr::group_by(trees, class) |>
+    dplyr::summarise(mean_error = mean(error), .groups = "drop")
+
+# Create the plot
+
+oobplot <-
+    ggplot2::ggplot() +
+    ggplot2::geom_line(data = results$oob_df, ggplot2::aes(
+        x = trees, y = error, color = class
+    ), alpha = 0.1) +
+    ggplot2::geom_line(data = results$random_oob_df, ggplot2::aes(
+        x = trees, y = error, color = class
+    ), alpha = 0.1, linetype = "dashed") +
+    ggplot2::geom_line(data = mean_oob_df, ggplot2::aes(
+        x = trees, y = mean_error, color = class
+    ), linewidth = 1.5) +
+    ggplot2::geom_line(data = mean_random_oob_df, ggplot2::aes(
+        x = trees, y = mean_error, color = class
+    ), linewidth = 1.5, linetype = "dashed") +
+    ggplot2::theme_minimal() +
+    ggplot2::labs(
+        title = "OOB error rate (Solid: Original, Dashed: Randomized)",
+        x = "Number of trees", y = "Percent correct"
+    ) +
+    ggplot2::scale_color_manual(values = c(
+        "OOB" = "#4e4e4e",
+        "immigrant" = "#5d8566", "resident" = "#c29007"
+    )) +
+    ggplot2::coord_cartesian(ylim = c(0, 100)) +
+    ggplot2::geom_hline(yintercept = 50, linetype = "dotted", color = "red") +
+    ggplot2::theme(aspect.ratio = 1, text = ggplot2::element_text(size = 12)) +
+    ggplot2::scale_y_continuous(labels = scales::percent_format(scale = 1))
+
+# Save the plot
+ggplot2::ggsave(file.path(config$path$figures, "oob_plot.jpg"),
+    plot =
+        oobplot,
+    width = 8,
+    height = 6
+)
+
+
+# Calculate summary statistics for the random and original models
+
+
+
+# randomForest::MDSplot(results$original_rf_models[[1]], results$original_train_labels[[1]])
+
+# rfPermute::confusionMatrix(results$original_rf_models[[1]])
+# # rfPermute::plotPredictedProbs(output.forest, bins = 20, plot = TRUE)
+# # rfPermute::plotProximity(output.forest)
+# # summary(output.forest)
+# # rfPermute::plotTrace(output.forest)
+
+# results$original_rf_models[[1]]$importance
+# results$original_rf_models[[1]]$confusion

# time: 2024-08-30 13:16:36 UTC
# mode: r
+# Calculate mean error for original data
+mean_oob_df <- results$oob_df |>
+    dplyr::group_by(trees, class) |>
+    dplyr::summarise(mean_error = mean(error), .groups = "drop") |>
+    dplyr::filter(class != "OOB")
+
+# Calculate mean error for randomized data
+mean_random_oob_df <- results$random_oob_df |>
+    dplyr::group_by(trees, class) |>
+    dplyr::summarise(mean_error = mean(error), .groups = "drop") |>
+    dplyr::filter(class != "OOB")
+
+# Create the plot
+
+oobplot <-
+    ggplot2::ggplot() +
+    ggplot2::geom_line(data = results$oob_df, ggplot2::aes(
+        x = trees, y = error, color = class
+    ), alpha = 0.1) +
+    ggplot2::geom_line(data = results$random_oob_df, ggplot2::aes(
+        x = trees, y = error, color = class
+    ), alpha = 0.1, linetype = "dashed") +
+    ggplot2::geom_line(data = mean_oob_df, ggplot2::aes(
+        x = trees, y = mean_error, color = class
+    ), linewidth = 1.5) +
+    ggplot2::geom_line(data = mean_random_oob_df, ggplot2::aes(
+        x = trees, y = mean_error, color = class
+    ), linewidth = 1.5, linetype = "dashed") +
+    ggplot2::theme_minimal() +
+    ggplot2::labs(
+        title = "OOB error rate (Solid: Original, Dashed: Randomized)",
+        x = "Number of trees", y = "Percent correct"
+    ) +
+    ggplot2::scale_color_manual(values = c(
+        "OOB" = "#4e4e4e",
+        "immigrant" = "#5d8566", "resident" = "#c29007"
+    )) +
+    ggplot2::coord_cartesian(ylim = c(0, 100)) +
+    ggplot2::geom_hline(yintercept = 50, linetype = "dotted", color = "red") +
+    ggplot2::theme(aspect.ratio = 1, text = ggplot2::element_text(size = 12)) +
+    ggplot2::scale_y_continuous(labels = scales::percent_format(scale = 1))
+
+# Save the plot
+ggplot2::ggsave(file.path(config$path$figures, "oob_plot.jpg"),
+    plot =
+        oobplot,
+    width = 8,
+    height = 6
+)
+
+
+# Calculate summary statistics for the random and original models
+
+
+
+# randomForest::MDSplot(results$original_rf_models[[1]], results$original_train_labels[[1]])
+
+# rfPermute::confusionMatrix(results$original_rf_models[[1]])
+# # rfPermute::plotPredictedProbs(output.forest, bins = 20, plot = TRUE)
+# # rfPermute::plotProximity(output.forest)
+# # summary(output.forest)
+# # rfPermute::plotTrace(output.forest)
+
+# results$original_rf_models[[1]]$importance
+# results$original_rf_models[[1]]$confusion

# time: 2024-08-30 13:17:19 UTC
# mode: r
+# Create the plot
+
+oobplot <-
+    ggplot2::ggplot() +
+    ggplot2::geom_line(data = results$oob_df |> dplyr::filter(class != "OOB"), ggplot2::aes(
+        x = trees, y = error, color = class
+    ), alpha = 0.1) +
+    ggplot2::geom_line(data = results$random_oob_df|> dplyr::filter(class != "OOB"), ggplot2::aes(
+        x = trees, y = error, color = class
+    ), alpha = 0.1, linetype = "dashed") +
+    ggplot2::geom_line(data = mean_oob_df, ggplot2::aes(
+        x = trees, y = mean_error, color = class
+    ), linewidth = 1.5) +
+    ggplot2::geom_line(data = mean_random_oob_df, ggplot2::aes(
+        x = trees, y = mean_error, color = class
+    ), linewidth = 1.5, linetype = "dashed") +
+    ggplot2::theme_minimal() +
+    ggplot2::labs(
+        title = "OOB error rate (Solid: Original, Dashed: Randomized)",
+        x = "Number of trees", y = "Percent correct"
+    ) +
+    ggplot2::scale_color_manual(values = c(
+        "OOB" = "#4e4e4e",
+        "immigrant" = "#5d8566", "resident" = "#c29007"
+    )) +
+    ggplot2::coord_cartesian(ylim = c(0, 100)) +
+    ggplot2::geom_hline(yintercept = 50, linetype = "dotted", color = "red") +
+    ggplot2::theme(aspect.ratio = 1, text = ggplot2::element_text(size = 12)) +
+    ggplot2::scale_y_continuous(labels = scales::percent_format(scale = 1))

# time: 2024-08-30 13:17:19 UTC
# mode: r
+# Save the plot
+ggplot2::ggsave(file.path(config$path$figures, "oob_plot.jpg"),
+    plot =
+        oobplot,
+    width = 8,
+    height = 6
+)

# time: 2024-08-30 13:18:33 UTC
# mode: r
+# Calculate mean error for original data
+mean_oob_df <- results$oob_df %>%
+    dplyr::group_by(trees, class) %>%
+    dplyr::summarise(mean_error = mean(error), .groups = "drop") %>%
+    dplyr::filter(class != "OOB")

# time: 2024-08-30 13:18:34 UTC
# mode: r
+# Calculate mean error for randomized data
+mean_random_oob_df <- results$random_oob_df %>%
+    dplyr::group_by(trees, class) %>%
+    dplyr::summarise(mean_error = mean(error), .groups = "drop") %>%
+    dplyr::filter(class != "OOB")

# time: 2024-08-30 13:18:49 UTC
# mode: r
+# Calculate mean error for original data
+mean_oob_df <- results$oob_df |>
+    dplyr::group_by(trees, class) |>
+    dplyr::summarise(mean_error = mean(error), .groups = "drop") |>
+    dplyr::filter(class != "OOB")

# time: 2024-08-30 13:18:49 UTC
# mode: r
+# Calculate mean error for randomized data
+mean_random_oob_df <- results$random_oob_df |>
+    dplyr::group_by(trees, class) |>
+    dplyr::summarise(mean_error = mean(error), .groups = "drop") |>
+    dplyr::filter(class != "OOB")

# time: 2024-08-30 13:18:50 UTC
# mode: r
+# Create the plot data
+plot_data <- list(
+    oob_df = results$oob_df |>
+        dplyr::filter(class != "OOB"),
+    random_oob_df = results$random_oob_df |>
+        dplyr::filter(class != "OOB"),
+    mean_oob_df = mean_oob_df,
+    mean_random_oob_df = mean_random_oob_df
+)

# time: 2024-08-30 13:18:51 UTC
# mode: r
+# Create the plot
+oobplot <- ggplot2::ggplot() +
+    ggplot2::geom_line(data = plot_data$oob_df, ggplot2::aes(
+        x = trees, y = error, color = class
+    ), alpha = 0.1) +
+    ggplot2::geom_line(data = plot_data$random_oob_df, ggplot2::aes(
+        x = trees, y = error, color = class
+    ), alpha = 0.1, linetype = "dashed") +
+    ggplot2::geom_line(data = plot_data$mean_oob_df, ggplot2::aes(
+        x = trees, y = mean_error, color = class
+    ), linewidth = 1.5) +
+    ggplot2::geom_line(data = plot_data$mean_random_oob_df, ggplot2::aes(
+        x = trees, y = mean_error, color = class
+    ), linewidth = 1.5, linetype = "dashed") +
+    ggplot2::theme_minimal() +
+    ggplot2::labs(
+        title = "OOB error rate (Solid: Original, Dashed: Randomized)",
+        x = "Number of trees", y = "Percent correct"
+    ) +
+    ggplot2::scale_color_manual(values = c(
+        "OOB" = "#4e4e4e",
+        "immigrant" = "#5d8566", "resident" = "#c29007"
+    )) +
+    ggplot2::coord_cartesian(ylim = c(0, 100)) +
+    ggplot2::geom_hline(yintercept = 50, linetype = "dotted", color = "red") +
+    ggplot2::theme(aspect.ratio = 1, text = ggplot2::element_text(size = 12)) +
+    ggplot2::scale_y_continuous(labels = scales::percent_format(scale = 1))

# time: 2024-08-30 13:18:51 UTC
# mode: r
+# Save the plot
+ggplot2::ggsave(file.path(config$path$figures, "oob_plot.jpg"),
+    plot =
+        oobplot,
+    width = 8,
+    height = 6
+)

# time: 2024-08-30 13:19:30 UTC
# mode: r
+# Calculate mean error for original data
+# Define a function to calculate mean error
+calculate_mean_error <- function(data) {
+    data |>
+        dplyr::group_by(trees, class) |>
+        dplyr::summarise(mean_error = mean(error), .groups = "drop") |>
+        dplyr::filter(class != "OOB")
+}

# time: 2024-08-30 13:19:31 UTC
# mode: r
+# Calculate mean error for original data
+mean_oob_df <- calculate_mean_error(results$oob_df)

# time: 2024-08-30 13:19:31 UTC
# mode: r
+# Calculate mean error for randomized data
+mean_random_oob_df <- calculate_mean_error(results$random_oob_df)

# time: 2024-08-30 13:19:32 UTC
# mode: r
+# Create the plot data
+plot_data <- list(
+    oob_df = results$oob_df |>
+        dplyr::filter(class != "OOB"),
+    random_oob_df = results$random_oob_df |>
+        dplyr::filter(class != "OOB"),
+    mean_oob_df = mean_oob_df,
+    mean_random_oob_df = mean_random_oob_df
+)

# time: 2024-08-30 13:19:34 UTC
# mode: r
+plot_data

# time: 2024-08-30 13:20:05 UTC
# mode: r
+calculate_mean_error <- function(data) {
+    data |>
+        dplyr::group_by(trees, class) |>
+        dplyr::summarise(mean_error = mean(error), .groups = "drop") |>
+        dplyr::filter(class != "OOB")
+}

# time: 2024-08-30 13:24:14 UTC
# mode: r
+renv::status()

# time: 2024-08-30 13:24:31 UTC
# mode: r
+renv::snapshot()

# time: 2024-08-30 13:25:05 UTC
# mode: r
+renv::dependencies('httpgd')

# time: 2024-08-30 13:25:12 UTC
# mode: r
+renv::dependencies()

# time: 2024-08-30 13:26:01 UTC
# mode: r
+renv::snapshot()
